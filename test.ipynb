{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.img_order_embedding.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at data_model/t5_pretrained were not used when initializing JointEncoder: ['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointEncoder were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.img_order_embedding.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.0.layer.0.layer_norm.weight', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.0.layer.1.layer_norm.weight', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.1.layer.0.layer_norm.weight', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.2.layer.0.SelfAttention.q.weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.layer.0.layer_norm.weight', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.layer_norm.weight', 'block.3.layer.0.SelfAttention.q.weight', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.3.layer.0.layer_norm.weight', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.3.layer.1.layer_norm.weight', 'block.4.layer.0.SelfAttention.q.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.4.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.layer_norm.weight', 'block.5.layer.0.SelfAttention.q.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.5.layer.0.layer_norm.weight', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.5.layer.1.layer_norm.weight', 'block.6.layer.0.SelfAttention.q.weight', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.6.layer.0.SelfAttention.o.weight', 'block.6.layer.0.layer_norm.weight', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.layer_norm.weight', 'block.7.layer.0.SelfAttention.q.weight', 'block.7.layer.0.SelfAttention.k.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.7.layer.0.layer_norm.weight', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.7.layer.1.layer_norm.weight', 'block.8.layer.0.SelfAttention.q.weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.8.layer.0.SelfAttention.o.weight', 'block.8.layer.0.layer_norm.weight', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.8.layer.1.layer_norm.weight', 'block.9.layer.0.SelfAttention.q.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.9.layer.0.SelfAttention.v.weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.9.layer.0.layer_norm.weight', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.9.layer.1.layer_norm.weight', 'block.10.layer.0.SelfAttention.q.weight', 'block.10.layer.0.SelfAttention.k.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.10.layer.0.SelfAttention.o.weight', 'block.10.layer.0.layer_norm.weight', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.10.layer.1.layer_norm.weight', 'block.11.layer.0.SelfAttention.q.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.v.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.11.layer.0.layer_norm.weight', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.layer.1.layer_norm.weight', 'final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18711/942544814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"experiments/configEncoder/training_params/training_vlt5.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_encoder_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/trainer_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_question_path, config_passage_path, config_training_path, train_loader, val_loader, test_loader, train)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m't5'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             self.encoder_question.resize_token_embeddings(\n\u001b[0m\u001b[1;32m    107\u001b[0m                 self.tokenizer_question.vocab_size)\n\u001b[1;32m    108\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'bart'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mresize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPointer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0mEmbeddings\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mmodel_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_num_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_resize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mold_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mnew_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_resized_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resized_embeddings\u001b[0;34m(self, old_embeddings, new_num_tokens)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mold_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mold_num_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_embedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_num_tokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mold_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/configEncoder/args_VLT5.json\"\n",
    "config_encoder_passage = \"experiments/configEncoder/args_VLT5.json\"\n",
    "config_training = \"experiments/configEncoder/training_params/training_vlt5.json\"\n",
    "\n",
    "trainer = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = trainer.tokenizer_passage('lapin [SEP]').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lapin [SEP]</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer_passage.decode(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.tokenization import VLT5Tokenizer, VLT5TokenizerFast\n",
    "tokenizer = VLT5TokenizerFast.from_pretrained('data/tokenizer/t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config\n",
    "config_class = T5Config\n",
    "config = config_class.from_pretrained(\"data_model/t5_pretrained\")\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"experiments/configEncoder/args_VLT5.json\", 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict(args)\n",
    "config.feat_dim = args.feat_dim\n",
    "config.pos_dim = args.pos_dim\n",
    "config.n_images = 2\n",
    "\n",
    "config.use_vis_order_embedding = args.use_vis_order_embedding\n",
    "\n",
    "config.dropout_rate = args.dropout\n",
    "config.dropout = args.dropout\n",
    "config.attention_dropout = args.dropout\n",
    "config.activation_dropout = args.dropout\n",
    "\n",
    "config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "config.individual_vis_layer_norm = args.individual_vis_layer_norm\n",
    "config.losses = args.losses\n",
    "\n",
    "config.share_vis_lang_layer_norm = args.share_vis_lang_layer_norm\n",
    "config.classifier = args.classifier\n",
    "\n",
    "from cvlep.VLT5.modeling_t5 import JointEncoder\n",
    "\n",
    "model_name = \"data_model/t5_pretrained\"\n",
    "model = JointEncoder.from_pretrained(\n",
    "    model_name,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger checkpoint via state dict\n",
    "from cvlep.VLT5.utils import load_state_dict\n",
    "ckpt_path = \"data_model/jointEncoder/state_dict/VLT5epoch30\"\n",
    "state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "original_keys = list(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_keys = list(state_dict.keys())\n",
    "for key in original_keys:\n",
    "    if key.startswith(\"vis_encoder.\"):\n",
    "        new_key = 'encoder.' + key[len(\"vis_encoder.\"):]\n",
    "        state_dict[new_key] = state_dict.pop(key)\n",
    "\n",
    "    if key.startswith(\"model.vis_encoder.\"):\n",
    "        new_key = 'model.encoder.' + key[len(\"model.vis_encoder.\"):]\n",
    "        state_dict[new_key] = state_dict.pop(key)\n",
    "\n",
    "results = model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointEncoder(\n",
       "  (visual_embedding): VisualEmbedding(\n",
       "    (feat_embedding): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (1): T5LayerNorm()\n",
       "    )\n",
       "    (absolute_vis_pos_embedding): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (1): T5LayerNorm()\n",
       "    )\n",
       "    (img_order_embedding): Embedding(2, 768)\n",
       "  )\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VL BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base import Trainer\n",
    "\n",
    "path_config = \"experiments/model_cvlep/encodersBart.json\"\n",
    "trainer = Trainer(path_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'backbone_passage': 'bart',\\n 'backbone_question': 'bart',\\n 'embedding_dim': 768,\\n 'num_embeddings': 50465,\\n 'shared_embedding': True,\\n 'use_projection': False}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.config.config_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just try to load without apply embedding and see if we already have the embedding\n",
    "from transformers import BartConfig\n",
    "from cvlep.VLT5.modeling_bart import JointEncoder\n",
    "temp = BartConfig.from_pretrained('data_model/jointEncoder/Bartepoch30')\n",
    "encoder = JointEncoder(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from cvlep.utils import device\n",
    "embedding = nn.Embedding(50465,768)\n",
    "embedding.load_state_dict(torch.load(\"data_model/embedding/Bartepoch30\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0087,  0.0107, -0.0100,  ...,  0.0134,  0.1015,  0.0041],\n",
       "        [ 0.0052,  0.0013, -0.0265,  ..., -0.0048,  0.0138, -0.0064],\n",
       "        [ 0.0361,  0.0057,  0.0085,  ...,  0.0547,  0.0270,  0.0469],\n",
       "        ...,\n",
       "        [ 0.0291, -0.0131, -0.0512,  ..., -0.0230, -0.0292,  0.0559],\n",
       "        [ 0.0612, -0.0263, -0.0298,  ..., -0.0385, -0.0123,  0.0363],\n",
       "        [ 0.0481,  0.0120, -0.0077,  ..., -0.0404, -0.0105,  0.0497]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0082, -0.0147,  0.0142,  ..., -0.0429,  0.0094, -0.0031],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0161,  0.0231,  0.0134,  ..., -0.0171,  0.0082,  0.0079],\n",
       "        ...,\n",
       "        [ 0.0042,  0.0283,  0.0208,  ...,  0.0025,  0.0201, -0.0083],\n",
       "        [ 0.0136,  0.0103,  0.0255,  ..., -0.0037,  0.0096, -0.0101],\n",
       "        [ 0.0113, -0.0148, -0.0266,  ..., -0.0179,  0.0142, -0.0216]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.image_question_encoder.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50465, 768, padding_idx=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15275, 4534, 5, 451, 1518, 21, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Michelle Obama. She stand for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer(\"James Bond [SEP]\").input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2816, 1258, 8019, 1], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"pikachu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Bond [SEP]</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2508, 5416, 102, 5416, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"James bond [SEP] bond\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2508, 5416, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('James Bond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de sortie T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel, T5TokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-small were not used when initializing T5EncoderModel: ['decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = T5EncoderModel.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[21603,    10,     8,  1419,    19,  6446,    68,    59,  5107,     1],\n",
      "        [  161,   161,     1,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=0'>1</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=1'>2</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msummarize: the situation is complicated but not dangerous\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mwork work\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=4'>5</a>\u001b[0m         ], \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=5'>6</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(input_ids)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pgrimal/Documents/Projects/visual_language_representation/test.ipynb#ch0000009?line=8'>9</a>\u001b[0m B, L \u001b[39m=\u001b[39m input_ids[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    [\n",
    "        'summarize: the situation is complicated but not dangerous',\n",
    "        'work work'\n",
    "        ], \n",
    "    return_tensors='pt', padding = True)\n",
    "print(input_ids)\n",
    "\n",
    "B, L = input_ids['input_ids'].size()[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder(output_hidden_states=True, **input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_all_hidden_states = output[0]\n",
    "all_hidden_states = output[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim last hidden : torch.Size([1, 7, 512])\n",
      "7  layers * 7x512\n"
     ]
    }
   ],
   "source": [
    "print(\"dim last hidden :\", last_hidden_states.shape)\n",
    "print(len(all_hidden_states), ' layers * 7x512')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 7   (initial embeddings + X layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 10\n",
      "Number of hidden units: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers:\", len(all_hidden_states),\n",
    "      \"  (initial embeddings + X layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print(\"Number of batches:\", len(all_hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print(\"Number of tokens:\", len(all_hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print(\"Number of hidden units:\", len(\n",
    "    all_hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 : Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states[layer_i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pooler output CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"a photo of a cat\"], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim hidden : torch.Size([1, 7, 512])\n",
      "dim pool :  torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "pooled_output = outputs.pooler_output\n",
    "\n",
    "print('dim hidden :', last_hidden_state.shape)\n",
    "print('dim pool : ',pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test chargement des encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.modeling_t5 import JointEncoder as t5encoder\n",
    "from cvlep.VLT5.modeling_bart import JointEncoder as BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data/jointEncoder//VLT5_without_pretrain were not used when initializing JointEncoder: ['embed_tokens.weight', 'visual_embedding.obj_order_embedding.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at data/jointEncoder//VLT5epoch30 were not used when initializing JointEncoder: ['embed_tokens.weight', 'visual_embedding.obj_order_embedding.weight']\n",
      "- This IS expected if you are initializing JointEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PATH = 'data/jointEncoder/'\n",
    "encoder1 = BartEncoder.from_pretrained(f'{PATH}/bart_without_pretrain')\n",
    "encoder2 = BartEncoder.from_pretrained(f'{PATH}/Bartepoch30')\n",
    "encoder3 = t5encoder.from_pretrained(f'{PATH}/VLT5_without_pretrain')\n",
    "encoder4 = t5encoder.from_pretrained(f'{PATH}/VLT5epoch30')\n",
    "\n",
    "# OK it worsks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"data/jointEncoder//bart_without_pretrain\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"JointEncoder\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"default_obj_order_ids\": [\n",
       "    50464,\n",
       "    50463,\n",
       "    50462,\n",
       "    50461,\n",
       "    50460,\n",
       "    50459,\n",
       "    50458,\n",
       "    50457,\n",
       "    50456,\n",
       "    50455,\n",
       "    50454,\n",
       "    50453,\n",
       "    50452,\n",
       "    50451,\n",
       "    50450,\n",
       "    50449,\n",
       "    50448,\n",
       "    50447,\n",
       "    50446,\n",
       "    50445,\n",
       "    50444,\n",
       "    50443,\n",
       "    50442,\n",
       "    50441,\n",
       "    50440,\n",
       "    50439,\n",
       "    50438,\n",
       "    50437,\n",
       "    50436,\n",
       "    50435,\n",
       "    50434,\n",
       "    50433,\n",
       "    50432,\n",
       "    50431,\n",
       "    50430,\n",
       "    50429,\n",
       "    50428,\n",
       "    50427,\n",
       "    50426,\n",
       "    50425,\n",
       "    50424,\n",
       "    50423,\n",
       "    50422,\n",
       "    50421,\n",
       "    50420,\n",
       "    50419,\n",
       "    50418,\n",
       "    50417,\n",
       "    50416,\n",
       "    50415,\n",
       "    50414,\n",
       "    50413,\n",
       "    50412,\n",
       "    50411,\n",
       "    50410,\n",
       "    50409,\n",
       "    50408,\n",
       "    50407,\n",
       "    50406,\n",
       "    50405,\n",
       "    50404,\n",
       "    50403,\n",
       "    50402,\n",
       "    50401,\n",
       "    50400,\n",
       "    50399,\n",
       "    50398,\n",
       "    50397,\n",
       "    50396,\n",
       "    50395,\n",
       "    50394,\n",
       "    50393,\n",
       "    50392,\n",
       "    50391,\n",
       "    50390,\n",
       "    50389,\n",
       "    50388,\n",
       "    50387,\n",
       "    50386,\n",
       "    50385,\n",
       "    50384,\n",
       "    50383,\n",
       "    50382,\n",
       "    50381,\n",
       "    50380,\n",
       "    50379,\n",
       "    50378,\n",
       "    50377,\n",
       "    50376,\n",
       "    50375,\n",
       "    50374,\n",
       "    50373,\n",
       "    50372,\n",
       "    50371,\n",
       "    50370,\n",
       "    50369,\n",
       "    50368,\n",
       "    50367,\n",
       "    50366,\n",
       "    50365\n",
       "  ],\n",
       "  \"dropout\": 0.1,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"feat_dim\": 2048,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"individual_vis_layer_norm\": true,\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"losses\": \"lm,obj,attr,feat\",\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"n_images\": 2,\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pos_dim\": 4,\n",
       "  \"scale_embedding\": false,\n",
       "  \"share_vis_lang_layer_norm\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_vis_layer_norm\": true,\n",
       "  \"use_vis_order_embedding\": true,\n",
       "  \"vocab_size\": 50465\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1.config\n",
    "# on doit pouvoir changer la T5 config ici pour en faire ce qu'on veut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlep/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "path = 'experiments/model_cvlep/encodersT5.json'\n",
    "cfg, k = Config.load_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/jointEncoder/VLT5epoch30'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_question\n",
      "2 {'model': {'backbone': 't5', 'pretrain': True, 'pretrained_model_name_or_path': 'data/jointEncoder/VLT5epoch30'}, 'tokenizer': {'backbone': 't5', 'use_vision': True, 'pretrained_model_name_or_path': 'data/t5_pretrained', 'max_length': 100, 'do_lower_case': True}}\n",
      "encoder_passage\n",
      "2 {'model': {'backbone': 't5', 'pretrain': True, 'pretrained_model_name_or_path': 'data/jointEncoder/VLT5epoch30'}, 'tokenizer': {'backbone': 't5', 'use_vision': True, 'pretrained_model_name_or_path': 'data/t5_pretrained', 'max_length': 100, 'do_lower_case': True}}\n",
      "cvlep\n",
      "1 {'use_projection': False}\n"
     ]
    }
   ],
   "source": [
    "for a, v in k.items():\n",
    "    print(a)\n",
    "    if isinstance(v, dict):\n",
    "        print(len(v), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test charger tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlep/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/tokenizer/bart-base/tokenizer_config.json',\n",
       " 'data/tokenizer/bart-base/special_tokens_map.json',\n",
       " 'data/tokenizer/bart-base/vocab.json',\n",
       " 'data/tokenizer/bart-base/merges.txt',\n",
       " 'data/tokenizer/bart-base/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "bart_tokenizer.save_pretrained('data/tokenizer/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/tokenizer/t5-base/tokenizer_config.json',\n",
       " 'data/tokenizer/t5-base/special_tokens_map.json',\n",
       " 'data/tokenizer/t5-base/spiece.model',\n",
       " 'data/tokenizer/t5-base/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_tokenizer.save_pretrained('data/tokenizer/t5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37b529e5b04cb6020e07ac9799378b67aa684445561c484d3885bcda8cad6052"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cvlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
