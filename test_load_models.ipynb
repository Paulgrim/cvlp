{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple_adapter.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "DPRDataset\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "a = nn.Linear(768, 10, bias= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26852/4112195018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5679/1841660083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_base_vladapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_encoder_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_encoder_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/trainer_base_vladapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJointEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mencoderVLT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvlep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPT5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneDDownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from .adapters import (\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmy_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mBART_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m )\n\u001b[0;32m-> 1293\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBartPretrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/CLIPT5/my_transformers/modeling_bart.py\u001b[0m in \u001b[0;36mBartModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSeq2SeqModelOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CONFIG_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m     def forward(\n",
      "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.model.image_passage_encoder.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLT5(\n",
      "  (shared): Embedding(32200, 768)\n",
      "  (encoder): JointEncoder(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (visual_embedding): VisualEmbedding(\n",
      "      (feat_embedding): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (absolute_vis_pos_embedding): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "        (1): T5LayerNorm()\n",
      "      )\n",
      "      (obj_order_embedding): Embedding(32200, 768)\n",
      "      (img_order_embedding): Embedding(2, 768)\n",
      "    )\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32200, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseReluDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (ff_adapter): AdapterController(\n",
      "              (adapters): ModuleDict(\n",
      "                (IR): Adapter(\n",
      "                  (activation): Activations()\n",
      "                  (down_sampler): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (up_sampler): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32200, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected 1 arguments, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25082/4206866470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_passage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected 1 arguments, got 0"
     ]
    }
   ],
   "source": [
    "trainer.model.image_passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLT5 without adapter or prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_model.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/prompt/encoder_prompting.json\"\n",
    "config_training = \"experiments/config_vladapter/prompt/training_prompt.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.prompt_modules.prompts.IR.prefix_embedding.0.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.1.bias\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.weight\n",
      "encoder.prompt_modules.prompts.IR.prefix_embedding.3.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapter config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/adapter_for_contrastive/encoder_simple_adapter.json\"\n",
    "config_training = \"experiments/config_vladapter/adapter_for_contrastive/training_simple_adapter.json\"\n",
    "\n",
    "trainer_clipt5 = Trainer(config_encoder_question, config_encoder_passage, config_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in trainer_clipt5.model.image_passage_encoder.named_parameters():\n",
    "    if p.requires_grad==True:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer_model.embedding_question( input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32200, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.model.image_passage_encoder.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]]), pooler_output=tensor([[ 5.6648e-02, -7.1184e-02, -1.5301e-01, -3.6746e-02, -1.1649e-01,\n",
       "          3.5650e-02,  4.6196e-02, -9.3280e-02, -1.2167e-01,  1.7649e-01,\n",
       "         -1.1129e-01, -5.8460e-02,  9.5520e-02,  1.1314e-02,  4.1585e-02,\n",
       "         -1.2416e-02, -1.0474e-01,  3.2099e-02,  6.2653e-02,  4.1071e-02,\n",
       "          8.7517e-02, -2.3704e-01,  7.3841e-02, -1.6983e-01, -1.1024e-01,\n",
       "          1.1043e-01, -8.2290e-02, -2.5255e-02, -3.4988e-01,  1.7297e-01,\n",
       "          1.3527e-01,  2.0638e-01, -9.9829e-02,  2.6702e-02, -5.7706e-02,\n",
       "         -3.6114e-02, -2.1935e-01, -1.2021e-01,  4.0559e-02,  1.3619e-01,\n",
       "          1.1547e-01,  2.1434e-01,  4.3598e-02,  8.4799e-02, -1.3910e-01,\n",
       "         -7.6862e-02,  4.0068e-02,  8.2855e-02, -1.2356e-01, -2.3356e-02,\n",
       "          8.9281e-02, -2.4328e-01,  9.7265e-02,  6.4604e-02, -6.1479e-02,\n",
       "         -7.6788e-02,  2.0053e-01, -3.4412e-02, -4.9680e-02,  7.8987e-02,\n",
       "         -5.9437e-02, -1.5208e-01, -4.1240e-02, -1.8452e-03,  9.6731e-02,\n",
       "          2.5129e-03, -2.7472e-01,  6.3285e-03, -1.7972e-02,  5.6221e-02,\n",
       "          7.1725e-04,  1.1230e-01, -1.5480e-01,  1.7283e-01,  1.7725e-02,\n",
       "         -8.6306e-03, -5.8082e-02, -5.2759e-02, -1.4953e-02, -3.4251e-02,\n",
       "         -1.8136e-02, -7.2912e-02,  8.0153e-02, -4.5119e-02,  1.3014e-01,\n",
       "          1.7871e-01,  2.4108e-01,  2.5335e-01, -9.1655e-02,  1.0435e-01,\n",
       "         -6.0266e-02, -1.0284e-02, -5.8338e-02, -1.3664e-01,  4.4927e-03,\n",
       "          2.5686e-01,  1.0977e-01,  6.0540e-03,  9.0139e-02, -3.6682e-02,\n",
       "          1.0168e-01, -9.5243e-02,  6.0577e-02, -2.1505e-02, -2.4120e-01,\n",
       "         -3.9939e-02, -2.8749e-02,  1.3322e-02,  6.9176e-02,  1.3201e-01,\n",
       "          7.8436e-02, -1.1388e-03, -7.8952e-02, -1.1538e-01, -1.4526e-01,\n",
       "         -2.6577e-02,  1.1842e-01, -9.1086e-02, -1.2194e-01,  8.6370e-02,\n",
       "         -6.8970e-02,  1.9308e-02,  2.5355e-02, -1.7590e-01, -2.4092e-01,\n",
       "         -4.3160e-03,  5.0034e-02, -1.3436e-01, -4.2643e-02, -1.6965e-01,\n",
       "         -1.9567e-01,  1.1454e-01, -1.0889e-02, -1.8757e-01,  9.9909e-02,\n",
       "         -1.1825e-01, -3.9800e-02,  8.9019e-02,  3.1374e-02, -2.2346e-01,\n",
       "          4.8511e-03, -1.9611e-03, -8.6634e-02,  2.3533e-02,  1.2248e-01,\n",
       "         -1.0433e-01, -6.2481e-02,  3.6984e-02, -1.5841e-01,  1.0640e-01,\n",
       "         -1.6670e-01,  7.9076e-02,  6.0324e-02, -7.1101e-02,  8.4890e-02,\n",
       "         -2.3785e-01,  9.3966e-02,  1.5102e-01, -3.0517e-01, -1.9984e-01,\n",
       "         -8.9604e-02,  3.3380e-02, -8.0320e-02,  1.7646e-01,  1.9851e-02,\n",
       "          1.0962e-01, -2.9159e-02, -1.4942e-01,  1.0430e-01,  1.3476e-01,\n",
       "         -2.5006e-02,  4.2738e-02,  9.7129e-02, -4.5894e-02,  1.9123e-01,\n",
       "         -1.9203e-01,  6.1338e-02, -1.3621e-01, -6.9330e-02,  1.9044e-02,\n",
       "          1.7211e-02, -1.8426e-01, -7.0352e-03, -3.2355e-02, -1.2859e-01,\n",
       "          1.2835e-01, -8.0131e-02,  2.7095e-02, -1.6080e-01, -5.7696e-02,\n",
       "         -7.1342e-02,  6.3420e-02, -1.0779e-01,  1.9965e-01, -9.6445e-02,\n",
       "         -6.2895e-02,  7.8046e-02,  1.7269e-01, -6.3947e-02,  2.3196e-03,\n",
       "          5.7397e-02, -2.3577e-02,  3.3009e-02,  9.0933e-02, -1.3351e-01,\n",
       "          4.1424e-02,  3.3542e-03, -4.5304e-02, -1.7649e-01,  1.3891e-01,\n",
       "          1.1829e-01, -1.3549e-01,  1.1509e-01,  5.2406e-02,  8.0264e-04,\n",
       "         -2.5466e-01,  1.9854e-01,  2.8487e-02, -1.5047e-01, -2.2250e-01,\n",
       "         -4.1130e-01, -1.3324e-01,  5.2766e-03, -3.3753e-02,  2.8282e-02,\n",
       "          9.0963e-02,  1.0388e-01,  3.5608e-01, -3.8365e-02, -7.9780e-03,\n",
       "          5.2078e-02,  2.4794e-01,  1.6939e-01,  4.0255e-02,  1.3860e-01,\n",
       "         -5.7090e-02, -4.2355e-02, -2.3558e-02,  4.7343e-02, -6.7799e-02,\n",
       "         -1.1853e-01, -1.3973e-01, -5.0139e-02,  1.0560e-01,  1.6904e-01,\n",
       "          5.3939e-02,  1.1163e-01, -4.3946e-02,  1.6269e-01, -2.6001e-01,\n",
       "          1.8189e-01, -1.9251e-01,  1.9725e-01,  5.3467e-02,  4.2570e-02,\n",
       "          2.4926e-02,  1.0339e-01, -4.6494e-02,  5.0573e-02, -1.0053e-01,\n",
       "         -3.5200e-02,  3.5145e-01,  3.4159e-03, -1.5362e-01, -9.6309e-03,\n",
       "          2.7890e-01, -6.3908e-02,  1.6790e-01, -7.0743e-02,  1.1093e-01,\n",
       "         -4.0014e-02, -8.8798e-02, -1.0509e-01, -2.9235e-02,  3.5896e-02,\n",
       "         -1.4439e-01,  2.7185e-02, -3.8404e-02, -1.4742e-02,  1.1536e-01,\n",
       "         -2.5055e-01,  7.6182e-02,  8.4615e-05,  1.0779e-01, -1.9729e-01,\n",
       "         -4.8202e-03, -1.1150e-01, -1.5042e-01, -6.8012e-03,  6.0199e-02,\n",
       "          1.0188e-01, -1.0791e-01,  1.1259e-01, -4.7317e-02,  7.9064e-02,\n",
       "         -8.2617e-02, -1.8037e-01, -5.9022e-02, -5.2889e-02,  4.6451e-02,\n",
       "          1.3843e-01, -1.2509e-01,  6.5980e-02, -1.9412e-02,  8.8937e-02,\n",
       "         -8.0666e-02,  4.3589e-02, -1.7870e-01, -7.4802e-03,  8.0770e-02,\n",
       "          1.0898e-01, -1.6556e-02,  9.7671e-02, -1.6894e-01,  1.0011e-01,\n",
       "          1.3348e-01, -3.3382e-02, -2.6455e-02,  4.9345e-02, -1.8567e-03,\n",
       "         -2.6573e-01,  9.9391e-02,  2.0684e-01, -1.5813e-02,  6.2283e-02,\n",
       "         -1.0348e-01,  2.5368e-01, -8.7927e-02, -3.6937e-02, -2.9318e-02,\n",
       "          9.8028e-02, -1.4213e-02, -9.6586e-02,  5.4695e-02, -1.5071e-01,\n",
       "         -4.2884e-02,  2.1310e-02,  5.0642e-02,  3.1646e-02, -1.3338e-01,\n",
       "          1.0224e-01, -2.3936e-01, -6.6592e-02, -2.0546e-01, -1.2527e-01,\n",
       "          1.1854e-01, -1.9952e-02,  9.4907e-02, -1.0361e-01, -7.1104e-02,\n",
       "         -1.6286e-01, -6.7716e-02,  1.6073e-01,  5.2321e-02,  1.1655e-01,\n",
       "          1.4920e-01, -1.7601e-01, -4.5349e-02, -1.0495e-01,  1.7389e-02,\n",
       "         -2.6232e-02,  2.5699e-01,  2.4474e-02,  1.0477e-01, -7.6520e-02,\n",
       "          9.9629e-02, -8.4367e-02, -9.9720e-03,  1.4627e-01,  1.5038e-01,\n",
       "         -8.4939e-02, -3.8132e-02, -5.5359e-02, -1.5582e-01, -1.0076e-01,\n",
       "          1.2360e-02, -6.2420e-02,  1.6432e-01,  1.5075e-01, -5.8578e-02,\n",
       "         -4.8055e-02,  1.3318e-02,  1.2016e-01, -3.3128e-02,  1.2273e-01,\n",
       "          2.2809e-02, -3.5356e-02,  1.0881e-01,  2.1894e-01, -8.9597e-02,\n",
       "         -5.1360e-02, -4.0064e-02, -3.1258e-02, -2.8262e-02,  1.0273e-01,\n",
       "         -3.8696e-02,  9.5733e-02, -7.1016e-02, -2.5011e-03,  9.0620e-02,\n",
       "          8.7721e-02,  5.9105e-02,  1.4423e-01,  8.5796e-02, -4.0406e-02,\n",
       "         -1.1256e-01,  2.5545e-02,  1.5949e-01,  7.2592e-02,  9.6070e-02,\n",
       "          3.1162e-03, -5.0051e-02, -5.5794e-03, -5.9596e-03, -9.1406e-02,\n",
       "          1.7434e-01,  7.1470e-02,  1.0839e-01, -2.6532e-03,  1.6383e-02,\n",
       "         -4.1331e-02,  1.4214e-02,  1.1198e-02,  1.6541e-02,  8.2796e-02,\n",
       "         -4.0043e-02, -1.0858e-01,  4.2423e-02,  1.2854e-01,  7.3324e-03,\n",
       "         -1.8378e-01, -9.4893e-03, -1.1000e-01,  1.4518e-01,  1.2635e-01,\n",
       "          1.1003e-02, -8.8769e-02, -4.3025e-02,  2.7864e-02,  1.2064e-01,\n",
       "          1.0577e-01, -1.5479e-01,  1.0298e-01,  1.8620e-02,  2.3587e-01,\n",
       "          4.6578e-03, -7.9362e-02, -2.6344e-02, -1.1929e-01, -1.4926e-02,\n",
       "          1.8412e-01, -3.6151e-02, -2.1832e-01, -1.3871e-01, -3.2910e-02,\n",
       "          8.7190e-02, -3.3584e-02, -5.9691e-02,  8.6242e-04, -2.4600e-01,\n",
       "          4.5402e-02,  1.5846e-01,  1.1855e-01,  4.0482e-02, -5.7792e-02,\n",
       "         -1.0772e-01,  1.7057e-01,  1.4509e-01, -1.4244e-02,  8.8743e-02,\n",
       "         -1.6682e-02,  2.1597e-01, -7.3645e-02,  8.4626e-02, -2.0321e-01,\n",
       "          5.3152e-02, -4.1354e-02, -1.1845e-01, -6.6002e-02, -2.3721e-01,\n",
       "          2.0754e-01, -2.4641e-01,  1.1394e-01,  7.3031e-02, -2.1068e-01,\n",
       "         -9.9003e-02, -1.0719e-01, -3.6220e-02,  3.8718e-02, -4.2863e-02,\n",
       "         -4.2863e-02, -4.4179e-02,  3.2717e-01,  1.8560e-01, -3.3248e-02,\n",
       "         -7.4413e-02,  9.2814e-02,  2.3129e-02, -5.6121e-02,  5.1840e-02,\n",
       "          2.8309e-02, -1.3142e-01,  2.4845e-02,  7.2732e-02,  3.9975e-02,\n",
       "          1.7479e-01, -1.3004e-01,  2.3654e-02,  5.0932e-02, -1.2324e-01,\n",
       "          7.7021e-02,  4.7436e-02, -2.6862e-01,  4.3295e-02,  6.1174e-02,\n",
       "          1.2143e-01,  4.1469e-01, -1.9343e-01,  1.1031e-01,  2.1305e-02,\n",
       "          1.2391e-02, -1.6763e-01, -1.6243e-01,  1.0325e-02, -1.5955e-01,\n",
       "         -1.9574e-01, -1.2972e-01,  2.8366e-04,  2.0014e-01, -1.5067e-01,\n",
       "         -1.3287e-01, -9.0149e-02, -2.6842e-01,  7.0291e-02,  2.4570e-02,\n",
       "         -3.4946e-02, -1.5780e-01, -2.4727e-01,  2.2608e-01, -6.9311e-02,\n",
       "          1.6046e-01, -1.3901e-01, -3.2820e-02,  2.8448e-01,  1.9665e-01,\n",
       "          1.8996e-01, -5.9038e-02, -1.6209e-01, -7.8300e-02,  1.0704e-01,\n",
       "          1.1561e-01,  3.5261e-02,  1.1100e-01,  6.1467e-02,  6.9499e-02,\n",
       "          1.4634e-01, -1.5710e-01,  3.1434e-02, -8.9579e-02, -1.1669e-01,\n",
       "         -5.8055e-02, -4.0590e-02,  9.0771e-02, -8.0851e-02, -1.1806e-01,\n",
       "         -2.5185e-02,  1.4855e-02, -5.9336e-02, -6.5623e-02,  9.7041e-02,\n",
       "          2.7690e-01, -1.9987e-02, -2.8658e-02, -1.4633e-01, -8.7065e-03,\n",
       "         -3.3145e-02,  1.4271e-02, -1.1132e-01,  5.0628e-02, -7.1439e-02,\n",
       "          1.4279e-01,  2.0576e-01,  1.3376e-01, -1.3618e-01,  1.0161e-01,\n",
       "         -6.6048e-02,  1.7167e-01,  6.4942e-02, -2.0768e-01,  9.5287e-02,\n",
       "          6.1489e-03, -4.3719e-02,  1.4714e-01,  1.7106e-02,  2.5687e-01,\n",
       "         -6.8975e-02,  6.0700e-02,  1.1607e-01, -8.8286e-02, -2.5594e-02,\n",
       "          1.4360e-01, -9.5792e-02, -6.7379e-02,  1.5858e-01, -1.8343e-01,\n",
       "         -4.1773e-03,  1.1835e-01, -1.2830e-01, -4.7252e-03, -6.9979e-02,\n",
       "         -4.3474e-03, -1.5716e-01, -7.8922e-03,  5.4170e-02, -8.5557e-02,\n",
       "         -1.3081e-01, -1.4872e-01, -1.4497e-02,  2.2065e-01, -1.3079e-01,\n",
       "         -5.6204e-02,  1.4542e-01,  7.2989e-02,  2.0288e-01,  8.7270e-02,\n",
       "         -2.5098e-02, -1.4824e-01, -1.6793e-01, -1.1220e-02, -2.1143e-01,\n",
       "         -8.6873e-02,  1.9970e-01,  1.1379e-01, -2.0107e-02,  1.3206e-01,\n",
       "          2.0396e-01,  1.4635e-01,  5.7540e-02,  2.1541e-01,  1.1418e-01,\n",
       "         -5.5772e-02,  2.7615e-01, -1.5337e-01, -5.8670e-03,  2.5867e-01,\n",
       "         -6.3398e-02,  3.8931e-01,  2.4190e-01,  1.5852e-01, -1.0393e-02,\n",
       "          2.4114e-02,  1.8368e-01,  1.6601e-01, -5.4443e-02, -1.8606e-01,\n",
       "         -2.8720e-01, -5.0861e-02, -2.4094e-02, -9.5253e-04,  1.2215e-02,\n",
       "          1.4893e-03, -9.4679e-02, -1.0176e-01,  7.2016e-02, -1.4162e-01,\n",
       "          8.9151e-04, -5.5513e-02,  2.1170e-01, -4.1009e-03, -5.7914e-02,\n",
       "         -8.8402e-02,  5.2346e-03, -8.2523e-02,  2.2422e-02, -7.5668e-02,\n",
       "         -1.3940e-02,  7.4526e-02,  5.4616e-02, -8.9429e-02,  8.5130e-02,\n",
       "         -1.8144e-02, -1.8300e-03,  1.3815e-01, -2.0179e-01, -1.2929e-01,\n",
       "         -2.5350e-01, -3.8829e-02, -6.4619e-02, -9.1117e-02, -1.3473e-01,\n",
       "          1.5278e-01, -2.3307e-01,  5.3524e-02,  9.3714e-02, -1.2227e-02,\n",
       "          7.9154e-02,  1.2854e-01, -8.1355e-02, -2.4539e-01,  1.7300e-02,\n",
       "          1.5464e-01, -3.2712e-01,  1.0083e-01, -3.0230e-02,  7.0219e-02,\n",
       "         -1.2951e-01, -2.4582e-03,  1.6231e-01, -1.1312e-01, -5.8386e-02,\n",
       "          1.9637e-01,  3.1791e-02, -3.1436e-02, -2.4544e-02,  2.0815e-02,\n",
       "         -7.6786e-02, -1.9849e-02,  6.4449e-03, -5.7112e-02, -4.5437e-02,\n",
       "         -1.0695e-01,  1.0656e-01,  4.3160e-02, -2.0950e-01,  2.1037e-01,\n",
       "         -1.7241e-02,  5.1989e-02,  1.6865e-02,  7.8504e-02,  1.9873e-03,\n",
       "          1.2072e-01,  1.2359e-01, -1.5053e-01,  1.2013e-01, -6.5865e-03,\n",
       "          8.9148e-02,  2.5787e-01, -8.8401e-02,  1.0379e-01,  2.1067e-01,\n",
       "         -1.3900e-02, -8.2743e-02,  1.4271e-02,  1.0033e-01, -1.3111e-01,\n",
       "          1.2430e-01, -4.0163e-02, -1.1760e-01,  1.0495e-01, -2.5412e-01,\n",
       "          3.0550e-02,  2.1363e-01, -6.0252e-02, -9.7014e-02, -2.9208e-02,\n",
       "          1.3326e-01,  5.4465e-02,  6.5086e-02,  1.4199e-01, -3.0988e-02,\n",
       "          1.9695e-01,  2.3529e-02, -1.9665e-01,  1.0276e-01, -1.8946e-01,\n",
       "         -1.2714e-01,  1.4359e-01, -3.7140e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_model.encoder_question.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 2\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer_clipt5.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "boxes = torch.squeeze(boxes, dim=1)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_vlt5 = trainer_vlt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids = input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_clipT5 = trainer_clipt5.model.image_question_encoder.encoder.forward(\n",
    "    input_ids=input_ids,\n",
    "    vis_inputs=(vision_features,boxes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0044,  0.0702, -0.1362,  ..., -0.0093, -0.0669,  0.2152],\n",
       "         [-0.3346, -0.0321, -0.2023,  ..., -0.0346, -0.2422,  0.1325],\n",
       "         [-0.3988, -0.1186, -0.2928,  ..., -0.1005, -0.0245,  0.0725],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0135,  0.0049,  ...,  0.0231, -0.0006, -0.0090],\n",
       "         [ 0.0780, -0.0152,  0.0395,  ...,  0.0947,  0.0811,  0.1531],\n",
       "         [ 0.0100,  0.0192,  0.0097,  ...,  0.0264, -0.0009, -0.0107]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder_vlt5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = output_encoder_clipT5[0] == output_encoder_vlt5[0]\n",
    "torch.equal(output_encoder_clipT5[0],output_encoder_vlt5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VLT5 were not initialized from the model checkpoint at data_model/t5_pretrained and are newly initialized: ['encoder.visual_embedding.feat_embedding.0.weight', 'encoder.visual_embedding.feat_embedding.0.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.weight', 'encoder.visual_embedding.absolute_vis_pos_embedding.0.bias', 'encoder.visual_embedding.absolute_vis_pos_embedding.1.weight', 'encoder.visual_embedding.obj_order_embedding.weight', 'encoder.visual_embedding.img_order_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Model loaded from  data_model/whole_model/vlt5/VLT5Epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['encoder.visual_embedding.layer_norm.weight'])\n",
      "Configurations\n",
      "{'share_embedding': True, 'share_vis_embedding': True}\n"
     ]
    }
   ],
   "source": [
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/model_only/args_vlt5_wo_adap.json\"\n",
    "config_model = \"experiments/config_vladapter/model_only/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/model_only/training_vlt5.json\"\n",
    "\n",
    "trainer_model = Trainer(config_encoder_question, config_encoder_passage,config_model, config_training)\n",
    "\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cvlep.utils import device\n",
    "\n",
    "\n",
    "def get_feat(item):\n",
    "    key_vision_features = \"vlt5_features\"\n",
    "    key_boxes = \"vlt5_normalized_boxes\"\n",
    "    key_text=\"input\"\n",
    "    vision_features = torch.Tensor(item[key_vision_features])\n",
    "    boxes = torch.Tensor(item[key_boxes])\n",
    "    vision_features = torch.squeeze(vision_features, dim=0)\n",
    "    boxes = torch.squeeze(boxes, dim=0)\n",
    "    return {\"text\":item[key_text], \"feats\":vision_features, \"boxes\":boxes, \"size\":boxes.size()[0]}\n",
    "\n",
    "B = 2\n",
    "question1 = get_feat(dataset[0])\n",
    "relevant1 = get_feat(dataset[0])   \n",
    "irrelevant1 = get_feat(dataset[1])\n",
    "question2 = get_feat(dataset[2])\n",
    "relevant2 = get_feat(dataset[2])\n",
    "irrelevant2 = get_feat(dataset[3])\n",
    "\n",
    "item1 = {}\n",
    "item1[\"question_text\"] = question1[\"text\"]\n",
    "item1[\"passage_relevant_text\"] = relevant1['text']\n",
    "item1['passage_irrelevant_text'] = irrelevant1[\"text\"]\n",
    "item1[\"n_boxes_question\"] = question1[\"size\"]\n",
    "item1[\"n_boxes_passage_relevant\"] = relevant1[\"size\"]\n",
    "item1[\"n_boxes_passage_irrelevant\"] = irrelevant1[\"size\"]\n",
    "item1[\"question_image_features\"] = question1['feats']\n",
    "item1[\"question_image_boxes\"] = question1[\"boxes\"]\n",
    "item1[\"passage_relevant_image_features\"]=relevant1[\"feats\"]\n",
    "item1[\"passage_relevant_image_boxes\"]=relevant1[\"boxes\"]\n",
    "item1[\"passage_irrelevant_image_features\"]=irrelevant1['feats']\n",
    "item1[\"passage_irrelevant_image_boxes\"]=irrelevant1[\"boxes\"]\n",
    "\n",
    "item2 = {}\n",
    "item2[\"question_text\"] = question2[\"text\"]\n",
    "item2[\"passage_relevant_text\"] = relevant2['text']\n",
    "item2['passage_irrelevant_text'] = irrelevant2[\"text\"]\n",
    "item2[\"n_boxes_question\"] = question2[\"size\"]\n",
    "item2[\"n_boxes_passage_relevant\"] = relevant2[\"size\"]\n",
    "item2[\"n_boxes_passage_irrelevant\"] = irrelevant2[\"size\"]\n",
    "item2[\"question_image_features\"] = question2['feats']\n",
    "item2[\"question_image_boxes\"] = question2[\"boxes\"]\n",
    "item2[\"passage_relevant_image_features\"]=relevant2[\"feats\"]\n",
    "item2[\"passage_relevant_image_boxes\"]=relevant2[\"boxes\"]\n",
    "item2[\"passage_irrelevant_image_features\"]=irrelevant2['feats']\n",
    "item2[\"passage_irrelevant_image_boxes\"]=irrelevant2[\"boxes\"]\n",
    "\n",
    "batch = [item1, item2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "tokenizer = trainer_model.tokenizer_question\n",
    "V_L_question = max(item['n_boxes_question'] for item in batch)\n",
    "V_L_context = max(max(item['n_boxes_passage_relevant'],\n",
    "                    item['n_boxes_passage_irrelevant']) for item in batch)\n",
    "feat_dim = batch[0]['question_image_features'].shape[-1]\n",
    "# boxes are represented by 4 points\n",
    "question_boxes = torch.zeros(B, V_L_question, 4, dtype=torch.float)\n",
    "question_vis_feats = torch.zeros(\n",
    "    B, V_L_question, feat_dim, dtype=torch.float)\n",
    "relevant_boxes = torch.zeros(B, V_L_context, 4, dtype=torch.float)\n",
    "relevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "irrelevant_boxes = torch.zeros(\n",
    "    B, V_L_context, 4, dtype=torch.float)\n",
    "irrelevant_vis_feats = torch.zeros(\n",
    "    B, V_L_context, feat_dim, dtype=torch.float)\n",
    "\n",
    "relevant_text, irrelevant_text, question_text, labels = list(), list(), list(), list()\n",
    "for i, item in enumerate(batch):\n",
    "    # TODO: voir si besoin de changer gestion pour le text, car a un impact sur attention mask\n",
    "    question_text.append(item['question_text'])\n",
    "    relevant_text.append(item['passage_relevant_text'])\n",
    "    irrelevant_text.append(item['passage_irrelevant_text'])\n",
    "    if tokenizer:\n",
    "        n_boxes_relevant = item['n_boxes_passage_relevant']\n",
    "        n_boxes_irrelevant = item['n_boxes_passage_irrelevant']\n",
    "        n_boxes_question = item['n_boxes_question']\n",
    "        question_boxes[i,\n",
    "                        :n_boxes_question] = item['question_image_boxes']\n",
    "        question_vis_feats[i,\n",
    "                            :n_boxes_question] = item['question_image_features']\n",
    "        relevant_boxes[i,\n",
    "                        :n_boxes_relevant] = item['passage_relevant_image_boxes']\n",
    "        relevant_vis_feats[i,\n",
    "                            :n_boxes_relevant] = item['passage_relevant_image_features']\n",
    "        irrelevant_boxes[i,\n",
    "                            :n_boxes_irrelevant] = item['passage_irrelevant_image_boxes']\n",
    "        irrelevant_vis_feats[i,\n",
    "                                :n_boxes_irrelevant] = item['passage_irrelevant_image_features']\n",
    "    if item['passage_relevant_text'] is None:\n",
    "        labels.append(-100)  # ignore index when computing the loss\n",
    "    else:\n",
    "        labels.append(i)\n",
    "\n",
    "question_input = tokenizer(\n",
    "    question_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "context_input = tokenizer(\n",
    "    relevant_text + irrelevant_text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "visual_feats_context = torch.concat(\n",
    "    [relevant_vis_feats, irrelevant_vis_feats])\n",
    "context_image_boxes = torch.concat(\n",
    "    [relevant_boxes, irrelevant_boxes])\n",
    "results = {\n",
    "    \"input_ids_question\": question_input.input_ids,\n",
    "    \"attention_mask_question\": question_input.attention_mask,\n",
    "    \"input_ids_context\": context_input.input_ids,\n",
    "    \"attention_mask_context\": context_input.attention_mask,\n",
    "    \"labels\": labels,\n",
    "    \"visual_feats_question\": question_vis_feats,\n",
    "    \"visual_feats_context\": visual_feats_context,\n",
    "    \"question_image_boxes\": question_boxes,\n",
    "    \"context_image_boxes\": context_image_boxes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1641)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss doit être faible car on met exactement même embedding pour question \n",
    "# et relevant passage\n",
    "trainer_model.model.train_step(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3]), tensor([   0,    1, -100,    3])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "loss_fct = nn.NLLLoss(reduction='mean')\n",
    "batch1 ={\n",
    "\"labels\": torch.tensor([0,1,2,3])\n",
    "}\n",
    "batch2 = {\n",
    "\"labels\": torch.tensor([0,1,-100,3])\n",
    "}\n",
    "labels1 = batch1.pop('labels')\n",
    "labels2 = batch2.pop('labels')\n",
    "labels_gatherer = [labels1, labels2]\n",
    "gatherers = zip(labels_gatherer)\n",
    "labels_gatherer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3]), tensor([   8,    9, -100,   11])]\n"
     ]
    }
   ],
   "source": [
    "# N questions taille du batch\n",
    "N = 4\n",
    "# nombre relevant et irrelevant 1 et 1\n",
    "global_labels = []\n",
    "label_shift = 0\n",
    "for i, (received_labels) in enumerate(gatherers):\n",
    "    received_labels = received_labels[0]\n",
    "    received_labels[received_labels!=-100] += label_shift\n",
    "    label_shift += 4 * 2 # N * M\n",
    "    global_labels.append(received_labels)\n",
    "print(global_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemple d'un batch de 1\n",
    "```\n",
    "| question1 | * [relevant1, irr1, relevant2, irr2]\n",
    "| question2 |\n",
    "```\n",
    "label devra donc etre [0, 2]\n",
    "ce qui explique le besoin du shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5051658153533936"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " epoch 0 | Loss 2.5: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 1 | Loss 0.625: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 2 | Loss 0.15625: 100%|████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      " epoch 3 | Loss 0.0390625: 100%|██████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "loss = 10\n",
    "for epoch in range(4):\n",
    "    pbar = tqdm(total=2, ncols=120)\n",
    "    for batch in range(2):\n",
    "        loss *= 0.5\n",
    "        time.sleep(1)\n",
    "        desc_str = f' epoch {epoch} | Loss {loss}'\n",
    "        pbar.set_description(desc_str)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvlep.VLT5.param import Config\n",
    "config = Config.load_json(\"experiments/config_vladapter/prompt/training_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rank = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/sentence-t5-base\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "model = SentenceTransformer(\"sentence-transformers/sentence-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.auto_model.shared.weight',\n",
       "              tensor([[ -0.7539,   0.5977,  -2.4375,  ...,   1.2500,  -0.7891,   3.5156],\n",
       "                      [  4.5000,  -2.0625,   4.1250,  ...,   1.4688,   5.5938,  -2.1562],\n",
       "                      [-13.5000,   9.3125, -22.2500,  ...,  12.8750,  27.3750,  21.8750],\n",
       "                      ...,\n",
       "                      [  2.2344,   6.7500, -11.0625,  ..., -11.3125,  13.5625,  16.6250],\n",
       "                      [  4.2500,   5.1250, -12.2500,  ..., -11.9375,  13.5000,  17.0000],\n",
       "                      [  4.0625,   6.9688, -12.2500,  ..., -11.3750,  11.9375,  16.6250]])),\n",
       "             ('0.auto_model.encoder.embed_tokens.weight',\n",
       "              tensor([[ -0.7539,   0.5977,  -2.4375,  ...,   1.2500,  -0.7891,   3.5156],\n",
       "                      [  4.5000,  -2.0625,   4.1250,  ...,   1.4688,   5.5938,  -2.1562],\n",
       "                      [-13.5000,   9.3125, -22.2500,  ...,  12.8750,  27.3750,  21.8750],\n",
       "                      ...,\n",
       "                      [  2.2344,   6.7500, -11.0625,  ..., -11.3125,  13.5625,  16.6250],\n",
       "                      [  4.2500,   5.1250, -12.2500,  ..., -11.9375,  13.5000,  17.0000],\n",
       "                      [  4.0625,   6.9688, -12.2500,  ..., -11.3750,  11.9375,  16.6250]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0752, -0.0430,  0.0231,  ...,  0.0157, -0.0518, -0.0527],\n",
       "                      [ 0.0471,  0.0020, -0.0045,  ..., -0.0063, -0.0242, -0.0145],\n",
       "                      [-0.0081, -0.0192, -0.0337,  ..., -0.0182,  0.0527, -0.0498],\n",
       "                      ...,\n",
       "                      [ 0.0143, -0.0469,  0.0437,  ...,  0.0447,  0.0311, -0.0134],\n",
       "                      [-0.0102,  0.0187, -0.0284,  ...,  0.0398, -0.0503, -0.0437],\n",
       "                      [-0.0179,  0.0549,  0.0012,  ..., -0.0171,  0.0420, -0.0417]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.5391, -0.1689, -0.0332,  ...,  0.1133, -0.7031,  0.1074],\n",
       "                      [ 0.4102,  0.1807,  0.1719,  ..., -0.2910, -0.1406, -0.3711],\n",
       "                      [-0.0786, -0.1196,  0.0510,  ..., -0.1006,  0.2217, -0.6953],\n",
       "                      ...,\n",
       "                      [ 0.4707, -0.0074,  0.3281,  ...,  0.1279,  0.2236,  0.0693],\n",
       "                      [-0.1689,  0.6641,  0.1562,  ..., -0.5039,  0.4844,  0.1953],\n",
       "                      [ 0.1230,  0.2324,  0.1147,  ..., -0.0532, -0.1475, -0.2178]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.4219,  0.9062, -0.4102,  ...,  0.3574,  0.0479, -0.2168],\n",
       "                      [ 0.1445, -0.0649,  0.0347,  ..., -0.2812,  0.1680,  0.1543],\n",
       "                      [ 0.3066, -0.5703,  0.0562,  ...,  0.2598,  0.2891,  0.4004],\n",
       "                      ...,\n",
       "                      [-0.1504, -0.4102, -0.1680,  ..., -0.0437, -0.0015, -0.3066],\n",
       "                      [-0.3984,  0.3477,  0.5000,  ...,  0.1055,  0.2891, -0.0045],\n",
       "                      [-0.2432, -0.0894,  0.3164,  ..., -0.0447, -0.1318, -0.1157]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.4629, -0.2754, -0.4707,  ...,  0.0972,  0.3887, -0.2734],\n",
       "                      [-0.9492,  0.0513,  0.4922,  ...,  0.0991, -0.2207, -0.5273],\n",
       "                      [ 0.3789, -0.3418, -0.1089,  ...,  0.2158,  0.7930,  0.4238],\n",
       "                      ...,\n",
       "                      [-0.4707,  0.0679, -0.2412,  ..., -0.2129, -0.4141, -0.0039],\n",
       "                      [ 0.0442,  0.0586, -0.6406,  ...,  0.2812, -0.0364, -0.3086],\n",
       "                      [ 0.1562, -0.2012, -0.5156,  ..., -0.1289,  0.0139,  0.2070]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       "              tensor([[ 2.3281e+00, -1.5469e+00,  6.9922e-01, -1.0156e+00, -4.4688e+00,\n",
       "                        2.5156e+00, -1.8000e+01, -5.7500e+00, -7.8125e-01,  6.7969e-01,\n",
       "                        8.7500e-01, -7.8125e+00],\n",
       "                      [ 8.7109e-01,  4.6875e+00,  3.2031e+00,  1.3047e+00,  3.0781e+00,\n",
       "                        9.1797e-01,  6.8359e-01,  8.7500e-01,  4.9375e+00, -2.7250e+01,\n",
       "                        1.1279e-01,  2.9883e-01],\n",
       "                      [ 8.8672e-01,  2.6250e+00,  2.9062e+00,  1.5312e+00,  3.0156e+00,\n",
       "                        6.9141e-01,  1.0781e+00,  1.0312e+00,  4.0312e+00, -1.2750e+01,\n",
       "                       -3.4766e-01,  6.4453e-01],\n",
       "                      [ 6.5625e-01,  1.4531e+00,  3.0156e+00,  1.6328e+00,  2.9688e+00,\n",
       "                        5.0391e-01,  1.3125e+00,  1.0547e+00,  3.3594e+00, -2.1875e+01,\n",
       "                       -3.7109e-01,  7.4609e-01],\n",
       "                      [ 6.2891e-01,  8.0469e-01,  2.8281e+00,  1.4688e+00,  2.8906e+00,\n",
       "                        1.0234e+00,  1.5469e+00,  8.7891e-01,  2.8906e+00, -6.3125e+00,\n",
       "                        4.0283e-03,  7.9297e-01],\n",
       "                      [ 9.6484e-01,  3.8672e-01,  2.7656e+00,  1.5312e+00,  2.7812e+00,\n",
       "                        5.7422e-01,  1.5391e+00,  1.0312e+00,  2.3906e+00, -6.6875e+00,\n",
       "                       -4.3555e-01,  9.2188e-01],\n",
       "                      [ 8.3984e-01, -4.9072e-02,  2.5312e+00,  1.5312e+00,  2.9062e+00,\n",
       "                        6.0547e-01,  1.5859e+00,  9.1406e-01,  2.0625e+00, -7.1250e+00,\n",
       "                       -2.3730e-01,  1.0156e+00],\n",
       "                      [ 7.5391e-01, -4.5312e-01,  2.6875e+00,  1.3750e+00,  2.6719e+00,\n",
       "                        1.4062e-01,  1.7891e+00,  7.6172e-01,  1.5781e+00, -7.6875e+00,\n",
       "                       -3.4375e-01,  1.1016e+00],\n",
       "                      [ 7.8906e-01, -1.0000e+00,  2.4531e+00,  1.4922e+00,  2.6406e+00,\n",
       "                        3.6523e-01,  1.6641e+00,  4.8047e-01,  8.9844e-01, -8.5000e+00,\n",
       "                       -2.6367e-01,  1.2109e+00],\n",
       "                      [ 7.3438e-01, -9.8828e-01,  2.3125e+00,  1.2266e+00,  2.5781e+00,\n",
       "                        2.9492e-01,  1.6328e+00,  9.7656e-02,  1.0547e-01, -9.1250e+00,\n",
       "                       -3.7305e-01,  1.0312e+00],\n",
       "                      [ 6.6406e-01, -1.1172e+00,  2.3125e+00,  1.1797e+00,  2.3438e+00,\n",
       "                        3.2617e-01,  1.7422e+00, -2.2559e-01, -8.1250e-01, -9.2500e+00,\n",
       "                       -4.8438e-01,  1.4375e+00],\n",
       "                      [ 5.3516e-01, -1.3047e+00,  2.0469e+00,  1.0938e+00,  2.2812e+00,\n",
       "                        2.4902e-01,  2.0312e+00, -6.2500e-01, -1.9688e+00, -9.2500e+00,\n",
       "                       -7.1094e-01,  1.4219e+00],\n",
       "                      [ 6.0156e-01, -1.4766e+00,  1.7578e+00,  1.0000e+00,  2.1250e+00,\n",
       "                        1.9922e-01,  1.9297e+00, -9.2188e-01, -2.6406e+00, -1.0812e+01,\n",
       "                       -9.8047e-01,  1.4375e+00],\n",
       "                      [ 6.1719e-01, -1.6641e+00,  1.6016e+00,  9.9609e-01,  2.0000e+00,\n",
       "                        6.7383e-02,  1.9453e+00, -1.2734e+00, -3.5156e+00, -1.1188e+01,\n",
       "                       -1.5156e+00,  1.4609e+00],\n",
       "                      [ 6.9922e-01, -1.5703e+00,  1.7500e+00,  8.0469e-01,  2.0000e+00,\n",
       "                        2.9102e-01,  2.1094e+00, -1.8594e+00, -4.4375e+00, -2.3125e+00,\n",
       "                       -4.5117e-01,  1.7266e+00],\n",
       "                      [ 8.7891e-01, -1.3438e+00, -2.8250e+01,  8.2031e-01,  2.0469e+00,\n",
       "                        4.9219e-01,  2.2969e+00, -2.2812e+00, -6.0312e+00,  7.6562e-01,\n",
       "                       -6.2109e-01,  1.9297e+00],\n",
       "                      [ 3.5742e-01, -2.5977e-01,  3.5352e-01,  9.7656e-02, -3.6328e-01,\n",
       "                        2.5195e-01,  2.6367e-01, -2.6562e-01, -1.2695e-01, -1.2109e-01,\n",
       "                       -3.5742e-01, -5.2734e-02],\n",
       "                      [ 9.5312e-01, -6.7969e-01,  4.0000e+00,  1.1875e+00,  3.1719e+00,\n",
       "                        2.1875e+00,  1.1328e+00, -3.4844e+00, -2.1719e+00,  5.3125e+00,\n",
       "                        6.3750e+00,  3.5352e-01],\n",
       "                      [ 1.0156e+00, -7.1484e-01,  3.6406e+00,  1.4219e+00,  2.9219e+00,\n",
       "                        1.3281e+00,  1.3984e+00, -3.5469e+00, -2.2500e+00,  5.1875e+00,\n",
       "                        4.7188e+00,  7.3438e-01],\n",
       "                      [ 1.1875e+00, -6.6797e-01,  3.2344e+00,  1.4844e+00,  2.9062e+00,\n",
       "                        9.2969e-01,  1.5078e+00, -3.5781e+00, -2.2812e+00,  4.8125e+00,\n",
       "                        3.7500e+00,  7.4609e-01],\n",
       "                      [ 1.1250e+00, -7.1094e-01,  3.2188e+00,  1.4375e+00,  2.9375e+00,\n",
       "                        8.5938e-01,  1.5703e+00, -3.6406e+00, -2.1406e+00,  4.4375e+00,\n",
       "                        3.0625e+00,  9.1406e-01],\n",
       "                      [ 9.4141e-01, -7.2656e-01,  3.1250e+00,  1.4609e+00,  2.7656e+00,\n",
       "                        8.6328e-01,  1.7109e+00, -3.8438e+00, -2.2656e+00,  4.1250e+00,\n",
       "                        2.5312e+00,  8.1250e-01],\n",
       "                      [ 1.0156e+00, -6.8750e-01,  2.9375e+00,  1.3672e+00,  2.6562e+00,\n",
       "                        6.7578e-01,  1.7109e+00, -3.5781e+00, -2.2188e+00,  3.8281e+00,\n",
       "                        2.2031e+00,  1.0000e+00],\n",
       "                      [ 8.5156e-01, -7.6562e-01,  2.9688e+00,  1.2734e+00,  2.8125e+00,\n",
       "                        6.5234e-01,  1.7656e+00, -3.9219e+00, -2.0312e+00,  3.5469e+00,\n",
       "                        1.8203e+00,  9.1016e-01],\n",
       "                      [ 8.5938e-01, -5.6641e-01,  2.6875e+00,  1.2812e+00,  2.6562e+00,\n",
       "                        7.3438e-01,  1.6641e+00, -3.6406e+00, -1.9219e+00,  3.1406e+00,\n",
       "                        1.5469e+00,  8.9062e-01],\n",
       "                      [ 7.1094e-01, -8.3984e-01,  2.6562e+00,  1.3047e+00,  2.3750e+00,\n",
       "                        2.6367e-02,  1.5703e+00, -3.7812e+00, -1.7891e+00,  2.7188e+00,\n",
       "                        1.0000e+00,  1.1875e+00],\n",
       "                      [ 7.5781e-01, -7.0703e-01,  2.2656e+00,  1.0312e+00,  2.3125e+00,\n",
       "                        8.3496e-02,  1.6562e+00, -3.5156e+00, -2.0156e+00,  2.3906e+00,\n",
       "                        7.6562e-01,  8.9062e-01],\n",
       "                      [ 6.4844e-01, -1.0156e+00,  2.1406e+00,  7.9688e-01,  2.0938e+00,\n",
       "                        1.7578e-01,  1.7734e+00, -3.6562e+00, -2.1875e+00,  1.8125e+00,\n",
       "                        4.8047e-01,  9.8047e-01],\n",
       "                      [ 6.0547e-01, -1.1719e+00,  1.8594e+00,  8.6328e-01,  2.0938e+00,\n",
       "                       -8.2031e-02,  1.8047e+00, -3.7031e+00, -2.1250e+00,  1.4609e+00,\n",
       "                        1.7188e-01,  1.0781e+00],\n",
       "                      [ 7.4609e-01, -1.0547e+00,  1.7734e+00,  4.9414e-01,  1.9531e+00,\n",
       "                       -2.7222e-02,  1.5859e+00, -3.8281e+00, -2.1094e+00,  1.0391e+00,\n",
       "                       -6.1035e-03,  1.1484e+00],\n",
       "                      [ 4.7266e-01, -1.1641e+00,  1.4453e+00,  6.9922e-01,  1.7656e+00,\n",
       "                       -1.5918e-01,  1.7656e+00, -3.7812e+00, -2.0781e+00,  6.4453e-01,\n",
       "                       -2.6562e-01,  1.1172e+00],\n",
       "                      [ 4.1602e-01, -1.1406e+00,  1.0078e+00,  5.7812e-01,  1.6094e+00,\n",
       "                       -2.1582e-01,  1.7031e+00, -3.6719e+00, -2.0625e+00,  1.2207e-01,\n",
       "                       -4.4531e-01,  1.1094e+00]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.0972,  0.1021,  0.1099,  0.0664,  0.0796,  0.0864,  0.1221,  0.1094,\n",
       "                       0.0991,  0.0864,  0.0830,  0.1157,  0.1250,  0.1201,  0.0820,  0.0864,\n",
       "                       0.1196,  0.1016,  0.1445,  0.0840,  0.0854,  0.0933,  0.1201,  0.0708,\n",
       "                       0.0781,  0.1309,  0.0811,  0.1377,  0.1045,  0.0845,  0.0923,  0.0737,\n",
       "                       0.0972,  0.0742,  0.1118,  0.0981,  0.1309,  0.0894,  0.0757,  0.1245,\n",
       "                       0.1270,  0.1147,  0.1025,  0.1074,  0.0508,  0.0869,  0.1572,  0.0879,\n",
       "                       0.0908,  0.1230,  0.1338,  0.0996,  0.1230,  0.0820,  0.0322,  0.0918,\n",
       "                       0.0371,  0.1001,  0.0977,  0.0884,  0.1094,  0.0830,  0.0957,  0.0889,\n",
       "                       0.0947,  0.0356,  0.0986,  0.0947,  0.2236,  0.0796,  0.1064,  0.1152,\n",
       "                       0.0845,  0.0903,  0.0913,  0.0850,  0.0547,  0.0879,  0.0576,  0.1216,\n",
       "                       0.0991,  0.1094,  0.0864,  0.1348,  0.0986,  0.0981,  0.1074,  0.1035,\n",
       "                       0.0942,  0.0796,  0.0957,  0.0752,  0.0825,  0.0962,  0.0786,  0.0894,\n",
       "                       0.0947,  0.0913,  0.0796,  0.0713,  0.1187,  0.0869,  0.1118,  0.0781,\n",
       "                       0.0884,  0.1689,  0.0913,  0.0957,  0.0732,  0.1348,  0.1074,  0.0908,\n",
       "                       0.0806,  0.0908,  0.0967,  0.0928,  0.0854,  0.0679,  0.3555,  0.0942,\n",
       "                       0.0742,  0.0869,  0.1699,  0.0913,  0.1240,  0.0933,  0.0952,  0.0884,\n",
       "                       0.0947,  0.1216,  0.1357,  0.1079,  0.0762,  0.0757,  0.0815,  0.1318,\n",
       "                       0.1025,  0.1060,  0.1021,  0.0918,  0.0981,  0.1162,  0.1235,  0.1992,\n",
       "                       0.1025,  0.1060,  0.0923,  0.1475,  0.0864,  0.0962,  0.0845,  0.0864,\n",
       "                       0.1025,  0.1631,  0.0845,  0.0820,  0.1201,  0.0791,  0.1079,  0.0854,\n",
       "                       0.0786,  0.0889,  0.1162,  0.1377,  0.0620,  0.0593,  0.0830,  0.1289,\n",
       "                       0.0762,  0.1099,  0.1748,  0.0894,  0.0967,  0.1426,  0.0981,  0.1128,\n",
       "                       0.0996,  0.1377,  0.1338,  0.1206,  0.0942,  0.0679,  0.0864,  0.0806,\n",
       "                       0.1299,  0.1416,  0.0830,  0.1074,  0.0884,  0.0972,  0.0913,  0.0913,\n",
       "                       0.0693,  0.0835,  0.0791,  0.1006,  0.0840,  0.1084,  0.0933,  0.0854,\n",
       "                       0.0781,  0.0879,  0.1021,  0.0864,  0.0972, -0.1289,  0.0894,  0.0923,\n",
       "                       0.0796,  0.0903,  0.1216,  0.0977,  0.0972,  0.1167,  0.0835,  0.1172,\n",
       "                       0.0918,  0.0942,  0.1030,  0.1377,  0.0659,  0.0894,  0.0757,  0.0928,\n",
       "                       0.0977,  0.0894,  0.0679,  0.1299,  0.0815,  0.1138,  0.0884,  0.0962,\n",
       "                       0.1279,  0.0776,  0.0835,  0.0962,  0.1211,  0.1030,  0.0957,  0.1240,\n",
       "                       0.1123,  0.1167,  0.0864,  0.0967,  0.0713,  0.0708,  0.1011,  0.1011,\n",
       "                       0.0630,  0.1001,  0.1045,  0.0713,  0.0894,  0.0830,  0.0894,  0.0879,\n",
       "                       0.0942,  0.0698,  0.0977,  0.0806,  0.1279,  0.1523,  0.1030,  0.0913,\n",
       "                       0.0884,  0.0830,  0.0864,  0.0928,  0.1016,  0.0840,  0.0417,  0.1514,\n",
       "                       0.1206,  0.0923,  0.1719,  0.1123,  0.0952,  0.1226,  0.1006,  0.0957,\n",
       "                      -0.1338,  0.2051,  0.0928,  0.1074,  0.1050,  0.1377,  0.0845,  0.0879,\n",
       "                       0.0864,  0.1758,  0.0728,  0.0977,  0.0776,  0.0864,  0.0933,  0.0613,\n",
       "                       0.1206,  0.0952,  0.0757,  0.1089,  0.1128,  0.0894,  0.1982,  0.1099,\n",
       "                       0.1069,  0.0869,  0.0869,  0.0879,  0.1211,  0.0771,  0.0825,  0.1157,\n",
       "                       0.0708,  0.1436,  0.1260,  0.0854,  0.1167,  0.1572,  0.0996,  0.0718,\n",
       "                       0.0664,  0.1177,  0.0967,  0.0742,  0.1094,  0.0845,  0.1064,  0.1504,\n",
       "                       0.1094,  0.0991,  0.1074,  0.1162,  0.1118,  0.0884,  0.0903,  0.0889,\n",
       "                       0.0801,  0.0786,  0.0854,  0.1768,  0.1050,  0.1138,  0.1021,  0.0889,\n",
       "                       0.0977,  0.0850,  0.1187,  0.0894,  0.0864,  0.1182,  0.1299,  0.0491,\n",
       "                       0.0801,  0.0942,  0.1045,  0.1050,  0.0962,  0.2422,  0.0835,  0.1177,\n",
       "                       0.0820,  0.1309,  0.0845,  0.0603,  0.0850,  0.0938,  0.1357,  0.1084,\n",
       "                       0.0850,  0.0962,  0.1216,  0.1187,  0.1543,  0.0972,  0.1162,  0.0718,\n",
       "                       0.1230,  0.0515,  0.1309,  0.1729,  0.0903,  0.0962,  0.2041,  0.0957,\n",
       "                       0.0894,  0.1406,  0.0884,  0.1270,  0.0811,  0.1104,  0.0923,  0.1187,\n",
       "                       0.0825,  0.2031,  0.1240,  0.1084,  0.1079,  0.0830,  0.0903,  0.0776,\n",
       "                       0.0762,  0.1445,  0.0903,  0.1094,  0.0747,  0.0913,  0.1475,  0.0869,\n",
       "                       0.0728,  0.0786,  0.0918,  0.0757,  0.0996,  0.1001,  0.0859,  0.0884,\n",
       "                       0.0869,  0.0791,  0.1226,  0.1152,  0.1206,  0.0981,  0.0801,  0.2832,\n",
       "                       0.0977,  0.0898,  0.1187,  0.0771,  0.0996,  0.0962,  0.1177,  0.0859,\n",
       "                       0.1416,  0.0762,  0.1250,  0.0859,  0.0938,  0.0903,  0.1123,  0.1387,\n",
       "                       0.1133,  0.1475,  0.1089,  0.1226,  0.0771,  0.1021,  0.0820,  0.1069,\n",
       "                       0.1904,  0.0903, -0.1025,  0.0913,  0.0918,  0.0884,  0.0957,  0.1270,\n",
       "                       0.1309,  0.0530,  0.1138,  0.0757,  0.0874,  0.0752,  0.0928,  0.1001,\n",
       "                       0.1133,  0.0903,  0.1060,  0.0889,  0.0684,  0.0825,  0.0679,  0.0898,\n",
       "                       0.0364,  0.0659,  0.0894,  0.0928,  0.0962,  0.1025,  0.1289,  0.0913,\n",
       "                       0.1143,  0.1338,  0.1069,  0.0864,  0.1826,  0.0830,  0.0742,  0.0986,\n",
       "                       0.0879,  0.0977,  0.0933,  0.1025,  0.1128,  0.0957,  0.0981,  0.1006,\n",
       "                       0.0879,  0.1045,  0.0698,  0.0791,  0.0708,  0.1270,  0.1157,  0.1338,\n",
       "                       0.0972,  0.1260,  0.1040,  0.1050,  0.0752,  0.0962,  0.0967,  0.0991,\n",
       "                       0.0732,  0.0776,  0.1099,  0.0962,  0.0957,  0.0820,  0.2871,  0.1099,\n",
       "                       0.0811,  0.0898,  0.0732,  0.0981,  0.0996,  0.1221,  0.1108,  0.1709,\n",
       "                       0.0962,  0.1133,  0.1230,  0.0859,  0.1025,  0.0986,  0.0928,  0.1206,\n",
       "                       0.0986,  0.0776,  0.1118,  0.1055,  0.1138,  0.0933,  0.1035,  0.1270,\n",
       "                       0.0996,  0.0850,  0.1069,  0.0918,  0.0796,  0.1270,  0.1279,  0.2109,\n",
       "                       0.0732,  0.1895,  0.0757,  0.1030,  0.1138,  0.0806,  0.0972,  0.1084,\n",
       "                       0.1113,  0.0820,  0.0806,  0.0977,  0.0933,  0.1650,  0.1436,  0.0962,\n",
       "                       0.1206,  0.0796,  0.1021,  0.1270,  0.0747,  0.0884,  0.0859,  0.1108,\n",
       "                       0.0977,  0.1299,  0.0747,  0.2930,  0.1011,  0.0742,  0.0894,  0.0801,\n",
       "                       0.0593,  0.0811,  0.0835,  0.0747,  0.0762,  0.0781,  0.0854,  0.1191,\n",
       "                       0.1025,  0.0869,  0.0864,  0.0991,  0.0903,  0.1030,  0.1494,  0.0981,\n",
       "                       0.1211,  0.0962,  0.0967,  0.0967,  0.1030,  0.0786,  0.0728, -0.1035,\n",
       "                       0.1123,  0.0884,  0.0913,  0.0737,  0.1069,  0.0796,  0.0645,  0.1040,\n",
       "                       0.0884,  0.0260,  0.1001,  0.1021,  0.1030,  0.0835,  0.1040,  0.1338,\n",
       "                       0.0933,  0.0791,  0.0859,  0.1318,  0.1001,  0.1289,  0.1089,  0.0938,\n",
       "                       0.1167,  0.1226,  0.0903,  0.1387,  0.1011,  0.1045,  0.1152,  0.0811,\n",
       "                       0.1299,  0.0908,  0.0664,  0.0986,  0.1040,  0.0962,  0.0449,  0.0801,\n",
       "                       0.0588,  0.0986,  0.0811,  0.0791,  0.1055,  0.1162,  0.1250,  0.1201,\n",
       "                       0.0923,  0.0972,  0.1006,  0.1172,  0.1289,  0.1250,  0.0933,  0.2051,\n",
       "                       0.0811,  0.1089,  0.1021,  0.0811,  0.0942,  0.0942,  0.0898,  0.1074,\n",
       "                       0.0938,  0.0688,  0.1094,  0.0942,  0.1001,  0.0864,  0.0933,  0.1006,\n",
       "                       0.0986,  0.0972,  0.1006,  0.0610,  0.0884,  0.0623,  0.0854,  0.1084,\n",
       "                       0.0742,  0.1030,  0.0840,  0.0996,  0.1797,  0.0830,  0.0957,  0.0830,\n",
       "                       0.1035,  0.0903,  0.0977,  0.1006,  0.0996,  0.0664,  0.1011,  0.0427,\n",
       "                       0.0972,  0.0796,  0.0791,  0.1030,  0.1797,  0.0815,  0.1216,  0.1035,\n",
       "                       0.0889,  0.0874,  0.2119,  0.0908,  0.1069,  0.1235,  0.0791,  0.1133,\n",
       "                       0.0835,  0.1177,  0.1357,  0.0845,  0.0962,  0.0469,  0.1089,  0.0493,\n",
       "                       0.0767,  0.1172,  0.0928,  0.0903,  0.0874,  0.0811,  0.1064,  0.0918,\n",
       "                       0.1006,  0.0811,  0.0908,  0.1196,  0.0986,  0.1299,  0.0957,  0.1396,\n",
       "                       0.0801,  0.1094,  0.0933,  0.0801,  0.1250,  0.1094,  0.0835,  0.0835,\n",
       "                       0.1426,  0.1021,  0.1104,  0.0991,  0.1206,  0.0579,  0.1514,  0.0996,\n",
       "                       0.0938,  0.0640,  0.0898,  0.0732,  0.1035,  0.1055,  0.0806,  0.1069])),\n",
       "             ('0.auto_model.encoder.block.0.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.3379,  0.1260,  0.2119,  ...,  0.5938, -0.4121,  0.0079],\n",
       "                      [ 0.2012, -0.3262, -0.4668,  ..., -0.7656,  0.1934, -0.2266],\n",
       "                      [-0.0757, -0.1172, -0.0693,  ...,  0.7422, -0.0013, -0.0864],\n",
       "                      ...,\n",
       "                      [ 0.3281, -0.4570, -0.0737,  ..., -0.4043,  0.8242, -0.3770],\n",
       "                      [ 0.1982,  0.0830,  0.3906,  ...,  0.1641, -0.5820, -0.7656],\n",
       "                      [-0.3281, -0.3145,  0.1719,  ...,  0.3906, -0.2002, -0.4238]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.1572,  0.1826, -0.0410,  ..., -0.1699,  0.0918, -0.3711],\n",
       "                      [-0.2061, -0.1660,  0.1572,  ..., -0.0286,  0.0835,  0.2891],\n",
       "                      [-0.1504, -0.0070,  0.2930,  ...,  0.0649,  0.1553, -0.1680],\n",
       "                      ...,\n",
       "                      [-0.2158, -0.2695,  0.1133,  ...,  0.0752, -0.0742, -0.2490],\n",
       "                      [-0.3906, -0.2461, -0.0046,  ...,  0.0579,  0.2256, -0.1143],\n",
       "                      [ 0.1196,  0.0933, -0.0244,  ..., -0.1582,  0.1001,  0.1040]])),\n",
       "             ('0.auto_model.encoder.block.0.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.2539,  0.3730,  0.3398,  0.3828,  0.1680,  0.2363,  0.2139,  0.2363,\n",
       "                       0.3457,  0.2041,  0.2432,  0.2295,  0.3164,  0.3633,  0.1787,  0.1641,\n",
       "                       0.2969,  0.2754, -0.3828,  0.1797,  0.1934,  0.2188,  0.1196,  0.1748,\n",
       "                       0.1689,  0.3125,  0.1797,  0.3828,  0.1982,  0.1865,  0.1885,  0.4434,\n",
       "                       0.2256,  0.1602,  0.2246,  0.2031,  0.2871,  0.4961,  0.1523,  0.2754,\n",
       "                       0.3359,  0.3008,  0.2334,  0.2139, -0.6484,  0.2119,  0.3672,  0.1963,\n",
       "                       0.1914,  0.3145,  0.3340,  0.2207,  0.2812,  0.2031,  0.0693,  0.2295,\n",
       "                       0.2930,  0.1982,  0.1631,  0.1738,  0.2471, -0.1514,  0.2236,  0.1787,\n",
       "                       0.1797,  0.2119,  0.1777,  0.2061,  0.5000,  0.1924,  0.1846,  0.2715,\n",
       "                       0.1523,  0.2217,  0.1875,  0.1943,  0.3926,  0.2285,  0.1621,  0.2451,\n",
       "                       0.1748,  0.2246,  0.2080,  0.3633,  0.2041,  0.2061,  0.9062,  0.2432,\n",
       "                       0.1982,  0.1748,  0.2090,  0.1738,  0.1924,  0.2432,  0.1699,  0.1787,\n",
       "                       0.1729,  0.1846,  0.1748,  0.1826,  0.2539,  0.2178,  0.3828,  0.1514,\n",
       "                       0.1943,  0.4492,  0.2021,  0.2002,  0.1709,  0.3008,  0.2129,  0.2217,\n",
       "                       0.2109,  0.2061,  0.2100,  0.1924,  0.2236,  0.1982,  0.7930,  0.1914,\n",
       "                       0.1670,  0.1914,  0.4316,  0.1934,  0.2637,  0.2334,  0.2188,  0.2363,\n",
       "                       0.2178,  0.3770,  0.4492,  0.2314,  0.1602,  0.1445,  0.2344,  0.3477,\n",
       "                       0.2305,  0.2441,  0.2188,  0.2754,  0.2012,  0.2578,  0.2656,  0.4258,\n",
       "                       0.2734,  0.2988,  0.1807,  0.3379,  0.1943,  0.2500,  0.1934,  0.2031,\n",
       "                       0.2148,  0.4141,  0.1807,  0.1621,  0.2539,  0.1621,  0.3926,  0.2598,\n",
       "                       0.1592,  0.2080,  0.3008,  0.3047,  0.1865,  0.2207,  0.1621,  0.3613,\n",
       "                       0.2598,  0.2354,  0.4453, -0.1973,  0.1660,  0.2812,  0.2031,  0.2559,\n",
       "                       0.2383,  0.3398,  0.3848,  0.0688,  0.1660,  0.1748,  0.1973,  0.1523,\n",
       "                       0.3105,  0.3340,  0.1797,  0.3203,  0.2188,  0.2715,  0.1963,  0.1777,\n",
       "                       0.1963,  0.2002,  0.1621,  0.3008,  0.1943,  0.2471,  0.1982,  0.1982,\n",
       "                       0.1602,  0.2617,  0.2041,  0.1719,  0.2109,  0.3359,  0.2188,  0.1816,\n",
       "                       0.1934,  0.1963,  0.3184,  0.2480,  0.2344,  0.2314,  0.1650,  0.2295,\n",
       "                       0.2197,  0.1953, -0.2275,  1.2578,  0.3633,  0.2217,  0.1875,  0.2324,\n",
       "                       0.2451,  0.1914,  0.2852,  0.2891,  0.2168,  0.2451,  0.2432,  0.2305,\n",
       "                       0.2656,  0.1406,  0.1650, -0.2295,  0.0815,  0.2793,  0.3359,  0.2559,\n",
       "                       0.9258,  0.2422,  0.1689,  0.2109,  0.1738,  0.2021,  0.2207,  0.2480,\n",
       "                       0.1738,  0.2500,  0.3281,  0.1855,  0.1846,  0.1592,  0.1650,  0.2295,\n",
       "                       0.1709,  0.2637,  0.3594,  0.2051,  0.4199,  0.2949,  0.1865,  0.1641,\n",
       "                       0.1875,  0.2031,  0.2051,  0.2041,  0.2412,  0.2002,  0.3691,  0.5391,\n",
       "                       0.2930,  0.3359,  0.5234,  0.2383,  0.1758,  0.2871,  0.2031,  0.2715,\n",
       "                      -0.6445,  0.4414,  0.1650,  0.2246,  0.2461,  0.3301,  0.1836,  0.1631,\n",
       "                       0.2598,  0.4961,  0.1729,  0.2051,  0.1982,  0.1641,  0.1855,  0.1523,\n",
       "                       0.3945,  0.1611,  0.1611,  0.2217,  0.2441,  0.1523,  0.5234,  0.2158,\n",
       "                       0.2285,  0.1562,  0.2109,  0.1758,  0.2832,  0.2891,  0.1758,  0.2324,\n",
       "                       0.2012,  0.3711,  0.2949,  0.2129,  0.2988,  0.6250,  0.2295,  0.1816,\n",
       "                       0.1602,  0.2695,  0.2559,  0.1924,  0.2910,  0.2021,  0.2676,  0.3242,\n",
       "                       0.2754,  0.2002,  0.1943,  0.3496,  0.2637,  0.2080,  0.1846,  0.1875,\n",
       "                       0.1719,  0.1572,  0.2295,  0.5391,  0.2363,  0.2520,  0.2373,  0.2012,\n",
       "                       0.2178,  0.2393,  0.2695,  0.2461,  0.3496,  0.3633,  0.3438,  0.1807,\n",
       "                       0.2168,  0.1748,  0.2988,  0.2100,  0.2246,  0.4961,  0.1826,  0.2793,\n",
       "                       0.1865,  0.3750,  0.4766,  0.1934,  0.1572,  0.2148,  0.3867,  0.2090,\n",
       "                       0.1699,  0.2539,  0.2402,  0.2676,  0.0923,  0.2285,  0.2812,  0.1709,\n",
       "                       0.3066,  0.1895,  0.3086,  0.4922,  0.2246,  0.2148,  0.4648,  0.2090,\n",
       "                       0.1836,  0.3535,  0.1982,  0.2930,  0.1758,  0.2656,  0.1768,  0.2891,\n",
       "                       0.2227,  0.5312,  0.2871,  0.2188,  0.2412,  0.1758,  0.2051,  0.1855,\n",
       "                       0.1924,  0.0562,  0.1650,  0.2100,  0.1738,  0.1982,  0.3691,  0.1963,\n",
       "                       0.1914,  0.1816,  0.1846,  0.1553,  0.2158,  0.2520,  0.1719,  0.2207,\n",
       "                       0.1885,  0.1738,  0.2051,  0.2598,  1.1250,  0.2363,  0.1641,  0.3555,\n",
       "                       0.2100,  0.1875,  0.2500,  0.1797,  0.2227,  0.2891,  0.2949,  0.1797,\n",
       "                       0.3359,  0.1621,  0.3184,  0.2021,  0.2002,  0.1904,  0.2158,  0.3145,\n",
       "                       0.2197,  0.3516,  0.3281,  0.2559,  0.1768,  0.2373,  0.1689,  0.2256,\n",
       "                       0.6094,  0.2285,  0.2285,  0.1641,  0.1953,  0.1758,  0.2207,  0.3145,\n",
       "                       0.3926,  0.2217,  0.2734,  0.1904,  0.1758,  0.1943,  0.1865,  0.1963,\n",
       "                       0.2461,  0.2158,  0.2695,  0.2178,  0.1816,  0.1836,  0.1680,  0.1709,\n",
       "                       0.1865,  0.1709,  0.2207,  0.1904,  0.1982,  0.1836,  0.2559,  0.2119,\n",
       "                       0.2891,  0.3184,  0.1826,  0.1816,  0.8164,  0.1943,  0.1816,  0.3359,\n",
       "                       0.1572,  0.2197,  0.1973,  0.2148,  0.2949,  0.1797,  0.1963,  0.9844,\n",
       "                       0.2256,  0.2207,  0.2012,  0.1689,  0.1572,  0.3828,  0.2344,  0.3320,\n",
       "                       0.1982,  0.2676,  0.2197,  0.2773,  0.1631,  0.2695,  0.2305,  0.1699,\n",
       "                       0.3887,  0.1777,  0.2617,  0.1611,  0.2412,  0.2520,  0.5391,  0.2197,\n",
       "                       0.1533,  0.2236,  0.1650,  0.2168,  0.1992,  0.3203,  0.2451,  0.3691,\n",
       "                       0.1934,  0.2617,  0.3359,  0.2090,  0.2344,  0.2168,  0.2451,  0.2295,\n",
       "                       0.2031,  0.2178,  0.2773,  0.2617,  0.2500,  0.1924,  0.2734,  0.3008,\n",
       "                       0.2041,  0.1855,  0.2373,  0.1816,  0.2090,  0.2793,  0.3555,  0.4062,\n",
       "                       0.1777,  0.6133,  0.1875,  0.2158,  0.3008,  0.1992,  0.2695,  0.2578,\n",
       "                       0.2207,  0.2168,  0.4062,  0.7383,  0.2188,  0.2949,  0.0630,  0.2139,\n",
       "                       0.2773,  0.1924,  0.2520,  0.1797,  0.1562,  0.1885,  0.2227,  0.2617,\n",
       "                       0.1885,  0.2832,  0.1689,  0.7031,  0.2695,  0.1572,  0.1816,  0.2021,\n",
       "                       0.1670,  0.1758,  0.1719,  0.1816,  0.1553,  0.1855,  0.1641,  0.2363,\n",
       "                       0.1758,  0.2129,  0.2041,  0.2080,  0.3535,  0.2227,  0.4531,  0.2012,\n",
       "                       0.3379,  0.1729,  0.2480,  0.2773,  0.2559,  0.1680,  0.1768,  0.4180,\n",
       "                       0.2617,  0.2002, -0.2129,  0.1514,  0.2324,  0.1797,  0.2285,  0.2246,\n",
       "                       0.2061,  0.3652,  0.2080,  0.2256,  0.2188,  0.1855,  0.1611,  0.3398,\n",
       "                       0.2275,  0.1807,  0.2490,  0.2910,  0.2637,  0.3555,  0.3047,  0.2021,\n",
       "                       0.3301,  1.0781,  0.1660,  0.3652,  0.2793,  0.2021,  0.2637,  0.1855,\n",
       "                      -0.3340,  0.2334,  0.1709,  0.2139,  0.3359,  0.2305,  0.3477,  0.1738,\n",
       "                       0.2021,  0.1758,  0.1982,  0.1924,  0.2227,  0.2539,  0.2715,  0.4570,\n",
       "                       0.2002,  0.2871,  0.2363,  0.2949,  0.2695,  0.0703,  0.2500,  0.4043,\n",
       "                       0.2236,  0.2637,  0.2012,  0.1543,  0.2246,  0.1699,  0.2139,  0.2773,\n",
       "                       0.2061,  0.2031,  0.2539,  0.1816,  0.2578,  0.1982,  0.1846,  0.2002,\n",
       "                       0.2236,  0.1865,  0.2402,  0.1641, -0.2451,  0.3691, -0.2119,  0.2812,\n",
       "                       0.1582,  0.2471, -0.1719,  0.2637,  0.4785,  0.1904,  0.2012,  0.2100,\n",
       "                       0.2344,  0.1738,  0.2168,  0.2539,  0.2256, -0.1768,  0.2471,  0.2393,\n",
       "                       0.2080,  0.1729,  0.1621,  0.5117,  0.4766, -0.1631,  0.4609,  0.2197,\n",
       "                       0.1914,  0.1768,  0.3730,  0.2334,  0.2412,  0.2676,  0.2246,  0.2969,\n",
       "                       0.2012,  0.2695,  0.3730,  0.1846,  0.1992,  0.3340,  0.2754,  0.2031,\n",
       "                       0.1641,  0.2949,  0.2373,  0.1777,  0.1895,  0.2695,  0.2246,  0.2002,\n",
       "                       0.1982,  0.1973,  0.2236,  0.2363,  0.2188,  0.3711,  0.2305,  0.3477,\n",
       "                       0.1787,  0.3105,  0.2598,  0.1660,  0.2598,  0.3223,  0.2285,  0.2002,\n",
       "                       0.4199,  0.2295,  0.2363,  0.2012,  0.2891,  0.1895,  0.4180,  0.2480,\n",
       "                       0.2988,  0.1963,  0.2051,  0.1641,  0.2578,  0.2441,  0.1953,  0.2812])),\n",
       "             ('0.auto_model.encoder.block.1.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0142, -0.0175,  0.0053,  ...,  0.0109,  0.0022, -0.0283],\n",
       "                      [ 0.0232,  0.0258,  0.0287,  ..., -0.0029, -0.0011, -0.0236],\n",
       "                      [-0.0635, -0.0251, -0.0728,  ..., -0.0184,  0.0376, -0.0393],\n",
       "                      ...,\n",
       "                      [ 0.0056, -0.0459,  0.0078,  ...,  0.0383, -0.0206,  0.0173],\n",
       "                      [ 0.0096, -0.0320, -0.0212,  ..., -0.0415,  0.0547, -0.0270],\n",
       "                      [-0.0344, -0.0032, -0.0056,  ...,  0.0145, -0.0566,  0.0220]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.1924,  0.4688,  0.1582,  ...,  0.1592,  0.3047,  0.4297],\n",
       "                      [ 0.1572,  0.6953,  0.2402,  ...,  0.1177, -0.1992, -0.3711],\n",
       "                      [-0.2441,  0.2715, -0.3770,  ..., -0.2793, -0.2578, -0.6953],\n",
       "                      ...,\n",
       "                      [ 0.1206, -0.1123, -0.3438,  ...,  0.1221, -0.3945,  0.0327],\n",
       "                      [ 0.1099, -0.2148, -0.6367,  ..., -0.0938,  0.5273,  0.1621],\n",
       "                      [-0.0374, -0.1030, -0.1377,  ...,  0.1855, -0.4375, -0.2285]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.2773, -0.2217, -0.1221,  ...,  0.1377,  0.0435, -0.2129],\n",
       "                      [-0.4746, -0.0254,  0.1719,  ..., -0.6914,  0.1592,  0.2793],\n",
       "                      [-0.8047,  0.0148,  0.4844,  ..., -0.8086,  0.1182, -0.5430],\n",
       "                      ...,\n",
       "                      [ 0.1338, -0.5508,  0.6094,  ..., -0.2812,  0.2139,  0.1357],\n",
       "                      [-0.0640, -0.1377,  0.2256,  ..., -0.3066,  0.2012, -0.0669],\n",
       "                      [-0.4551, -0.3242, -0.0552,  ..., -0.2676, -0.3633, -0.3496]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.0554, -0.3340,  0.0074,  ...,  0.0315, -0.2715,  0.3926],\n",
       "                      [-0.0400,  0.0030,  0.2812,  ...,  0.8945, -0.4492,  0.5000],\n",
       "                      [ 0.2012, -0.1406, -0.6406,  ..., -0.2988,  0.1660,  0.7227],\n",
       "                      ...,\n",
       "                      [-0.0898,  0.2617,  0.5391,  ...,  0.1816, -0.1191, -0.0405],\n",
       "                      [ 0.1040,  0.0369, -0.1328,  ..., -0.4551, -0.2266, -0.6484],\n",
       "                      [ 0.4062, -0.3672,  0.3379,  ..., -0.7539, -0.6172, -0.2793]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1426,  0.1562,  0.1543,  0.1621,  0.1094,  0.1250,  0.0361,  0.1543,\n",
       "                       0.1270,  0.1279,  0.1475,  0.1670,  0.1157,  0.1602,  0.1064,  0.1338,\n",
       "                       0.1582,  0.1533,  0.2109,  0.1162,  0.1221,  0.1406,  0.0359,  0.1060,\n",
       "                       0.1191,  0.1631,  0.1138,  0.1455,  0.1465,  0.1074,  0.1201,  0.1270,\n",
       "                       0.1484,  0.1104,  0.1553,  0.1240,  0.1650,  0.2158,  0.0996,  0.1963,\n",
       "                       0.1670,  0.1621,  0.1738,  0.1455,  0.0903,  0.1201,  0.1514,  0.1230,\n",
       "                       0.1377,  0.1582,  0.1895,  0.1309,  0.1660,  0.1123,  0.0308,  0.1211,\n",
       "                       0.0388,  0.1455,  0.1240,  0.1230,  0.1484,  0.1123,  0.1357,  0.1211,\n",
       "                       0.1250,  0.0342,  0.1279,  0.1260,  0.2383,  0.1206,  0.1260,  0.1846,\n",
       "                       0.1104,  0.1377,  0.1299,  0.1187,  0.1484,  0.1328,  0.1069,  0.1631,\n",
       "                       0.1328,  0.1387,  0.1162,  0.1680,  0.1475,  0.1348,  0.2295,  0.1484,\n",
       "                       0.1387,  0.1182,  0.1455,  0.1006,  0.1221,  0.1445,  0.1089,  0.1060,\n",
       "                       0.1328,  0.1250,  0.1152,  0.1187,  0.1758,  0.1338,  0.1523,  0.0942,\n",
       "                       0.1260,  0.2070,  0.1289,  0.1206,  0.1060,  0.1729,  0.1533,  0.1348,\n",
       "                       0.1152,  0.1128,  0.1260,  0.1133,  0.1357,  0.1206,  0.2695,  0.1318,\n",
       "                       0.1006,  0.1172,  0.1924,  0.1309,  0.1855,  0.1475,  0.1475,  0.1289,\n",
       "                       0.1387,  0.1660,  0.1318,  0.1572,  0.1133, -0.1074,  0.1328,  0.1689,\n",
       "                       0.1533,  0.1348,  0.1465,  0.1416,  0.1260,  0.1738,  0.1816,  0.2002,\n",
       "                       0.1445,  0.1533,  0.1226,  0.1650,  0.1138,  0.1416,  0.1279,  0.1367,\n",
       "                       0.1348,  0.2207,  0.1187,  0.1260,  0.1514,  0.1113,  0.1543,  0.1318,\n",
       "                       0.1050, -0.1240,  0.1260,  0.1089,  0.1118,  0.1094,  0.1250,  0.1602,\n",
       "                       0.1387,  0.1836,  0.2119,  0.1270,  0.1387,  0.1885,  0.1328,  0.1631,\n",
       "                       0.1523,  0.2100,  0.1826,  0.0270,  0.1191,  0.1147,  0.1328,  0.1128,\n",
       "                       0.1670,  0.1904,  0.1123,  0.1758,  0.1426,  0.1445,  0.1172,  0.1318,\n",
       "                       0.0981,  0.1318,  0.1079,  0.0630,  0.1177,  0.1582,  0.1396,  0.1172,\n",
       "                       0.1001,  0.1396,  0.1260,  0.1196,  0.1328,  0.1816,  0.1270,  0.1270,\n",
       "                       0.1230,  0.1270,  0.1670,  0.1250,  0.1445,  0.1660,  0.1328,  0.1572,\n",
       "                       0.1367,  0.1196,  0.1455,  0.2656,  0.1118,  0.1455,  0.1011,  0.1406,\n",
       "                       0.1396,  0.1226,  0.1523,  0.1992,  0.1299,  0.1611,  0.1250,  0.1387,\n",
       "                       0.0864,  0.1055,  0.1099,  0.0811,  0.0693,  0.1631,  0.1338,  0.1729,\n",
       "                       0.2168,  0.1699,  0.1299,  0.1348,  0.1016,  0.1143,  0.1299,  0.1436,\n",
       "                       0.1001,  0.1396,  0.1260,  0.1128,  0.1074,  0.1104,  0.1079,  0.1182,\n",
       "                       0.1211,  0.1240,  0.1216,  0.1240,  0.1982,  0.1777,  0.1357,  0.1191,\n",
       "                       0.1196,  0.1309,  0.1318,  0.1235,  0.1377,  0.1167,  0.0306,  0.2129,\n",
       "                       0.1836,  0.0359,  0.2021,  0.1475,  0.1118,  0.1797,  0.1377,  0.1426,\n",
       "                       0.2217,  0.2197,  0.1235,  0.1436,  0.1494,  0.1729,  0.1099,  0.1196,\n",
       "                       0.1367,  0.2021,  0.1069,  0.1318,  0.1050,  0.1177,  0.1216,  0.1040,\n",
       "                       0.1826,  0.1260,  0.1201,  0.1641,  0.1572,  0.1196,  0.2441,  0.1455,\n",
       "                       0.1436,  0.1177,  0.1260,  0.1226,  0.1514,  0.1309,  0.1240,  0.1689,\n",
       "                       0.1162,  0.1250,  0.1748,  0.1367,  0.0972,  0.2520,  0.1377,  0.1182,\n",
       "                       0.1045,  0.1699,  0.1426,  0.1050,  0.1650,  0.1055,  0.1816,  0.2158,\n",
       "                       0.1416,  0.1279,  0.1260,  0.1572,  0.1572,  0.1201,  0.1270,  0.1226,\n",
       "                       0.1074,  0.1201,  0.1260,  0.2168,  0.1494,  0.1582,  0.1504,  0.1226,\n",
       "                       0.1387,  0.1465,  0.1582,  0.1367,  0.1543,  0.1748,  0.1904,  0.1133,\n",
       "                       0.1387,  0.1279,  0.1611,  0.1465,  0.1416,  0.2178,  0.1143,  0.1699,\n",
       "                       0.1191,  0.2031,  0.1904,  0.0840,  0.1162,  0.1201,  0.1768,  0.1289,\n",
       "                       0.1157,  0.1201,  0.1533,  0.1602,  0.0251,  0.1118,  0.1816,  0.1108,\n",
       "                       0.1836,  0.1133,  0.1807,  0.1650,  0.1289,  0.1455,  0.2227,  0.1338,\n",
       "                       0.1260,  0.1040,  0.1289,  0.1924,  0.1006,  0.1650,  0.1201,  0.1787,\n",
       "                       0.1357,  0.2051,  0.1611,  0.1387,  0.1455,  0.1104,  0.1328,  0.1177,\n",
       "                       0.1133,  0.0278,  0.1060,  0.1387,  0.1074,  0.1240,  0.1914,  0.1230,\n",
       "                       0.1143,  0.1216,  0.1240,  0.1035,  0.1279,  0.1396,  0.1128,  0.1338,\n",
       "                       0.1104,  0.1118,  0.0344,  0.1631,  0.2812,  0.1455,  0.0996,  0.0635,\n",
       "                       0.1377,  0.1221,  0.1475,  0.1035,  0.1455,  0.1396,  0.1572,  0.1201,\n",
       "                       0.1846,  0.1021,  0.1963,  0.1245,  0.1348,  0.1191,  0.1787,  0.1963,\n",
       "                       0.1494,  0.1670,  0.1611,  0.1875,  0.1104,  0.1348,  0.1167,  0.1465,\n",
       "                       0.2139,  0.1318,  0.1484,  0.1152,  0.1328,  0.1309,  0.1475,  0.1660,\n",
       "                       0.1699,  0.1260,  0.1582,  0.1099,  0.1060,  0.1069,  0.1216,  0.1406,\n",
       "                       0.1523,  0.1348,  0.1123,  0.1289,  0.0986,  0.1187,  0.1035,  0.1162,\n",
       "                       0.0908,  0.1064,  0.1377,  0.1318,  0.1279,  0.1260,  0.1641,  0.1533,\n",
       "                       0.1797,  0.1777,  0.1396,  0.1123,  0.2812,  0.1230,  0.1030,  0.1719,\n",
       "                       0.1118,  0.1426,  0.1377,  0.1670,  0.1035,  0.1260,  0.1377,  0.2080,\n",
       "                       0.1465,  0.1387,  0.1108,  0.1084,  0.1128,  0.1748,  0.1582,  0.0352,\n",
       "                       0.1416,  0.1660,  0.1484, -0.1533,  0.1006,  0.1523,  0.1445,  0.1089,\n",
       "                       0.0806,  0.1260,  0.1553,  0.1196,  0.1377,  0.1250,  0.2207,  0.1602,\n",
       "                       0.1016,  0.1270,  0.0933,  0.1348,  0.1416,  0.1924,  0.1543,  0.2070,\n",
       "                       0.1206,  0.1621,  0.1797,  0.1328,  0.1260,  0.1406,  0.1396,  0.1621,\n",
       "                       0.1377,  0.1406,  0.1758,  0.1416,  0.1631,  0.1196,  0.1484,  0.1875,\n",
       "                       0.1245,  0.1289,  0.1631,  0.1270,  0.1099,  0.1631,  0.1943,  0.2256,\n",
       "                       0.0972,  0.1992,  0.1118,  0.1416,  0.1514,  0.1230,  0.1465,  0.1836,\n",
       "                       0.1650,  0.1113,  0.1680,  0.2051,  0.1494,  0.1885,  0.0238,  0.1387,\n",
       "                       0.1689,  0.1201,  0.1533,  0.0322,  0.1030,  0.1226,  0.1157,  0.1553,\n",
       "                       0.1299,  0.1826,  0.1123,  0.2490,  0.1406,  0.1055,  0.1289,  0.1279,\n",
       "                       0.0991,  0.1138,  0.1113,  0.1230,  0.1108,  0.1108,  0.1187,  0.1641,\n",
       "                       0.1235,  0.1270,  0.1367,  0.1387,  0.1436,  0.1357,  0.1963,  0.1226,\n",
       "                       0.1748,  0.1367,  0.1475,  0.0996,  0.1543,  0.1064,  0.1230,  0.1670,\n",
       "                       0.1699,  0.1201,  0.1406,  0.1001,  0.1387,  0.1196,  0.1279,  0.1553,\n",
       "                       0.1157,  0.1475,  0.1279,  0.1543,  0.1396,  0.1001,  0.1196,  0.1797,\n",
       "                       0.1396,  0.1187,  0.1455,  0.1748,  0.1367,  0.1670,  0.1650,  0.1309,\n",
       "                       0.1689,  0.2402,  0.1240,  0.1680,  0.1426,  0.1494,  0.1914,  0.1299,\n",
       "                       0.1719,  0.1465,  0.0933,  0.1270,  0.1465,  0.1533,  0.1318,  0.1157,\n",
       "                       0.1094,  0.1211,  0.1162,  0.1289,  0.1318,  0.1357,  0.1846,  0.1797,\n",
       "                       0.1216,  0.1436,  0.1602,  0.1582,  0.1729,  0.0284,  0.1279,  0.2617,\n",
       "                       0.1270,  0.1533,  0.1250,  0.1128,  0.1426,  0.1196,  0.1299,  0.1523,\n",
       "                       0.1279,  0.1187,  0.1484,  0.1240,  0.1201,  0.1279,  0.1270,  0.1377,\n",
       "                       0.1465,  0.1289,  0.1445,  0.0977,  0.1396,  0.0742,  0.1299,  0.1602,\n",
       "                       0.1094,  0.1406,  0.1167,  0.1436,  0.2217,  0.1216,  0.1191,  0.1235,\n",
       "                       0.1592,  0.1167,  0.1309,  0.1592,  0.1484,  0.0996,  0.1387,  0.1157,\n",
       "                       0.1279,  0.1152,  0.1196,  0.1865,  0.2520,  0.1221,  0.1245,  0.1416,\n",
       "                       0.1357,  0.1216,  0.0249,  0.1367,  0.1602,  0.1738,  0.1206,  0.1807,\n",
       "                       0.1177,  0.1309,  0.1875,  0.1128,  0.1533,  0.1367,  0.1699,  0.1079,\n",
       "                       0.1206,  0.1709,  0.1387,  0.1235,  0.1187,  0.1016,  0.1426,  0.1328,\n",
       "                       0.1562,  0.1377,  0.1318,  0.1670,  0.1426,  0.1768,  0.1631,  0.1670,\n",
       "                       0.1191,  0.1650,  0.1436,  0.1133,  0.1621,  0.1680,  0.1377,  0.1196,\n",
       "                       0.1934,  0.1553,  0.1572,  0.1226,  0.1553,  0.1123,  0.1914,  0.1494,\n",
       "                       0.1318,  0.1084,  0.1260,  0.1025,  0.1406,  0.1670,  0.1299,  0.1738])),\n",
       "             ('0.auto_model.encoder.block.1.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.3555,  0.2197,  0.9219,  ..., -0.9492,  1.0156, -1.1406],\n",
       "                      [-0.0093, -0.0099,  0.3223,  ..., -0.3008,  0.5469,  0.3359],\n",
       "                      [ 0.0016,  0.5234, -0.4883,  ..., -0.3184,  0.3672, -0.3926],\n",
       "                      ...,\n",
       "                      [ 0.2930, -0.2773,  0.2012,  ..., -0.4219,  0.2520,  0.1689],\n",
       "                      [-0.7266,  0.2598, -0.1934,  ..., -0.1982, -0.1177, -0.5312],\n",
       "                      [-0.1055,  0.9258,  0.4883,  ..., -0.5781,  0.0330,  0.7148]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.1206, -0.1572,  0.1216,  ..., -0.2041,  0.2021,  0.0747],\n",
       "                      [-0.1040, -0.0496, -0.0884,  ..., -0.1836, -0.1865,  0.0776],\n",
       "                      [-0.0160,  0.0374,  0.1611,  ...,  0.1396, -0.1914, -0.0679],\n",
       "                      ...,\n",
       "                      [ 0.2490,  0.3008, -0.2656,  ..., -0.3828, -0.6172, -0.0859],\n",
       "                      [-0.0488, -0.0293, -0.1514,  ...,  0.0884, -0.2305,  0.0703],\n",
       "                      [-0.2256, -0.2275,  0.2676,  ..., -0.0879,  0.1621, -0.2295]])),\n",
       "             ('0.auto_model.encoder.block.1.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.5469,  0.5977,  0.6133,  0.6406,  0.3438,  0.4727,  0.1992,  0.5117,\n",
       "                       1.1094,  0.3965,  0.5273,  0.5078,  0.5039,  0.7305,  0.3203,  0.3906,\n",
       "                       0.5547,  0.5156,  0.6641,  0.3477,  0.4141,  0.4336,  0.3184,  0.3516,\n",
       "                       0.3574,  0.5938,  0.3418,  0.5625,  0.4277,  0.3359, -0.3750,  0.5195,\n",
       "                       0.5234,  0.3594,  0.5547,  0.4316,  0.6992,  0.9180,  0.3047,  0.5820,\n",
       "                       0.6172,  0.6133,  0.5469,  0.4121,  1.3594,  0.4180,  0.6484,  0.4160,\n",
       "                       0.4336,  0.6328,  0.6328,  0.4629,  0.6406,  0.3809,  0.3535,  0.4531,\n",
       "                       0.5820,  0.4336,  0.3418,  0.3730,  0.4668,  0.3086,  0.4258,  0.3398,\n",
       "                       0.3438,  0.4492,  0.3809,  0.4062,  0.8047,  0.3809,  0.4023,  0.5273,\n",
       "                       0.3359,  0.4551,  0.3984,  0.3711,  0.7148,  0.4395,  0.3398,  0.4746,\n",
       "                       0.3848,  0.4746,  0.4160,  0.7656,  0.4375,  0.4258,  1.0781,  0.5039,\n",
       "                       0.4043,  0.3164,  0.4336,  0.3516,  0.3711,  0.5000,  0.3398,  0.3672,\n",
       "                       0.3398,  0.3574,  0.3516,  0.3418,  0.5508,  0.4551,  0.6836,  0.3359,\n",
       "                       0.4043,  0.8516,  0.3848,  0.3730,  0.3457,  0.6055,  0.5156,  0.4238,\n",
       "                       0.4062,  0.4297,  0.3945,  0.3164,  0.4297,  0.3887,  1.0938,  0.3613,\n",
       "                       0.3516,  0.3594,  0.7891,  0.3418,  0.6094,  0.4551,  0.4473,  0.4316,\n",
       "                       0.4336,  0.6367,  0.8477,  0.4883,  0.3262,  0.3418,  0.4141,  0.6992,\n",
       "                       0.4707,  0.4258,  0.4414,  0.5234,  0.4434,  0.5781,  0.6484,  0.8164,\n",
       "                       0.4922,  0.6250,  0.3789,  0.5781,  0.3652,  0.5039,  0.3262,  0.4297,\n",
       "                       0.4727,  0.8047,  0.3203,  0.3398,  0.5703,  0.3242,  0.6719,  0.4902,\n",
       "                       0.3496, -0.4395,  0.5391,  0.9492,  0.3613,  0.3711,  0.3730,  0.5781,\n",
       "                       0.4844,  0.5742,  0.8086,  0.3945,  0.3242,  0.6445,  0.4180,  0.5117,\n",
       "                       0.5273,  0.6211,  0.6602,  0.1006,  0.3867,  0.3496,  0.3926,  0.3379,\n",
       "                       0.5469,  0.6289,  0.3789,  0.6211,  0.4199,  0.5039,  0.4180,  0.3750,\n",
       "                       0.3496,  0.3750,  0.3633,  0.2695,  0.3730,  0.5117,  0.4297,  0.3340,\n",
       "                       0.3320,  0.4961,  0.4355,  0.4180,  0.4199,  0.6836,  0.4551,  0.4238,\n",
       "                       0.3711,  0.4199,  0.6250,  0.4453,  0.5312,  0.4922,  0.4160,  0.4785,\n",
       "                       0.4453,  0.3965,  0.5039,  1.7344,  0.6133,  0.4473,  0.3652,  0.4336,\n",
       "                       0.4980,  0.4375,  0.6250,  0.6445,  0.4355,  0.5039,  0.4805,  0.4902,\n",
       "                       0.3809,  0.2930,  0.3105,  0.3164,  0.1680,  0.5625,  0.5664,  0.5898,\n",
       "                       0.9492,  0.5312,  0.3633,  0.4512,  0.3418,  0.3789,  0.4141,  0.5352,\n",
       "                       0.2988,  0.4727,  0.9453,  0.3711,  0.3867,  0.3418,  0.3223,  0.3906,\n",
       "                       0.3672,  0.4590,  0.7148,  0.4082,  0.7812,  0.4219,  0.4082,  0.3711,\n",
       "                       0.4082,  0.4219,  0.4004,  0.4082,  0.4473,  0.4023,  0.6367,  0.8477,\n",
       "                       0.6680,  0.8086,  0.8242,  0.4805,  0.3418,  0.6172,  0.3965,  0.4980,\n",
       "                       0.8594,  0.8594,  0.3945,  0.5586,  0.4941,  0.6367,  0.3516,  0.3770,\n",
       "                       0.4980,  0.9336,  0.3223,  0.3945,  0.3633,  0.3906,  0.3906,  0.3086,\n",
       "                       0.6055,  0.3477,  0.3594,  0.5156,  0.4648,  0.3828,  0.8398,  0.5430,\n",
       "                       0.4453,  0.3184,  0.4238,  0.3926,  0.5195,  0.5703,  0.3730,  0.5391,\n",
       "                       0.3555,  0.5664,  0.5508,  0.4277,  0.5039,  0.8750,  0.5430,  0.3594,\n",
       "                       0.3398,  0.5781,  0.4824,  0.3340,  0.6133,  0.3379,  0.5781,  0.6250,\n",
       "                       0.5273,  0.4316,  0.3809,  0.6719,  0.5391,  0.4414,  0.3477,  0.3965,\n",
       "                       0.3457,  0.3340,  0.4492,  0.8945,  0.5273,  0.5078,  0.4531,  0.4277,\n",
       "                       0.4492,  0.4590,  0.5742,  0.4629,  0.6055,  0.7891,  0.6914,  0.3691,\n",
       "                       0.4102,  0.3633,  0.6367,  0.4180,  0.5234,  0.7734,  0.3418,  0.6133,\n",
       "                       0.3652,  0.7031,  0.9180, -0.3242,  0.3105,  0.4160,  0.7344,  0.4941,\n",
       "                       0.3672,  0.4609,  0.5352,  0.5000,  0.0417,  0.4258,  0.6055,  0.3086,\n",
       "                       0.6172,  0.3945,  0.6367,  0.8281,  0.4238,  0.4492,  0.7500,  0.3711,\n",
       "                       0.3867,  0.7578,  0.3398,  0.6016,  0.3457,  0.5117,  0.3848,  0.6250,\n",
       "                       0.4609,  0.8047,  0.5547,  0.4531,  0.5547,  0.3359,  0.4492,  0.3457,\n",
       "                       0.3770,  0.2275,  0.3105,  0.3633,  0.3867,  0.4336,  0.6953,  0.3926,\n",
       "                       0.3672,  0.3652,  0.3984,  0.3223,  0.4883,  0.4863,  0.3301,  0.4531,\n",
       "                       0.3848,  0.3789,  0.1943,  0.5859,  1.2969,  0.4766,  0.3340,  0.2949,\n",
       "                       0.4570,  0.3730,  0.5742,  0.3086,  0.4336,  0.5547,  0.5703,  0.3477,\n",
       "                       0.6680,  0.3867,  0.6719,  0.3750,  0.4336,  0.3672,  0.5352,  0.6133,\n",
       "                       0.4688,  0.6484,  0.5898,  0.5781,  0.3438,  0.5117,  0.3691,  0.4668,\n",
       "                       0.9219,  0.4277,  0.4473,  0.3574,  0.4043,  0.3770,  0.4980,  0.5938,\n",
       "                       0.6992,  0.4512,  0.5586,  0.3535,  0.3105, -0.3770,  0.4062,  0.4492,\n",
       "                       0.4727,  0.3945, -0.4316,  0.4102,  0.3184,  0.3594,  0.3262,  0.3164,\n",
       "                       0.3164,  0.3262,  0.4238,  0.3828,  0.3574,  0.3809,  0.5469,  0.4043,\n",
       "                       0.5820,  0.8086,  0.4375,  0.3887,  1.1562,  0.3945,  0.3145,  0.6680,\n",
       "                       0.3281,  0.4863,  0.3848,  0.4883,  0.6055,  0.3008,  0.4551,  1.0156,\n",
       "                       0.4375,  0.4102,  0.3574,  0.3145,  0.3457,  0.6680,  0.5391,  0.6953,\n",
       "                       0.3848,  0.5352,  0.4570,  0.5625,  0.3047,  0.5156,  0.4590,  0.3594,\n",
       "                       0.9766,  0.3555,  0.5547,  0.3457,  0.4883,  0.4785,  0.8906,  0.4902,\n",
       "                       0.3555,  0.4609,  0.3145,  0.4551,  0.4180,  0.7305,  0.5000,  0.7383,\n",
       "                       0.4043,  0.5234,  0.6602,  0.4043,  0.5000,  0.4141,  0.5156,  0.4785,\n",
       "                       0.4297,  0.3926,  0.6289,  0.4902,  0.5273,  0.3789,  0.5703,  0.6523,\n",
       "                       0.3770,  0.3789,  0.5312,  0.3652,  0.3945,  0.5391,  0.7578,  0.8047,\n",
       "                       0.3633,  1.2969,  0.3457,  0.4238,  0.6367,  0.3672,  0.5391,  0.5391,\n",
       "                       0.5195,  0.3652,  0.7617,  0.8594,  0.4336,  0.6016,  0.1226,  0.4609,\n",
       "                       0.5703,  0.3711,  0.4941,  0.1865,  0.3359,  0.4062,  0.4102,  0.4609,\n",
       "                       0.3945,  0.6289,  0.3418,  1.0312,  0.5039,  0.3164,  0.3496,  0.3711,\n",
       "                       0.3086,  0.3340,  0.3828,  0.4121,  0.3535,  0.3887,  0.3594,  0.5352,\n",
       "                       0.3594,  0.4277,  0.4609,  0.4102,  0.5312,  0.4590,  0.7422,  0.3848,\n",
       "                       0.6445,  0.3242,  0.4492,  0.4238,  0.5078,  0.3086,  0.3555,  0.6133,\n",
       "                       0.5938,  0.3711,  0.4355,  0.3457,  0.4863,  0.3516,  0.4551,  0.4766,\n",
       "                      -0.3730,  0.6133,  0.4414,  0.4531,  0.4434, -0.3535,  0.3945,  0.6445,\n",
       "                       0.4590,  0.3770,  0.4844,  0.5820,  0.4453,  0.6562,  0.5898,  0.4082,\n",
       "                       0.6055,  1.0938,  0.3398,  0.6562,  0.4980,  0.4551,  0.5820,  0.3594,\n",
       "                       0.6328,  0.4746,  0.3633,  0.4434,  0.5391,  0.5078,  0.5781,  0.3301,\n",
       "                       0.3477,  0.3809,  0.4277,  0.4062,  0.4551,  0.4980,  0.5586,  0.7852,\n",
       "                       0.3926,  0.5156,  0.5195,  0.5625,  0.5859,  0.0781,  0.4863,  0.8008,\n",
       "                       0.4609,  0.5508,  0.4297,  0.2969,  0.4570,  0.3652,  0.4121,  0.6211,\n",
       "                       0.4590,  0.4082,  0.4883,  0.3438,  0.4395,  0.4062,  0.4082,  0.4023,\n",
       "                       0.4590,  0.3750,  0.4629,  0.3223,  0.4375,  0.7852,  0.4336,  0.5508,\n",
       "                       0.3320,  0.4746,  0.3555,  0.5156,  0.8594,  0.3730,  0.3652,  0.3926,\n",
       "                       0.4902,  0.3438,  0.4414,  0.5039,  0.4395,  0.3398,  0.4785,  0.4492,\n",
       "                       0.4258,  0.3789,  0.3926,  0.9570,  0.7773,  0.3730,  1.2578,  0.4551,\n",
       "                       0.4609,  0.3184,  0.3320,  0.5000,  0.5469,  0.6094,  0.4238,  0.6328,\n",
       "                       0.3906, -0.4961,  0.6523,  0.3203,  0.4375,  0.5586,  0.5156,  0.3301,\n",
       "                       0.3848,  0.5898,  0.4609,  0.3633,  0.3594, -0.3652,  0.4492,  0.4492,\n",
       "                       0.4180,  0.4336,  0.4199,  0.5234,  0.4688,  0.6953,  0.5273,  0.6289,\n",
       "                       0.3438,  0.5977,  0.4922,  0.3184,  0.5312,  0.6836,  0.4531,  0.4141,\n",
       "                       0.7891,  0.4590,  0.4785,  0.3945,  0.5547,  0.3633,  0.6367,  0.4609,\n",
       "                       0.5547,  0.3223,  0.4648,  0.3340,  0.5312,  0.5234,  0.4160,  0.5703])),\n",
       "             ('0.auto_model.encoder.block.2.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0055,  0.0181, -0.0088,  ..., -0.0718,  0.0500,  0.0791],\n",
       "                      [-0.0503,  0.0505,  0.0175,  ..., -0.0364, -0.0515,  0.1113],\n",
       "                      [-0.0447,  0.0664,  0.0291,  ...,  0.0317, -0.0229, -0.0097],\n",
       "                      ...,\n",
       "                      [ 0.0366, -0.0212, -0.0040,  ...,  0.0051,  0.0039, -0.0554],\n",
       "                      [ 0.0299,  0.0067, -0.0005,  ..., -0.0122,  0.0056,  0.0408],\n",
       "                      [-0.0413, -0.0056, -0.0243,  ..., -0.0417,  0.0063,  0.0126]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.3418,  0.6094, -1.1641,  ..., -0.1445,  0.8984, -0.0703],\n",
       "                      [-0.2559,  0.3730,  0.6328,  ..., -0.2422, -0.1377,  0.4648],\n",
       "                      [-0.3477, -0.6367, -0.0199,  ...,  0.6328, -0.1963, -0.1895],\n",
       "                      ...,\n",
       "                      [-0.4551,  0.0226,  0.1934,  ...,  0.0454,  0.1738,  0.2432],\n",
       "                      [ 0.1836, -0.2832,  0.1855,  ..., -0.2314, -0.5391, -0.2715],\n",
       "                      [-0.0527, -0.0081,  0.0767,  ...,  0.7539, -0.2598,  0.0481]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.0361, -0.1357, -0.3809,  ..., -0.4902, -0.1279, -0.2891],\n",
       "                      [ 0.2539,  0.2197,  0.2236,  ..., -0.5742, -0.0698, -0.3789],\n",
       "                      [-0.1543,  0.0811,  0.1689,  ...,  0.3984, -0.2617, -0.1904],\n",
       "                      ...,\n",
       "                      [ 0.1387,  0.7812,  0.3965,  ...,  0.5898,  0.1426, -1.1562],\n",
       "                      [ 0.4277,  0.3906, -0.4629,  ..., -0.4277, -0.7695,  0.3477],\n",
       "                      [ 1.1406, -0.5625,  0.6133,  ...,  0.5195, -0.5703, -0.4414]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.1758, -0.5156,  0.4082,  ...,  0.4102,  0.1641, -0.9648],\n",
       "                      [-0.5469, -0.3281, -0.1914,  ..., -1.1406, -0.5352,  1.0234],\n",
       "                      [-0.1953, -0.2383,  0.4922,  ..., -0.5352,  0.6523, -0.5664],\n",
       "                      ...,\n",
       "                      [ 0.3691,  0.7422, -0.0645,  ...,  0.1533,  0.2891,  0.1816],\n",
       "                      [ 0.4453,  0.0923,  0.0762,  ...,  0.0437,  0.6172,  0.7500],\n",
       "                      [ 0.8047,  0.0048,  0.0287,  ...,  1.3672, -0.9297,  1.4219]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1641,  0.1494,  0.1777,  0.1846,  0.1216,  0.1328,  0.0430,  0.1680,\n",
       "                       0.1094,  0.1436,  0.1553,  0.1455,  0.1108,  0.1709,  0.1060,  0.1318,\n",
       "                       0.1680,  0.1670,  0.2168,  0.1230,  0.1187,  0.1426,  0.0312,  0.1245,\n",
       "                       0.1279,  0.1680,  0.1260,  0.1475,  0.1406,  0.1191,  0.1328,  0.1377,\n",
       "                       0.1582,  0.1318,  0.1611,  0.1504,  0.1562,  0.2275,  0.1177,  0.2139,\n",
       "                       0.1592,  0.1641,  0.1738,  0.1484,  0.0845,  0.1250,  0.1445,  0.1357,\n",
       "                       0.1387,  0.1465,  0.2139,  0.1543,  0.1553,  0.1064,  0.0344,  0.1250,\n",
       "                       0.0234,  0.1426,  0.1357,  0.1289,  0.1602,  0.1118,  0.1523,  0.1279,\n",
       "                       0.1279,  0.0376,  0.1299,  0.1318,  0.2207,  0.1260,  0.1396,  0.1758,\n",
       "                       0.1187,  0.1426,  0.1299,  0.1177,  0.1494,  0.1357,  0.1182,  0.1680,\n",
       "                       0.1318,  0.1602,  0.1426,  0.1826,  0.1465,  0.1484,  0.2188,  0.1719,\n",
       "                       0.1416,  0.1162,  0.1602,  0.1089,  0.1309,  0.1475,  0.1289,  0.1152,\n",
       "                       0.1338,  0.1426,  0.1221,  0.1172,  0.1943,  0.1426,  0.1562,  0.1084,\n",
       "                       0.1309,  0.1973,  0.1328,  0.1328,  0.1177,  0.1670,  0.1611,  0.1436,\n",
       "                       0.1318,  0.1069,  0.1445,  0.1221,  0.1328,  0.1138,  0.2354,  0.1309,\n",
       "                       0.1152,  0.1348,  0.1992,  0.1357,  0.1904,  0.1396,  0.1436,  0.1367,\n",
       "                       0.1445,  0.1875,  0.1094,  0.1611,  0.1128,  0.1011,  0.1338,  0.1729,\n",
       "                       0.1533,  0.1328,  0.1475,  0.1689,  0.1406,  0.1826,  0.1924,  0.1943,\n",
       "                       0.1543,  0.1533,  0.1260,  0.1855,  0.1367,  0.1475,  0.1187,  0.1387,\n",
       "                       0.1523,  0.2314,  0.1182,  0.1328,  0.1602,  0.1118,  0.1709,  0.1328,\n",
       "                       0.1138,  0.1357,  0.1226,  0.1016,  0.1221,  0.1089,  0.1221,  0.1631,\n",
       "                       0.1641,  0.2080,  0.2041,  0.1406,  0.1289,  0.1787,  0.1553,  0.1523,\n",
       "                       0.1582,  0.1992,  0.1660, -0.0227,  0.1377,  0.1172,  0.1396,  0.1279,\n",
       "                       0.1709,  0.2021,  0.1230,  0.1982,  0.1504,  0.1387,  0.1221,  0.1216,\n",
       "                       0.1006,  0.1426,  0.1216,  0.0693,  0.1396,  0.1865,  0.1396,  0.1084,\n",
       "                       0.1084,  0.1504,  0.1592,  0.1309,  0.1631,  0.1924,  0.1299,  0.1396,\n",
       "                       0.1279,  0.1494,  0.1641,  0.1377,  0.1621,  0.1709,  0.1396,  0.1738,\n",
       "                       0.1484,  0.1445,  0.1562,  0.2334,  0.1060,  0.1523,  0.1123,  0.1406,\n",
       "                       0.1504,  0.1367,  0.1719,  0.2158,  0.1396,  0.1719,  0.1270,  0.1436,\n",
       "                       0.0801,  0.1172,  0.1167,  0.0952,  0.0522,  0.1777,  0.1494,  0.1953,\n",
       "                       0.2002,  0.1846,  0.1260,  0.1553,  0.1143,  0.1221,  0.1426,  0.1592,\n",
       "                       0.1069,  0.1436,  0.1152,  0.1084,  0.1206,  0.1157,  0.1094,  0.1299,\n",
       "                       0.1357,  0.1289,  0.1040,  0.1387,  0.1885,  0.2051,  0.1260,  0.1377,\n",
       "                       0.1309,  0.1260,  0.1406,  0.1260,  0.1475,  0.1279,  0.0342,  0.2246,\n",
       "                       0.1846,  0.0315,  0.2148,  0.1592,  0.1211,  0.1855,  0.1377,  0.1426,\n",
       "                       0.2236,  0.2119,  0.1318,  0.1582,  0.1592,  0.1768,  0.1108,  0.1235,\n",
       "                       0.1426,  0.1875,  0.1123,  0.1348,  0.1299,  0.1318,  0.1357,  0.1094,\n",
       "                       0.1982,  0.1260,  0.1226,  0.1748,  0.1807,  0.1338,  0.2461,  0.1562,\n",
       "                       0.1602,  0.1138,  0.1328,  0.1338,  0.1484,  0.1426,  0.1318,  0.1602,\n",
       "                       0.1357,  0.1123,  0.1885,  0.1465,  0.0986,  0.2617,  0.1211,  0.1201,\n",
       "                       0.1226,  0.1738,  0.1514,  0.1187,  0.1758,  0.1177,  0.1826,  0.2158,\n",
       "                       0.1562,  0.1377,  0.1299,  0.1729,  0.1406,  0.1226,  0.1196,  0.1279,\n",
       "                       0.1157,  0.1221,  0.1465,  0.2324,  0.1553,  0.1787,  0.1543,  0.1309,\n",
       "                       0.1523,  0.1406,  0.1523,  0.1455,  0.1650,  0.1504,  0.1982,  0.1230,\n",
       "                       0.1357,  0.1367,  0.1680,  0.1455,  0.1553,  0.2236,  0.1270,  0.1846,\n",
       "                       0.1221,  0.2021,  0.1738,  0.0928,  0.1074,  0.1377,  0.1562,  0.1436,\n",
       "                       0.1191,  0.1426,  0.1455,  0.2021,  0.0072,  0.1260,  0.2002,  0.1260,\n",
       "                       0.1826,  0.1187,  0.2100,  0.1445,  0.1436,  0.1611,  0.2188,  0.1406,\n",
       "                       0.1416,  0.0674,  0.1299,  0.2061,  0.1040,  0.1768,  0.1299,  0.1602,\n",
       "                       0.1562,  0.2275,  0.1895,  0.1484,  0.1592,  0.1177,  0.1328,  0.1206,\n",
       "                       0.1123,  0.0244,  0.1187,  0.1514,  0.1250,  0.1445,  0.2070,  0.1357,\n",
       "                       0.1338,  0.1299,  0.1260,  0.1138,  0.1445,  0.1445,  0.1245,  0.1465,\n",
       "                       0.1118,  0.1191,  0.0334,  0.1660,  0.2432,  0.1670,  0.1177,  0.0466,\n",
       "                       0.1494,  0.1328,  0.1592,  0.1270,  0.1416,  0.1602,  0.1787,  0.1318,\n",
       "                       0.1816,  0.1157,  0.2070,  0.1387,  0.1465,  0.1216,  0.1855,  0.2119,\n",
       "                       0.1689,  0.1592,  0.1875,  0.1934,  0.1260,  0.1465,  0.1338,  0.1484,\n",
       "                       0.2109,  0.1523,  0.1426,  0.1318,  0.1465,  0.1328,  0.1680,  0.1680,\n",
       "                       0.1729,  0.1494,  0.1631,  0.1123,  0.1240,  0.1235,  0.1240,  0.1436,\n",
       "                       0.1475,  0.1543,  0.1104,  0.1357,  0.0986,  0.1377,  0.1035,  0.1309,\n",
       "                       0.1021,  0.1157,  0.1641,  0.1289,  0.1416,  0.1318,  0.1562,  0.1445,\n",
       "                       0.1689,  0.1348,  0.1406,  0.1147,  0.2578,  0.1235,  0.1040,  0.1729,\n",
       "                       0.1128,  0.1553,  0.1475,  0.1426,  0.0752,  0.1348,  0.1426,  0.1738,\n",
       "                       0.1475,  0.1543,  0.1279,  0.1211,  0.1260,  0.1797,  0.1670,  0.0306,\n",
       "                       0.1406,  0.1748,  0.1670,  0.1787,  0.1104,  0.1406,  0.1592,  0.1270,\n",
       "                       0.0767,  0.1299,  0.1699,  0.1318,  0.1270,  0.1309,  0.1787,  0.1670,\n",
       "                       0.1064,  0.1426,  0.0913,  0.1377,  0.1426,  0.1650,  0.1602,  0.2266,\n",
       "                       0.1206,  0.1758,  0.1846,  0.1455,  0.1562,  0.1348,  0.1592,  0.1650,\n",
       "                       0.1484,  0.1318,  0.1914,  0.1504,  0.1748,  0.1201,  0.1621,  0.1992,\n",
       "                       0.1396,  0.1309,  0.1650,  0.1367,  0.1201,  0.1533,  0.1982,  0.2227,\n",
       "                       0.1094,  0.1699,  0.1240,  0.1611,  0.1631,  0.1367,  0.1475,  0.1670,\n",
       "                       0.1670,  0.1084,  0.1914,  0.2158,  0.1631,  0.1953,  0.0227,  0.1406,\n",
       "                       0.1689,  0.1196,  0.1621,  0.0359,  0.0957,  0.1279,  0.1338,  0.1592,\n",
       "                       0.1270,  0.2100,  0.1206,  0.2295,  0.1475,  0.1240,  0.1318,  0.1377,\n",
       "                       0.1016,  0.1167,  0.1196,  0.1299,  0.1084,  0.1235,  0.1211,  0.1729,\n",
       "                       0.1455,  0.1328,  0.1523,  0.1465,  0.1387,  0.1348,  0.2002,  0.1494,\n",
       "                       0.2061,  0.1514,  0.1455,  0.0850,  0.1680,  0.1201,  0.1221,  0.1680,\n",
       "                       0.1855,  0.1396,  0.1514,  0.1309,  0.1553,  0.1191,  0.1377,  0.1650,\n",
       "                       0.1289,  0.1777,  0.1416,  0.1445,  0.1592,  0.1118,  0.1328,  0.1963,\n",
       "                       0.1455,  0.1328,  0.1426,  0.1553,  0.1387,  0.1553,  0.1768,  0.1445,\n",
       "                       0.1729,  0.2334,  0.1279,  0.1768,  0.1572,  0.1426,  0.2012,  0.1387,\n",
       "                       0.1602,  0.1602,  0.1289,  0.1328,  0.1855,  0.1602,  0.1572,  0.1152,\n",
       "                       0.1221,  0.1128,  0.1289,  0.1455,  0.1426,  0.1357,  0.1846,  0.1621,\n",
       "                       0.1309,  0.1602,  0.1768,  0.1699,  0.1797,  0.0199,  0.1328,  0.2578,\n",
       "                       0.1367,  0.1631,  0.1270,  0.1182,  0.1475,  0.1348,  0.1523,  0.1504,\n",
       "                       0.1348,  0.1245,  0.1719,  0.1426,  0.1426,  0.1270,  0.1309,  0.1338,\n",
       "                       0.1602,  0.1279,  0.1719,  0.1050,  0.1416,  0.0527,  0.1406,  0.1699,\n",
       "                       0.1230,  0.1699,  0.1187,  0.1611,  0.2266,  0.1216,  0.1396,  0.1162,\n",
       "                       0.1709,  0.1108,  0.1445,  0.1553,  0.1553,  0.0996,  0.1553,  0.1191,\n",
       "                       0.1426,  0.1172,  0.1328,  0.1680,  0.2441,  0.1182,  0.1099,  0.1367,\n",
       "                       0.1445,  0.1338,  0.0300,  0.1465,  0.1797,  0.1758,  0.1309,  0.1963,\n",
       "                       0.1279,  0.1289,  0.1836,  0.1309,  0.1553,  0.1533,  0.1689,  0.1123,\n",
       "                       0.1348,  0.1758,  0.1514,  0.1279,  0.1250,  0.1011,  0.1367,  0.1357,\n",
       "                       0.1494,  0.1475,  0.1377,  0.1650,  0.1514,  0.1846,  0.1777,  0.1768,\n",
       "                       0.1377,  0.1748,  0.1650,  0.1182,  0.1865,  0.1729,  0.1406,  0.1226,\n",
       "                       0.1953,  0.1494,  0.1523,  0.1377,  0.1592,  0.1299,  0.1807,  0.1650,\n",
       "                       0.1504,  0.1196,  0.1416,  0.1187,  0.1680,  0.1660,  0.1484,  0.1934])),\n",
       "             ('0.auto_model.encoder.block.2.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-6.2988e-02,  4.7461e-01, -7.5000e-01,  ..., -1.4844e-01,\n",
       "                       -3.8281e-01, -3.1836e-01],\n",
       "                      [-3.0859e-01, -2.2070e-01, -5.1562e-01,  ...,  8.0078e-01,\n",
       "                       -5.3125e-01, -6.2500e-02],\n",
       "                      [-6.5625e-01,  2.2266e-01, -1.7871e-01,  ..., -1.5991e-02,\n",
       "                        2.0117e-01, -2.6367e-01],\n",
       "                      ...,\n",
       "                      [ 1.3770e-01, -7.2656e-01,  2.9492e-01,  ..., -2.3926e-01,\n",
       "                        7.1094e-01, -3.5352e-01],\n",
       "                      [ 1.2793e-01, -1.5234e-01, -4.1797e-01,  ...,  7.3438e-01,\n",
       "                        2.2070e-01,  5.7220e-04],\n",
       "                      [-5.1270e-02,  1.1963e-01, -2.3828e-01,  ..., -8.4229e-03,\n",
       "                       -8.3984e-02, -2.7930e-01]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.0172, -0.5938,  0.1030,  ..., -0.0908, -0.1533,  0.4805],\n",
       "                      [-0.0708,  0.5547,  0.0483,  ..., -0.0732,  0.0776, -0.0723],\n",
       "                      [ 0.2012, -0.2559,  0.1196,  ...,  0.1069, -0.2910, -0.3672],\n",
       "                      ...,\n",
       "                      [ 0.3262,  0.0096,  0.0771,  ...,  0.1865,  0.3340,  0.0586],\n",
       "                      [ 0.1748, -0.2559,  0.3145,  ...,  0.1875,  0.1216, -0.4336],\n",
       "                      [-0.0330, -0.3105,  0.4199,  ..., -0.1582,  0.1455,  0.2637]])),\n",
       "             ('0.auto_model.encoder.block.2.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.7930,  0.9297,  0.8516,  0.9922,  0.5977,  0.7734,  0.3281,  0.7500,\n",
       "                       2.0312,  0.6680,  0.7578,  0.7539,  0.7227,  1.0391,  0.5586,  0.6016,\n",
       "                       0.8906,  0.8047,  0.9766,  0.6133,  0.6758,  0.7383,  0.5625,  0.5312,\n",
       "                       0.5273,  0.8516,  0.6133,  0.8594,  0.6758,  0.5391, -0.6719,  0.7109,\n",
       "                       0.7383,  0.5625,  0.8789,  0.6641,  1.1562,  1.2031,  0.5508,  0.9414,\n",
       "                       0.9062,  1.0391,  0.7891,  0.6836,  2.1719,  0.6758,  1.0078,  0.6484,\n",
       "                       0.7148,  1.0000,  0.9883,  0.7461,  1.1016,  0.6367,  0.6680,  0.6445,\n",
       "                       0.8906,  0.6445,  0.5781,  0.6250,  0.6992,  0.5508,  0.7617,  0.6602,\n",
       "                       0.6094,  0.8477,  0.7227,  0.6406,  1.0625,  0.6016,  0.6211,  0.9531,\n",
       "                       0.5586,  0.6914,  0.6484,  0.6133,  1.0703,  0.6992,  0.5859,  0.7344,\n",
       "                       0.6641,  0.7109,  0.5977,  1.1172,  0.7148,  0.7305,  1.3594,  0.7500,\n",
       "                       0.6523,  0.5664,  0.7227,  0.5938,  0.6055,  0.7422,  0.5742,  0.5742,\n",
       "                       0.5781,  0.5977,  0.6055,  0.6367,  0.8711,  0.6914,  0.9727,  0.5430,\n",
       "                       0.7070,  1.1719,  0.7070,  0.6406,  0.5312,  0.9805,  0.7422,  0.6367,\n",
       "                       0.6953,  0.6562,  0.6289,  0.5586,  0.7461,  0.5703,  1.4219,  0.5859,\n",
       "                       0.5117,  0.6016,  1.1172,  0.6523,  0.9180,  0.7148,  0.6602,  0.7148,\n",
       "                       0.6992,  0.8906,  1.2656,  0.8047,  0.5508,  0.5547,  0.6172,  1.0391,\n",
       "                       0.7656,  0.6641,  0.7656,  0.8125,  0.6719,  0.9375,  0.9766,  1.0859,\n",
       "                       0.7773,  0.9453,  0.6289,  0.8672,  0.6289,  0.7969,  0.6602,  0.7070,\n",
       "                       0.7148,  1.1328,  0.5938,  0.5664,  0.8320,  0.6055,  1.0156, -0.7305,\n",
       "                       0.5938,  0.6680,  0.7031,  1.8516,  0.6211,  0.5664,  0.5898,  0.8711,\n",
       "                       0.6562,  0.9492,  1.1484,  0.6172,  0.6094,  1.0391,  0.6641,  0.7188,\n",
       "                       0.7578,  0.8164,  0.9961,  0.1660,  0.6523,  0.5742,  0.6562, -0.5039,\n",
       "                       0.8086,  0.8906,  0.5859,  0.9688,  0.7812,  0.8008,  0.6641,  0.5625,\n",
       "                       0.5078,  0.6641,  0.4766,  0.3594,  0.7109,  0.8438,  0.7617,  0.5781,\n",
       "                       0.5039,  0.8047,  0.6836,  0.6445,  0.6211,  1.0391,  0.6914,  0.6367,\n",
       "                       0.6328,  0.6875,  0.9297,  0.6367,  0.8555,  0.8125,  0.6172,  0.7891,\n",
       "                       0.7773,  0.6680,  0.8164,  2.3125,  0.8203,  0.7266,  0.5781,  0.8125,\n",
       "                       0.8008,  0.7344,  0.8984,  1.0000,  0.6562,  0.8242,  0.6758,  0.7109,\n",
       "                       0.4883,  0.5117,  0.5273,  0.4961,  0.2295,  0.8594,  0.8906,  0.8828,\n",
       "                       1.2500,  0.7852,  0.5977,  0.6406,  0.5625,  0.5742,  0.6211,  0.7930,\n",
       "                       0.4902,  0.7969,  1.6484,  0.6016,  0.6406,  0.5859,  0.5625,  0.6250,\n",
       "                       0.5898,  0.6406,  1.0000,  0.7148,  1.0312,  0.4863,  0.6016,  0.6055,\n",
       "                       0.5898,  0.6875,  0.6289,  0.6602,  0.7773,  0.6953,  0.9062,  1.1953,\n",
       "                       1.0625,  1.2578,  1.1094,  0.7969,  0.5859,  0.9531,  0.6602,  0.7266,\n",
       "                       1.1484,  1.1406,  0.5703,  0.8984,  0.7422,  0.9766,  0.5078,  0.5820,\n",
       "                       0.7266,  1.3594,  0.5547,  0.6797,  0.6289,  0.5508,  0.6055,  0.4863,\n",
       "                       0.9297,  0.6562,  0.5938,  0.8516,  0.7969,  0.6445,  1.1797,  0.7930,\n",
       "                       0.7422,  0.6133,  0.6562,  0.6484,  0.8516,  0.7578,  0.6406,  0.8320,\n",
       "                       0.6562,  0.7578,  0.8711,  0.6797,  0.7148,  1.1250,  0.8477,  0.5781,\n",
       "                       0.5430,  0.9141,  0.7930,  0.5312,  0.9570,  0.5898,  0.9727,  0.9961,\n",
       "                       0.7617,  0.7188,  0.6133,  0.9883,  0.8594,  0.6836,  0.6055,  0.6484,\n",
       "                       0.5664,  0.6094,  0.7109,  1.1250,  0.8516,  0.8203,  0.7227,  0.6367,\n",
       "                       0.7344,  0.7148,  0.9219,  0.7812,  0.8672,  1.2578,  1.0156,  0.5547,\n",
       "                       0.6953,  0.5742,  0.8164,  0.7500,  0.7461,  1.1953,  0.6055,  0.9727,\n",
       "                       0.5469,  1.0469,  1.3203,  0.5273,  0.5547,  0.6211,  1.0000,  0.7773,\n",
       "                       0.5977,  0.8203,  0.8359,  0.8711,  0.0242,  0.6055,  0.9570,  0.5156,\n",
       "                       0.9609,  0.6250,  0.9727,  1.0781,  0.6289,  0.7578,  1.1172,  0.6133,\n",
       "                       0.6719,  1.1094,  0.6016,  0.9180,  0.4980,  0.8203,  0.6094,  1.1250,\n",
       "                       0.7461,  1.0547,  0.8672,  0.7109,  0.8477,  0.6289,  0.6914,  0.5508,\n",
       "                       0.6094,  0.4688,  0.5195,  0.6094,  0.6523,  0.6602,  1.0703,  0.6172,\n",
       "                       0.6406,  0.5820,  0.5586,  0.5352,  0.8164,  0.7891,  0.5508,  0.7695,\n",
       "                       0.5547,  0.5938,  0.2070,  0.9570,  1.5703,  0.7266,  0.5273,  0.3594,\n",
       "                       0.7344,  0.6016,  0.8086,  0.5781,  0.7500,  0.8945,  0.9219,  0.6133,\n",
       "                       1.0469,  0.5703,  1.0547,  0.5820,  0.6719,  0.5625,  0.8359,  0.9766,\n",
       "                       0.7695,  0.8711,  0.9180,  0.9062,  0.5352,  0.8164,  0.5977,  0.6914,\n",
       "                       1.1172,  0.6836,  0.7383,  0.5742,  0.6367,  0.6250,  0.8203,  0.9414,\n",
       "                       0.9180,  0.7812,  0.8242,  0.5195,  0.5273,  0.6445,  0.6328,  0.6836,\n",
       "                       0.6836,  0.6680,  0.5742,  0.7148,  0.4941,  0.5820,  0.5938,  0.5312,\n",
       "                       0.5312,  0.5430,  0.6641,  0.6133,  0.6250,  0.6602,  0.8398,  0.7070,\n",
       "                       0.8555,  1.3359,  0.6641,  0.5703,  1.3672,  0.6484,  0.5586,  0.9141,\n",
       "                       0.5273,  0.6562,  0.6680,  0.7617,  0.9062,  0.6055,  0.7578,  1.1562,\n",
       "                       0.7188,  0.7070,  0.6523,  0.5391,  0.6055,  0.9102,  0.8672,  1.0469,\n",
       "                       0.6367,  0.8477,  0.7617,  0.8438,  0.5312,  0.7383,  0.7812,  0.5898,\n",
       "                       1.6797,  0.5977,  0.8438,  0.5273,  0.7109,  0.7500,  1.3359,  0.7930,\n",
       "                       0.5156,  0.7578,  0.5391,  0.7578,  0.6641,  1.2656,  0.8320,  1.0703,\n",
       "                       0.6641,  0.9141,  0.9414,  0.6914,  0.7812,  0.7266,  0.8125,  0.7578,\n",
       "                       0.6875,  0.6562,  1.0625,  0.6797,  0.8555,  0.6016,  0.8320,  1.0234,\n",
       "                       0.6953,  0.6289,  0.6953,  0.6211,  0.6406,  0.7539,  1.0391,  1.1172,\n",
       "                       0.5664,  1.7031,  0.6250,  0.6641,  0.8984,  0.6367,  0.7148,  0.9609,\n",
       "                       0.7812,  0.6016,  1.1172,  1.1719,  0.7344,  0.8594,  0.1582,  0.7227,\n",
       "                       0.8125,  0.6328,  0.7812,  0.2217,  0.5547,  0.6758,  0.6875,  0.8047,\n",
       "                       0.6016,  0.9336,  0.5586,  1.3594,  0.7773,  0.5625,  0.5898,  0.6641,\n",
       "                       0.5547,  0.5312,  0.5781,  0.6484,  0.5781,  0.5508,  0.5820,  0.8398,\n",
       "                       0.5977,  0.6602,  0.7500,  0.6680,  0.7891,  0.6992,  1.0938,  0.6914,\n",
       "                       0.9414,  0.6445,  0.7344,  0.5430,  0.7852,  0.5195,  0.5742,  0.8398,\n",
       "                       0.9062,  0.7148,  0.7031,  0.5703,  0.8633,  0.5977,  0.7070,  0.8242,\n",
       "                       0.7188,  0.9023,  0.6992,  0.7656,  0.7539,  0.5586,  0.6602,  1.0234,\n",
       "                       0.6680,  0.6523,  0.7812,  1.0156,  0.6836,  0.9609,  0.8398,  0.6875,\n",
       "                       0.8633,  1.3203,  0.5508,  0.8867,  0.7539,  0.7070,  0.8789,  0.6445,\n",
       "                       0.8867,  0.7227,  0.6016,  0.7773,  0.7539,  0.7773,  0.8164,  0.5977,\n",
       "                       0.6055,  0.6289,  0.6562,  0.6211,  0.7578,  0.7266,  0.8672,  1.0234,\n",
       "                       0.6484,  0.7930,  0.8008,  0.8047,  0.7969,  0.1230,  0.7383,  1.1172,\n",
       "                       0.6523,  0.8633,  0.6836, -0.5664,  0.6680,  0.6016,  0.7070,  1.0156,\n",
       "                       0.6484,  0.6211,  0.7227,  0.6484,  0.6641,  0.6562,  0.6523,  0.7070,\n",
       "                       0.7344,  0.6445,  0.7617,  0.5586,  0.6523,  1.0312,  0.6523,  0.8320,\n",
       "                       0.5352,  0.7461,  0.6211,  0.8125,  1.2266,  0.5703,  0.7109,  0.6953,\n",
       "                       0.8125,  0.5859,  0.7188,  0.7695,  0.8047, -0.5195,  0.7578,  0.7227,\n",
       "                       0.7617,  0.5859,  0.6094,  1.2500,  1.1016,  0.5039,  1.9922,  0.7109,\n",
       "                       0.6797,  0.5508,  0.3125,  0.7461,  0.7578,  0.8516,  0.6914,  0.9023,\n",
       "                       0.6328,  0.7969,  0.9688,  0.6094,  0.7656,  0.8242,  0.7852,  0.5820,\n",
       "                       0.5938,  0.9414,  0.7617,  0.6328,  0.5781,  0.4961,  0.7109,  0.6406,\n",
       "                       0.6797,  0.7539,  0.6836,  0.7148,  0.7266,  0.9766,  0.8125,  0.8672,\n",
       "                       0.5898,  0.9141,  0.6758,  0.6172,  0.8398,  1.0078,  0.7188,  0.6602,\n",
       "                       1.1172,  0.7305,  0.7148,  0.6641,  0.8438,  0.4922,  0.8672,  0.7773,\n",
       "                       0.8125,  0.5508,  0.7617,  0.5625,  0.7578,  0.8516,  0.6680,  0.8945])),\n",
       "             ('0.auto_model.encoder.block.3.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0006, -0.0014, -0.0356,  ...,  0.0017,  0.0415,  0.0244],\n",
       "                      [ 0.0400,  0.0208, -0.0620,  ...,  0.0184, -0.0139, -0.0020],\n",
       "                      [ 0.0242, -0.0282, -0.0437,  ..., -0.0189, -0.0282, -0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0187,  0.0020, -0.0282,  ...,  0.0354, -0.0109, -0.0033],\n",
       "                      [ 0.0035,  0.0391, -0.0110,  ..., -0.0405,  0.0019, -0.0014],\n",
       "                      [ 0.0703,  0.0024,  0.0442,  ...,  0.0918,  0.0283, -0.0029]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.0757, -0.2314, -0.3672,  ..., -0.1641,  0.0977,  0.1084],\n",
       "                      [-0.1973, -0.0187,  0.0708,  ...,  0.1147, -0.8281, -0.0962],\n",
       "                      [ 0.1553,  0.3594,  0.3223,  ..., -0.3125,  0.0081,  0.2480],\n",
       "                      ...,\n",
       "                      [-0.3301,  0.3965,  0.0825,  ..., -0.2715,  0.0425, -0.0289],\n",
       "                      [-0.0366,  0.3711, -0.0188,  ..., -0.5898, -0.3477, -0.1650],\n",
       "                      [ 0.6406,  0.0447, -0.0283,  ...,  0.4297, -0.0405, -0.0903]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.1016,  0.0088,  0.6836,  ...,  0.0104, -0.1064,  0.0029],\n",
       "                      [ 0.6758, -0.7109, -0.0464,  ...,  0.0535, -0.7148, -0.3828],\n",
       "                      [ 0.6797,  0.7812, -0.7617,  ...,  0.0219,  0.2793,  0.2930],\n",
       "                      ...,\n",
       "                      [ 0.2559,  0.7891, -0.8281,  ...,  0.2051,  0.2910,  0.0256],\n",
       "                      [-0.4629,  0.0260, -0.3398,  ..., -0.2148, -0.2598, -0.2158],\n",
       "                      [ 0.0073, -0.3535, -0.3203,  ...,  0.7656,  0.1738,  0.1602]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.0212, -0.3242, -0.9648,  ...,  0.3789,  0.9141, -0.2188],\n",
       "                      [ 0.0593,  0.7070, -0.5703,  ...,  0.1089, -0.2061,  0.3340],\n",
       "                      [-0.7617, -0.1836,  1.0547,  ...,  0.0537,  0.5703, -0.2168],\n",
       "                      ...,\n",
       "                      [-0.1553, -0.0698, -0.4004,  ..., -0.3945,  0.5547, -0.1572],\n",
       "                      [-0.1914,  0.4668, -0.2969,  ..., -0.4238, -0.2480, -0.0972],\n",
       "                      [-0.0505,  0.0334, -0.1138,  ..., -0.0640, -0.2793,  0.5430]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1836,  0.1738,  0.1943,  0.2031,  0.1309,  0.1455,  0.0510,  0.1973,\n",
       "                       0.1426,  0.1621,  0.1689,  0.1719,  0.1152,  0.1787,  0.1338,  0.1641,\n",
       "                       0.1797,  0.1807,  0.2188,  0.1543,  0.1455,  0.1660,  0.0361,  0.1357,\n",
       "                       0.1494,  0.1875,  0.1357,  0.1680,  0.1670,  0.1187,  0.1377,  0.1533,\n",
       "                       0.1738,  0.1455,  0.1924,  0.1504,  0.1572,  0.2656,  0.1289,  0.2246,\n",
       "                       0.1719,  0.1826,  0.2070,  0.1816,  0.0942,  0.1416,  0.1543,  0.1465,\n",
       "                       0.1611,  0.1338,  0.2168,  0.1758,  0.1631,  0.1406, -0.0308,  0.1396,\n",
       "                       0.0354,  0.1562,  0.1602,  0.1455,  0.1855,  0.1191,  0.1670,  0.1455,\n",
       "                       0.1484,  0.0505,  0.1494,  0.1436,  0.2148,  0.1396,  0.1494,  0.2109,\n",
       "                       0.1416,  0.1768,  0.1445,  0.1504,  0.1670,  0.1689,  0.1328,  0.2002,\n",
       "                       0.1494,  0.1768,  0.1592,  0.2061,  0.1572,  0.1758,  0.2412,  0.1934,\n",
       "                       0.1660,  0.1562,  0.1758,  0.1260,  0.1416,  0.1611,  0.1416,  0.1465,\n",
       "                       0.1514,  0.1455,  0.1494,  0.1387,  0.1943,  0.1562,  0.1699,  0.1455,\n",
       "                       0.1494,  0.1709,  0.1553,  0.1543,  0.1387,  0.1465,  0.1787,  0.1660,\n",
       "                       0.1445,  0.1328,  0.1611,  0.1465,  0.1582,  0.1235,  0.2344,  0.1592,\n",
       "                       0.1328,  0.1445,  0.1836,  0.1475,  0.1982,  0.1670,  0.1631,  0.1562,\n",
       "                       0.1719,  0.1914,  0.1138,  0.1846,  0.1416,  0.1260,  0.1533,  0.1699,\n",
       "                       0.1787,  0.1562,  0.1631,  0.1934,  0.1533,  0.2041,  0.2109,  0.1670,\n",
       "                       0.1729,  0.2158,  0.1426,  0.1885,  0.1611,  0.1631,  0.1475,  0.1621,\n",
       "                       0.1650,  0.2441,  0.1221,  0.1387,  0.1836,  0.1475,  0.1631,  0.1436,\n",
       "                       0.1396,  0.1514,  0.1318,  0.1270,  0.1406,  0.1201,  0.1445,  0.1992,\n",
       "                       0.1650,  0.2363,  0.2119,  0.1650,  0.1602,  0.1699,  0.1582,  0.1680,\n",
       "                       0.1748,  0.1758,  0.1719,  0.0286,  0.1543,  0.1406,  0.1670,  0.1250,\n",
       "                       0.1904,  0.2227,  0.1338,  0.1934,  0.1562,  0.1689,  0.1309,  0.1514,\n",
       "                       0.0962,  0.1592,  0.1279,  0.0588,  0.1426,  0.1885,  0.1699,  0.1172,\n",
       "                       0.1426,  0.1689,  0.1602,  0.1475,  0.1729,  0.2139,  0.1475,  0.1465,\n",
       "                       0.1484,  0.1641,  0.1738,  0.1494,  0.1758,  0.1846,  0.1377,  0.1943,\n",
       "                       0.1709,  0.1543,  0.1758,  0.2109,  0.1089,  0.1768,  0.1172,  0.1689,\n",
       "                       0.1484,  0.1631,  0.1973,  0.2275,  0.1572,  0.1875,  0.1465,  0.1689,\n",
       "                       0.0947,  0.1338,  0.1387,  0.1011,  0.0640,  0.1836,  0.1699,  0.1992,\n",
       "                       0.2246,  0.1934,  0.1484,  0.1582,  0.1260,  0.1328,  0.1611,  0.1846,\n",
       "                       0.1309,  0.1631,  0.1455,  0.1191,  0.1367,  0.1426,  0.1196,  0.1348,\n",
       "                       0.1357,  0.1221,  0.1201,  0.1455,  0.2148,  0.1572,  0.1592,  0.1523,\n",
       "                       0.1367,  0.1484,  0.1523,  0.1602,  0.1689,  0.1514,  0.0371,  0.2422,\n",
       "                       0.1826,  0.0339,  0.2158,  0.1836,  0.1533,  0.1934,  0.1543,  0.1670,\n",
       "                       0.2256,  0.2295,  0.1426,  0.1553,  0.1719,  0.1807,  0.1357,  0.1484,\n",
       "                       0.1533,  0.1885,  0.1406,  0.1699,  0.1533,  0.1445,  0.1553,  0.1318,\n",
       "                       0.2119,  0.1562,  0.1367,  0.1836,  0.1953,  0.1543,  0.2305,  0.1807,\n",
       "                       0.1738,  0.1455,  0.1611,  0.1318,  0.1748,  0.1680,  0.1514,  0.1895,\n",
       "                       0.1416,  0.1118,  0.2197,  0.1631,  0.1099,  0.2402,  0.1279,  0.1455,\n",
       "                       0.1309,  0.1719,  0.1797,  0.1260,  0.2012,  0.1289,  0.1982,  0.2402,\n",
       "                       0.1729,  0.1777,  0.1465,  0.1729,  0.1670,  0.1582,  0.1602,  0.1475,\n",
       "                       0.1279,  0.1406,  0.1631,  0.2412,  0.1895,  0.1934,  0.1729,  0.1416,\n",
       "                       0.1768,  0.1758,  0.1582,  0.1631,  0.1885,  0.1611,  0.2109,  0.1445,\n",
       "                       0.1562,  0.1367,  0.1768,  0.1768,  0.1836,  0.2266,  0.1416,  0.2031,\n",
       "                       0.1514,  0.2363,  0.1787,  0.1040,  0.1377,  0.1338,  0.1553,  0.1562,\n",
       "                       0.1387,  0.1621,  0.1680,  0.1816,  0.0286,  0.1299,  0.2080,  0.1348,\n",
       "                       0.1846,  0.1416,  0.2314,  0.1416,  0.1475,  0.1816,  0.2197,  0.1465,\n",
       "                       0.1582,  0.0608,  0.1553,  0.2012,  0.1157,  0.1982,  0.1494,  0.1553,\n",
       "                       0.1602,  0.2158,  0.2002,  0.1768,  0.1865,  0.1436,  0.1602,  0.1426,\n",
       "                       0.1309,  0.0317,  0.1191,  0.1631,  0.1309,  0.1602,  0.2100,  0.1494,\n",
       "                       0.1455,  0.1357,  0.1465,  0.1387,  0.1621,  0.1572,  0.1514,  0.1582,\n",
       "                       0.1318,  0.1289,  0.0391,  0.1758,  0.2383,  0.1816,  0.1387,  0.0513,\n",
       "                       0.1748,  0.1553,  0.1709,  0.1426,  0.1709,  0.1748,  0.1895,  0.1621,\n",
       "                       0.1504,  0.1250,  0.2129,  0.1533,  0.1660,  0.1494,  0.1914,  0.1943,\n",
       "                       0.1816,  0.1582,  0.2012,  0.2266,  0.1553,  0.1670,  0.1328,  0.1670,\n",
       "                       0.2041,  0.1650,  0.1611,  0.1494,  0.1543,  0.1436,  0.1973,  0.1963,\n",
       "                       0.1865,  0.1562,  0.1758,  0.1475,  0.1377,  0.1338,  0.1475,  0.1631,\n",
       "                       0.1689,  0.1729,  0.1182,  0.1436,  0.1162,  0.1514,  0.1104,  0.1328,\n",
       "                       0.1172,  0.1250,  0.1650,  0.1582,  0.1504,  0.1426,  0.1455,  0.1738,\n",
       "                       0.1973,  0.1533,  0.1592,  0.1396,  0.2773,  0.1367,  0.1270,  0.1758,\n",
       "                      -0.1240,  0.1777,  0.1494,  0.1797,  0.0698,  0.1582,  0.1631,  0.1895,\n",
       "                       0.1719,  0.1680,  0.1445,  0.1387,  0.1406,  0.1895,  0.1797,  0.0369,\n",
       "                       0.1514,  0.2041,  0.1680,  0.1807,  0.1279,  0.1797,  0.1709,  0.1523,\n",
       "                       0.1035,  0.1719,  0.1709,  0.1406,  0.1621,  0.1445,  0.1777,  0.1895,\n",
       "                       0.1367,  0.1235,  0.1240,  0.1611,  0.1836,  0.1562,  0.2051,  0.2334,\n",
       "                       0.1436,  0.1816,  0.1924,  0.1660,  0.1650,  0.1650,  0.1572,  0.1768,\n",
       "                       0.1807,  0.1494,  0.1963,  0.1748,  0.1924,  0.1445,  0.1943,  0.2363,\n",
       "                       0.1455,  0.1426,  0.1953,  0.1484,  0.1514,  0.1201,  0.2080,  0.2236,\n",
       "                       0.1245,  0.1924,  0.1387,  0.1562,  0.1807,  0.1562,  0.1729,  0.1777,\n",
       "                       0.1689,  0.1216,  0.2090,  0.2275,  0.1748,  0.1924,  0.0294,  0.1553,\n",
       "                       0.1807,  0.1582,  0.1865,  0.0640,  0.1206,  0.1553,  0.1582,  0.1865,\n",
       "                       0.1602,  0.2129,  0.1406,  0.2295,  0.1689,  0.1465,  0.1396,  0.1514,\n",
       "                       0.1226,  0.1416,  0.1494,  0.1514,  0.1455,  0.1416,  0.1514,  0.1973,\n",
       "                       0.1543,  0.1729,  0.1660,  0.1670,  0.1631,  0.1689,  0.1963,  0.1494,\n",
       "                       0.2080,  0.1562,  0.1670,  0.0791,  0.1777,  0.1201,  0.1426,  0.1836,\n",
       "                       0.1973,  0.1553,  0.1621,  0.1367,  0.1611,  0.1445,  0.1553,  0.1924,\n",
       "                       0.1514,  0.1875,  0.1621,  0.1650,  0.1689,  0.1157,  0.1611,  0.2148,\n",
       "                       0.1748,  0.1572,  0.1650,  0.1670,  0.1611,  0.1543,  0.1934,  0.1543,\n",
       "                       0.1680,  0.2451,  0.1484,  0.1709,  0.1670,  0.1699,  0.2305,  0.1436,\n",
       "                       0.1748,  0.1729,  0.1279,  0.1689,  0.2051,  0.1855,  0.1650,  0.1406,\n",
       "                       0.1230,  0.1465,  0.1377,  0.1592,  0.1523,  0.1484,  0.1816,  0.1875,\n",
       "                       0.1543,  0.1494,  0.1826,  0.1875,  0.1826,  0.0295,  0.1631,  0.2656,\n",
       "                       0.1553,  0.1777,  0.1504,  0.1426,  0.1836,  0.1582,  0.1797,  0.1641,\n",
       "                       0.1475,  0.1250,  0.1914,  0.1611,  0.1562,  0.1494,  0.1406,  0.1719,\n",
       "                       0.1562,  0.1484,  0.1768,  0.1221,  0.1523,  0.0608,  0.1670,  0.1963,\n",
       "                       0.1445,  0.1768,  0.1309,  0.1953,  0.2305,  0.1367,  0.1650,  0.1436,\n",
       "                       0.1904,  0.1455,  0.1602,  0.1709,  0.1729,  0.1035,  0.1758,  0.1465,\n",
       "                       0.1494,  0.1367,  0.1582,  0.1816,  0.2314,  0.1406,  0.1084,  0.1621,\n",
       "                       0.1553,  0.1455,  0.0356,  0.1855,  0.1631,  0.1982,  0.1709,  0.2158,\n",
       "                       0.1250,  0.1338,  0.1924,  0.1504,  0.1748,  0.1562,  0.1826,  0.1318,\n",
       "                       0.1582,  0.1797,  0.1807,  0.1572,  0.1318,  0.1108,  0.1465,  0.1689,\n",
       "                       0.1602,  0.1680,  0.1631,  0.1816,  0.1572,  0.1885,  0.2021,  0.1846,\n",
       "                       0.1543,  0.1914,  0.1758,  0.1396,  0.2061,  0.1914,  0.1621,  0.1533,\n",
       "                       0.2197,  0.1748,  0.1719,  0.1504,  0.1650,  0.1475,  0.1875,  0.2002,\n",
       "                       0.1758,  0.1196,  0.1709,  0.1309,  0.1768,  0.1807,  0.1611,  0.2178])),\n",
       "             ('0.auto_model.encoder.block.3.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.1865, -0.5742,  1.0781,  ..., -0.3555, -0.4141, -0.2471],\n",
       "                      [-0.1230, -0.2236, -0.2832,  ...,  0.6250,  0.0045,  0.3301],\n",
       "                      [-0.2500,  0.1465,  0.2021,  ..., -0.5430,  0.6758,  0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0104, -0.5469, -0.4434,  ...,  0.4727, -0.6211,  0.0718],\n",
       "                      [-0.2617,  0.0403,  0.5898,  ..., -0.5586, -0.3555, -0.1699],\n",
       "                      [ 0.9844, -0.2441, -0.7070,  ...,  0.5117,  0.2520, -0.6758]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.6172, -0.0698,  0.0840,  ..., -0.3027,  0.1875,  0.1875],\n",
       "                      [-0.0201,  0.1021, -0.1426,  ..., -0.0845, -0.1089, -0.1245],\n",
       "                      [-0.1650, -0.0605, -0.1650,  ..., -0.2344,  0.5156,  0.0825],\n",
       "                      ...,\n",
       "                      [ 0.2832,  0.2871, -0.0732,  ..., -0.1260, -0.3926,  0.1445],\n",
       "                      [ 0.2041,  0.1572, -0.2832,  ...,  0.3008,  0.2812, -0.3379],\n",
       "                      [ 0.0189, -0.0635,  0.0957,  ..., -0.3457, -0.0854, -0.3047]])),\n",
       "             ('0.auto_model.encoder.block.3.layer.1.layer_norm.weight',\n",
       "              tensor([ 1.0078,  1.1250,  1.0938,  1.1953,  0.7109,  0.9805,  0.4766,  1.0859,\n",
       "                       2.8594,  0.8750,  0.9805,  1.0781,  0.8711,  1.1797,  0.7266,  0.7812,\n",
       "                       1.1484,  0.9609,  1.1484,  0.8398,  0.8828,  1.0156,  0.8945,  0.7852,\n",
       "                       0.7969,  1.1719,  0.7461,  0.9336,  1.0000,  0.7148,  0.9180,  0.8516,\n",
       "                       1.0156,  0.7578,  1.0938,  0.9180,  1.4297,  1.3438,  0.7773,  1.2656,\n",
       "                       0.9883,  1.2656,  1.1562,  0.8438,  2.9688,  0.8867,  1.1172,  0.9219,\n",
       "                       0.9336,  1.2656,  1.2188,  0.9336,  1.4531,  0.7383,  0.9648,  0.9336,\n",
       "                       1.1875,  0.9180,  0.8164,  0.9023,  1.0156,  0.7148,  1.0156,  0.7969,\n",
       "                       0.8203,  1.2188,  0.8359,  0.8594,  1.3984,  0.8359,  0.9102,  1.2812,\n",
       "                       0.7148,  0.9336,  0.8438,  0.8438,  1.2500,  0.9336,  0.8125,  0.9180,\n",
       "                       0.7461,  1.0078,  0.9141,  1.3125,  0.9453,  0.9531,  1.4219,  0.9531,\n",
       "                       0.8281,  0.7539,  0.9492,  0.8281,  0.8203,  1.0547,  0.7852,  0.7422,\n",
       "                       0.7461,  0.7891,  0.7617,  0.8281,  1.1562,  0.9375,  1.2578,  0.8203,\n",
       "                       0.8594,  1.4453,  0.8906,  0.9531,  0.7891,  1.4297,  1.0703,  0.9492,\n",
       "                       0.8438,  0.9297,  0.8750,  0.8516,  0.9414,  0.7500,  1.6328,  0.7930,\n",
       "                       0.7109,  0.8477,  1.2812,  0.7461,  1.2266,  1.0234,  0.8359,  0.8594,\n",
       "                       0.9688,  1.0625,  1.5781,  0.9531,  0.7383,  0.7656,  0.7461,  1.2578,\n",
       "                       1.0625,  0.8125,  0.9727,  1.0391,  0.9609,  1.1719,  1.1953,  1.3125,\n",
       "                       0.9766,  1.2031,  0.8047,  1.0625,  0.7500,  0.9844,  0.8594,  0.9258,\n",
       "                       0.8906,  1.3203,  0.7344,  0.8555,  1.0938,  0.7383,  1.1406,  0.8984,\n",
       "                       0.7070,  0.9141,  0.9023,  2.6875,  0.7500,  0.7617,  0.7812,  1.1719,\n",
       "                       0.8359,  1.2266,  1.2969,  0.8398,  0.8828,  1.3359,  0.9180,  0.9336,\n",
       "                       0.8594,  0.8516,  1.2891,  0.2637,  0.8789,  0.8008,  0.9023,  0.6797,\n",
       "                       1.0625,  1.0938,  0.7773,  1.1562,  0.9609,  1.0391,  0.8359,  0.8047,\n",
       "                       0.6719,  0.8281,  0.7891,  0.4844,  0.9258,  1.1406,  0.9414,  0.8008,\n",
       "                       0.7227,  1.0859,  0.9336,  0.8242,  0.8945,  1.1875,  1.0000,  0.9180,\n",
       "                       0.8867,  0.9453,  1.2344,  0.8086,  1.0781,  1.1016,  0.8477,  1.0625,\n",
       "                       0.9531,  0.8906,  1.0234,  2.6406,  1.0156,  0.9688,  0.7656,  0.9414,\n",
       "                       1.1406,  0.8789,  1.1406,  1.2578,  0.9883,  1.0312,  0.8633,  0.9648,\n",
       "                      -0.5508,  0.7500,  0.7656,  0.6211,  0.4297,  1.1406,  1.1250,  1.2109,\n",
       "                       1.5078,  1.1562,  0.8867,  0.8516,  0.7891,  0.8125,  0.8906,  1.0000,\n",
       "                       0.7266,  1.0000,  2.5000,  0.8594,  0.8281,  0.7266,  0.6875,  0.8711,\n",
       "                       0.8086,  0.8398,  1.1719,  0.7539,  1.2656,  0.5273,  0.9141,  0.8125,\n",
       "                       0.7734,  0.8828,  0.8203,  0.9023,  0.8438,  0.8672,  1.0781,  1.2969,\n",
       "                       1.4688,  1.6719,  1.2422,  1.0469,  0.8516,  1.1797,  0.9297,  0.9961,\n",
       "                       1.4453,  1.3594,  0.8828,  1.1953,  0.9727,  1.2422,  0.7461,  0.8047,\n",
       "                       0.9727,  1.6953,  0.7852,  0.8945,  0.8555,  0.7500,  0.8125,  0.7227,\n",
       "                       1.0703,  0.8320,  0.7773,  1.0703,  1.1172,  0.7656,  1.4219,  1.0312,\n",
       "                       0.9609,  0.7109,  0.8867,  0.8125,  0.9609,  1.0312,  0.7539,  1.0859,\n",
       "                       0.8281, -0.9023, -1.0547,  1.0469,  0.8359,  1.2266,  1.3047,  0.8633,\n",
       "                       0.7344,  1.1953,  1.0312,  0.7734,  1.2500,  0.7734,  1.2578,  1.2031,\n",
       "                       0.9453,  0.8711,  0.7969,  1.2656,  1.0625,  0.9102,  0.8477,  0.9531,\n",
       "                       0.7617,  0.8008,  0.9023,  1.4375,  1.0625,  0.9688,  1.0703,  0.8828,\n",
       "                       0.8516,  1.0312,  1.2734,  0.9570,  1.1797,  1.6484,  1.1562,  0.7500,\n",
       "                       0.8594,  0.7109,  1.0859,  0.9414,  0.9414,  1.2266,  0.7891,  1.1719,\n",
       "                       0.8477,  1.2734,  1.5234,  0.6680,  0.8242,  0.8867,  1.1406,  1.0391,\n",
       "                       0.8945,  1.0547,  1.0312,  1.0859, -0.0125,  0.7891,  1.2344,  0.7383,\n",
       "                       1.2734,  0.7617,  1.2969,  1.3984,  0.9180,  0.9961,  1.3047,  0.8984,\n",
       "                       0.8906,  1.4062,  0.7773,  1.2422,  0.7344,  1.1250,  0.8125,  1.6641,\n",
       "                       0.9922,  1.2031,  1.1016,  0.9492,  1.0703,  0.8008,  0.8906,  0.7930,\n",
       "                       0.7656,  0.7578,  0.7070,  0.8828,  0.7656,  0.9141,  1.2656,  0.8555,\n",
       "                       0.7852,  0.7773,  0.7852,  0.7344,  0.9609,  0.9844,  0.8711,  0.9570,\n",
       "                       0.7344,  0.8555,  0.3965,  1.2656,  1.8125,  0.9727,  0.7266,  0.4004,\n",
       "                       0.9336,  0.8047,  1.0547,  0.7773,  0.9336,  1.0234,  1.1719,  0.8633,\n",
       "                       1.3672,  0.7383,  1.3203,  0.7734,  0.9688,  0.8242,  1.1094,  1.1562,\n",
       "                       0.9805,  0.9648,  1.1250,  1.2344,  0.7656,  0.9492,  0.7617,  0.9648,\n",
       "                       1.2422,  0.8828,  0.9492,  0.7734,  0.9141,  0.7305,  1.0625,  1.2031,\n",
       "                       1.0078,  0.8594,  1.1016,  0.7461,  0.7930,  0.8242,  0.8555,  0.9492,\n",
       "                       1.0078,  0.8477,  0.7734,  0.8125,  0.6602,  0.8320,  0.8320,  0.7930,\n",
       "                       0.6875,  0.7617,  1.0078,  0.7344,  0.8281,  0.8594,  1.1484,  0.9102,\n",
       "                       1.0938,  1.8203,  0.9062,  0.7930,  1.4453,  0.8594,  0.8125,  1.1797,\n",
       "                       0.7539,  1.0000,  0.9180,  1.0469,  1.2188,  0.7773,  0.9883,  1.3516,\n",
       "                       0.9492,  0.9102,  0.8711,  0.7891,  0.8672,  0.9922,  1.0781,  1.3516,\n",
       "                       0.8672,  1.0547,  0.9297,  0.9961,  0.6992,  1.0391,  0.9141,  0.8125,\n",
       "                       2.4219,  0.7812,  1.0312,  0.7305,  0.8867,  0.9414,  1.5234,  0.9883,\n",
       "                       0.7461,  1.0312,  0.6758,  1.0078,  0.9531,  1.6875,  1.0234,  1.2344,\n",
       "                       0.8555,  1.0625,  1.2188,  0.8789,  0.9883,  0.9023,  0.9805,  1.0000,\n",
       "                       0.9180,  0.8984,  1.2422,  0.9219,  1.0859,  0.8008,  1.0703,  1.1641,\n",
       "                       0.8867,  0.8672,  0.9727,  0.9141,  0.7617,  0.7539,  1.3594,  1.3125,\n",
       "                       0.7539,  1.4688,  0.7656,  0.9609,  1.1719,  0.7852,  1.0391,  1.1484,\n",
       "                       1.1094,  0.7617,  1.4375,  1.3672,  0.9258,  1.0703,  0.0223,  0.9375,\n",
       "                       1.0078,  0.8359,  1.1094,  0.2754,  0.7070,  0.8711,  0.8750,  1.0000,\n",
       "                       0.7852,  1.1484,  0.7969,  1.5625,  0.9922,  0.7500,  0.8281,  0.8516,\n",
       "                       0.7500,  0.6875,  0.6953,  0.7969,  0.7812,  0.8164,  0.8008,  1.0234,\n",
       "                       0.8086,  0.8516,  0.8438,  0.9102,  0.9883,  0.8711,  1.3516,  0.8594,\n",
       "                       1.1406,  0.7617,  0.9805,  0.6328,  0.9961,  0.7461,  0.8320,  1.0781,\n",
       "                       1.2344,  0.8281,  0.9453,  0.7109,  1.1250,  0.8125,  0.9023,  1.0938,\n",
       "                       0.7969,  1.1016,  0.8164,  0.9531,  0.9961,  0.8086,  0.8008,  1.2812,\n",
       "                       0.9805,  0.7891,  0.9570,  1.2578,  0.8867,  1.1641,  1.0938,  0.8984,\n",
       "                       1.0156,  1.5547,  0.8086,  1.0547,  0.9609,  1.0234,  1.1797,  0.7852,\n",
       "                       1.0938,  0.9375,  0.7969,  1.0156,  1.0156,  0.9922,  1.0625,  0.7695,\n",
       "                       0.8438,  0.8203,  0.8320,  0.8867,  0.8633,  0.9492,  1.0234,  1.2109,\n",
       "                       0.8516,  1.0547,  0.9883,  1.1406,  1.0547,  0.0605,  0.8516,  1.3828,\n",
       "                       0.8945,  1.0938,  0.8789,  0.7773,  0.9570,  0.8008,  0.9531,  1.3828,\n",
       "                       0.9023,  0.8086,  0.8906,  0.7891,  0.8984,  0.8633,  0.8984,  0.8945,\n",
       "                       1.0234,  0.8477,  0.9570,  0.7188,  0.9375,  1.4844,  0.9102,  1.0938,\n",
       "                       0.8086,  0.9805,  0.7109,  0.9609,  1.4375,  0.8281,  0.8711,  0.8828,\n",
       "                       1.1094,  0.8008,  0.9023,  1.0391,  1.0078,  0.7930,  0.9883,  0.8164,\n",
       "                       0.9258,  0.8047,  0.7812,  1.6406,  1.3828,  0.7734,  2.6875,  0.9609,\n",
       "                       0.8516,  0.7891,  0.4395,  0.9609,  1.0859,  1.0859,  0.8359,  1.2188,\n",
       "                       0.8516,  1.0156,  1.2031,  0.8164,  0.9297,  0.9258,  0.9844,  0.7773,\n",
       "                       0.8125,  1.0781,  1.0078,  0.8438,  0.8320,  0.7109,  0.8242,  0.8711,\n",
       "                       0.9375,  0.9570,  0.8867,  0.9375,  0.9727,  1.2109,  1.0859,  1.1016,\n",
       "                       0.8516,  1.2031,  0.9375,  0.7812,  1.0000,  1.3281,  0.8906,  0.8438,\n",
       "                       1.3594,  0.8906,  0.8828,  0.8945,  0.8945,  0.8359,  1.0547,  0.9727,\n",
       "                       1.0391,  0.7422,  0.9180,  0.7383,  1.0234,  1.0156,  0.8945,  1.0859])),\n",
       "             ('0.auto_model.encoder.block.4.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0098, -0.0669,  0.0374,  ...,  0.0415,  0.0225,  0.0427],\n",
       "                      [ 0.0103, -0.0161, -0.0630,  ..., -0.0133,  0.0488, -0.0325],\n",
       "                      [ 0.0645, -0.0125,  0.0074,  ...,  0.0096, -0.0354, -0.0233],\n",
       "                      ...,\n",
       "                      [ 0.0359, -0.0542,  0.0172,  ..., -0.0101, -0.0579,  0.0053],\n",
       "                      [ 0.0291, -0.0349, -0.0500,  ...,  0.0159, -0.0070,  0.0120],\n",
       "                      [-0.0588,  0.0098, -0.0003,  ..., -0.0115,  0.0068,  0.0011]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.0762,  0.1963,  0.0986,  ..., -0.0261,  0.0306, -0.2539],\n",
       "                      [ 0.0991,  0.6523, -0.0815,  ..., -0.3105, -0.0913,  0.1875],\n",
       "                      [-0.0203,  0.0378, -0.2080,  ...,  0.0623,  0.0303, -0.1846],\n",
       "                      ...,\n",
       "                      [ 0.3867, -0.4414,  0.2100,  ...,  0.4531, -0.6094, -0.1816],\n",
       "                      [ 0.3125, -0.2773, -0.5938,  ..., -0.0197, -0.0918, -0.1348],\n",
       "                      [ 0.0007, -0.1758, -0.4395,  ...,  0.0908, -0.0874,  0.0903]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.1660,  0.5391,  0.0206,  ...,  0.0070, -0.1807,  0.3613],\n",
       "                      [ 0.4082, -0.2656,  0.4668,  ...,  0.0576,  0.2002, -0.1943],\n",
       "                      [-0.2051,  0.3008,  0.4688,  ..., -0.7266, -0.1875,  0.0532],\n",
       "                      ...,\n",
       "                      [-0.2988, -0.1875,  0.3809,  ...,  0.0718, -0.3418,  0.0505],\n",
       "                      [ 0.0177,  0.1221,  0.9648,  ...,  0.3027,  0.6094, -0.4629],\n",
       "                      [-1.0547,  0.7227, -0.6250,  ..., -0.1133,  0.1582, -0.1816]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.1426, -0.3398,  0.4590,  ...,  0.7383,  0.7266,  0.0913],\n",
       "                      [-0.9570,  0.1641,  0.0253,  ...,  0.1885, -0.0352,  0.2412],\n",
       "                      [ 0.2500, -0.1416, -0.3984,  ...,  1.2109, -0.1206, -0.3867],\n",
       "                      ...,\n",
       "                      [-0.4668,  0.4688,  0.5938,  ...,  0.7344, -0.2266,  0.3457],\n",
       "                      [ 0.4629, -0.3906,  0.2891,  ..., -0.3516,  0.6172,  0.3398],\n",
       "                      [-0.1533,  0.1602,  0.1836,  ...,  0.3672, -0.6133, -0.0674]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1631,  0.1533,  0.1748,  0.1807,  0.1260,  0.1543,  0.0457,  0.1904,\n",
       "                       0.0996,  0.1494,  0.1768,  0.1494,  0.1152,  0.1533,  0.1279,  0.1738,\n",
       "                       0.1592,  0.1650,  0.2002,  0.1523,  0.1211,  0.1445,  0.0432,  0.1367,\n",
       "                       0.1455,  0.1729,  0.1270,  0.1455,  0.1650,  0.1367,  0.1357,  0.1426,\n",
       "                       0.1650,  0.1152,  0.1650,  0.1445,  0.1465,  0.2109,  0.1226,  0.2070,\n",
       "                       0.1660,  0.1445,  0.1768,  0.1543,  0.0845,  0.1436,  0.1270,  0.1357,\n",
       "                       0.1572,  0.0693,  0.2100,  0.1504,  0.1279,  0.1318, -0.0310,  0.1406,\n",
       "                       0.0250,  0.1504,  0.1553,  0.1445,  0.1738,  0.1187,  0.1504,  0.1465,\n",
       "                       0.1289,  0.0410,  0.1504,  0.1245,  0.2080,  0.1445,  0.1533,  0.1777,\n",
       "                       0.1348,  0.1357,  0.1514,  0.1445,  0.1445,  0.1650,  0.1177,  0.1934,\n",
       "                       0.1426,  0.1660,  0.1621,  0.1885,  0.1611,  0.1562,  0.1992,  0.1699,\n",
       "                       0.1602,  0.1328,  0.1543,  0.1196,  0.1357,  0.1465,  0.1182,  0.1426,\n",
       "                       0.1387,  0.1406,  0.1357,  0.1235,  0.1758,  0.1768,  0.1250,  0.1367,\n",
       "                       0.1416,  0.1592,  0.1484,  0.1328,  0.1416,  0.0972,  0.1592,  0.1455,\n",
       "                       0.1338,  0.1260,  0.1621,  0.1367,  0.1475,  0.1177,  0.1914,  0.1426,\n",
       "                       0.1260,  0.1504,  0.1797,  0.1514,  0.1777,  0.1553,  0.1689,  0.1475,\n",
       "                       0.1562,  0.1836,  0.0864,  0.1787,  0.1348,  0.1143,  0.1387,  0.1416,\n",
       "                       0.1670,  0.1523,  0.1699,  0.1709,  0.1299,  0.2246,  0.1670,  0.1670,\n",
       "                       0.1631,  0.1738,  0.1416,  0.1738,  0.1387,  0.1787,  0.1416,  0.1592,\n",
       "                       0.1553,  0.2070,  0.1494,  0.1387,  0.1660,  0.1309,  0.1514,  0.1260,\n",
       "                       0.1328,  0.1416,  0.1128,  0.0771,  0.1221,  0.1211,  0.1455,  0.1660,\n",
       "                       0.1709,  0.2334,  0.1963,  0.1768,  0.1650,  0.1387,  0.1396,  0.1455,\n",
       "                       0.1670,  0.1318,  0.1592, -0.0210,  0.1455,  0.1299,  0.1523,  0.1328,\n",
       "                       0.1787,  0.1992,  0.1406,  0.1758,  0.1475,  0.1631,  0.1367,  0.1484,\n",
       "                       0.1055,  0.1504,  0.1328,  0.0588,  0.1416,  0.1924,  0.1562,  0.1270,\n",
       "                       0.1270,  0.1553,  0.1572,  0.1465,  0.1572,  0.1875,  0.1240,  0.1455,\n",
       "                       0.1514,  0.1396,  0.1855,  0.1475,  0.1738,  0.1777,  0.1436,  0.1797,\n",
       "                       0.1406,  0.1553,  0.1562,  0.1631,  0.1094,  0.1631,  0.1270,  0.1680,\n",
       "                       0.1289,  0.1494,  0.1836,  0.2119,  0.1494,  0.1816,  0.1230,  0.1553,\n",
       "                       0.0830,  0.1060,  0.1235,  0.0869,  0.0806,  0.1826,  0.1553,  0.1699,\n",
       "                       0.1924,  0.1758,  0.1475,  0.1504,  0.1182,  0.1367,  0.1533,  0.1748,\n",
       "                       0.1133,  0.1611,  0.1167,  0.1230,  0.1377,  0.1348,  0.1250,  0.1250,\n",
       "                       0.1465,  0.1279,  0.0962,  0.1367,  0.1895,  0.1357,  0.1426,  0.1445,\n",
       "                       0.1367,  0.1299,  0.1436,  0.1465,  0.1592,  0.1426,  0.0371,  0.2158,\n",
       "                       0.1592,  0.0300,  0.1943,  0.1680,  0.1357,  0.1582,  0.1426,  0.1455,\n",
       "                       0.2041,  0.2021,  0.1465,  0.1309,  0.1621,  0.1494,  0.1270,  0.1475,\n",
       "                       0.1338,  0.1426,  0.1250,  0.1484,  0.1357,  0.1377,  0.1416,  0.1143,\n",
       "                       0.1895,  0.1455,  0.1357,  0.1787,  0.1885,  0.1416,  0.2266,  0.1582,\n",
       "                       0.1533,  0.1177,  0.1455,  0.1543,  0.1484,  0.1416,  0.1592,  0.1797,\n",
       "                       0.1416,  0.1069,  0.1865,  0.1455,  0.0977,  0.2324,  0.1279,  0.1436,\n",
       "                       0.1206,  0.1533,  0.1592,  0.1484,  0.1582,  0.1309,  0.1836,  0.2334,\n",
       "                       0.1523,  0.1592,  0.1475,  0.1387,  0.1416,  0.1455,  0.1562,  0.1367,\n",
       "                       0.1201,  0.1299,  0.1514,  0.2188,  0.1768,  0.1758,  0.1494,  0.1270,\n",
       "                       0.1582,  0.1621,  0.1348,  0.1455,  0.1553,  0.1299,  0.1826,  0.1191,\n",
       "                       0.1338,  0.1582,  0.1562,  0.1514,  0.1689,  0.2080,  0.1279,  0.1777,\n",
       "                       0.1338,  0.1992,  0.1484,  0.0942,  0.1260,  0.1348,  0.1250,  0.1465,\n",
       "                       0.1328,  0.1553,  0.1543,  0.1689,  0.0275,  0.1133,  0.1865,  0.1260,\n",
       "                       0.1494,  0.1436,  0.2070,  0.1216,  0.1475,  0.1582,  0.1953,  0.1533,\n",
       "                       0.1328,  0.0371,  0.1494,  0.1553,  0.1196,  0.1807,  0.1406,  0.1279,\n",
       "                       0.1494,  0.2061,  0.1689,  0.1455,  0.1631,  0.1523,  0.1504,  0.1211,\n",
       "                       0.1328, -0.0223,  0.1138,  0.1582,  0.1338,  0.1445,  0.1807,  0.1211,\n",
       "                       0.1484,  0.1289,  0.1367,  0.1260,  0.1309,  0.1475,  0.1299,  0.1494,\n",
       "                       0.1035,  0.1387,  0.0454,  0.1670,  0.1973,  0.1621,  0.1177,  0.0544,\n",
       "                       0.1709,  0.1445,  0.1787,  0.1357,  0.1680,  0.1572,  0.1797,  0.1396,\n",
       "                       0.1318,  0.1191,  0.1777,  0.1387,  0.1523,  0.1299,  0.1826,  0.2070,\n",
       "                       0.1729,  0.1309,  0.1719,  0.2070,  0.1309,  0.1670,  0.1406,  0.1611,\n",
       "                       0.2109,  0.1758,  0.1367,  0.1396,  0.1484,  0.1348,  0.1689,  0.1729,\n",
       "                       0.1689,  0.1475,  0.1641,  0.1128,  0.1367,  0.1211,  0.1348,  0.1592,\n",
       "                       0.1621,  0.1582,  0.1006,  0.1387,  0.1289,  0.1396,  0.1206,  0.1206,\n",
       "                       0.1079,  0.1270,  0.1602,  0.1523,  0.1387,  0.1387,  0.1060,  0.1514,\n",
       "                       0.1865,  0.1201,  0.1572,  0.1367,  0.2383,  0.1328,  0.1436,  0.1680,\n",
       "                       0.1157,  0.1641,  0.1553,  0.1689,  0.0513,  0.1367,  0.1484,  0.1689,\n",
       "                       0.1484,  0.1475,  0.1240,  0.1226,  0.1387,  0.1729,  0.1631,  0.0337,\n",
       "                       0.1631,  0.1680,  0.1631,  0.1699,  0.1270,  0.1602,  0.1885,  0.1328,\n",
       "                       0.0835,  0.1406,  0.1865,  0.1445,  0.1436,  0.1245,  0.1602,  0.1758,\n",
       "                       0.1133,  0.1299,  0.1104,  0.1465,  0.1699,  0.1235,  0.1846,  0.2070,\n",
       "                       0.1455,  0.1797,  0.1826,  0.1533,  0.1543,  0.1436,  0.1445,  0.1592,\n",
       "                       0.1523,  0.1602,  0.1777,  0.1660,  0.1797,  0.1387,  0.1611,  0.2051,\n",
       "                       0.1318,  0.1348,  0.1826,  0.1543,  0.1416,  0.0889,  0.1855,  0.1934,\n",
       "                       0.1089,  0.2061,  0.1221,  0.1562,  0.1494,  0.1504,  0.1543,  0.1553,\n",
       "                       0.1572,  0.1074,  0.1855,  0.2002,  0.1592,  0.1875,  0.0250,  0.1406,\n",
       "                       0.1514,  0.1416,  0.1855,  0.0527,  0.1177,  0.1396,  0.1592,  0.1582,\n",
       "                       0.1455,  0.2090,  0.1318,  0.1797,  0.1377,  0.1299,  0.1553,  0.1348,\n",
       "                       0.1177,  0.1221,  0.1338,  0.1270,  0.1260,  0.1328,  0.1514,  0.1807,\n",
       "                       0.1455,  0.1543,  0.1631,  0.1611,  0.1396,  0.1357,  0.1895,  0.1494,\n",
       "                       0.2002,  0.1465,  0.1660,  0.0957,  0.1562,  0.1113,  0.1348,  0.1553,\n",
       "                       0.1807,  0.1465,  0.1582,  0.1211,  0.1611,  0.1436,  0.1416,  0.2031,\n",
       "                       0.1279,  0.1748,  0.1582,  0.1523,  0.1670,  0.1260,  0.1367,  0.1826,\n",
       "                       0.1553,  0.1328,  0.1562,  0.1416,  0.1504,  0.1250,  0.1738,  0.1426,\n",
       "                       0.1572,  0.2148,  0.1338,  0.1445,  0.1592,  0.1650,  0.2100,  0.1328,\n",
       "                       0.1621,  0.1631,  0.1260,  0.1602,  0.1787,  0.1729,  0.1553,  0.1260,\n",
       "                       0.1289,  0.1260,  0.1357,  0.1455,  0.1562,  0.1245,  0.1641,  0.1748,\n",
       "                       0.1494,  0.1602,  0.1865,  0.1738,  0.1572,  0.0255,  0.1289,  0.2295,\n",
       "                       0.1426,  0.1641,  0.1406,  0.1377,  0.1689,  0.1465,  0.1572,  0.1348,\n",
       "                       0.1406,  0.1279,  0.1748,  0.1484,  0.1396,  0.1494,  0.1367,  0.1533,\n",
       "                       0.1553,  0.1416,  0.1631,  0.1187,  0.1523,  0.0811,  0.1357,  0.1836,\n",
       "                       0.1387,  0.1670,  0.1270,  0.1768,  0.1943,  0.1299,  0.1484,  0.1377,\n",
       "                       0.1943,  0.1279,  0.1543,  0.1631,  0.1484,  0.1074,  0.1631,  0.1270,\n",
       "                       0.1484,  0.1309,  0.1348,  0.1562,  0.1963,  0.1270,  0.0713,  0.1533,\n",
       "                       0.1494,  0.1396,  0.0388,  0.1553,  0.1504,  0.1787,  0.1426,  0.1816,\n",
       "                       0.1309,  0.1177,  0.1826,  0.1377,  0.1768,  0.1504,  0.1709,  0.1128,\n",
       "                       0.1465,  0.1680,  0.1582,  0.1484,  0.1270,  0.1104,  0.1357,  0.1543,\n",
       "                       0.1650,  0.1602,  0.1572,  0.1885,  0.1631,  0.1748,  0.1973,  0.1865,\n",
       "                       0.1475,  0.1670,  0.1709,  0.1367,  0.1855,  0.1768,  0.1475,  0.1504,\n",
       "                       0.1885,  0.1680,  0.1592,  0.1514,  0.1465,  0.1279,  0.1855,  0.1885,\n",
       "                       0.1523,  0.1045,  0.1562,  0.1260,  0.1748,  0.1855,  0.1436,  0.2002])),\n",
       "             ('0.auto_model.encoder.block.4.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.0894,  0.1060, -0.3496,  ...,  0.5898, -0.2119, -0.1699],\n",
       "                      [ 0.9258,  0.1182, -0.1250,  ..., -0.6914,  0.3066, -0.1641],\n",
       "                      [ 0.1475, -0.5742, -0.1699,  ...,  0.1738, -0.2256,  0.4434],\n",
       "                      ...,\n",
       "                      [ 0.2793,  0.0286, -0.4785,  ...,  0.8594,  0.1973,  0.4668],\n",
       "                      [-0.1748, -0.1738,  0.2197,  ...,  1.3984, -1.1484,  0.1826],\n",
       "                      [ 0.2129, -0.5000,  0.3691,  ...,  0.1865,  0.4629,  0.0269]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 8.3008e-02, -6.7871e-02, -4.7363e-02,  ...,  2.3535e-01,\n",
       "                        3.0273e-01,  9.2773e-03],\n",
       "                      [-5.1270e-02,  4.1797e-01, -5.2734e-01,  ..., -4.7607e-02,\n",
       "                       -2.3926e-01,  1.1536e-02],\n",
       "                      [-4.1504e-02, -6.2988e-02, -6.1035e-02,  ..., -1.5717e-03,\n",
       "                        1.0791e-01, -1.0205e-01],\n",
       "                      ...,\n",
       "                      [ 1.1182e-01,  1.1621e-01,  1.7480e-01,  ...,  5.5420e-02,\n",
       "                        1.9409e-02, -3.6621e-02],\n",
       "                      [ 5.5859e-01, -4.4336e-01,  4.2725e-02,  ..., -1.0449e-01,\n",
       "                        1.3065e-04, -9.9609e-02],\n",
       "                      [ 4.2725e-02, -2.5391e-01, -3.0273e-01,  ..., -2.0508e-01,\n",
       "                        4.8633e-01, -9.1797e-02]])),\n",
       "             ('0.auto_model.encoder.block.4.layer.1.layer_norm.weight',\n",
       "              tensor([1.1797, 1.1562, 1.0312, 1.2344, 0.8398, 1.0859, 0.4805, 1.1953, 2.6094,\n",
       "                      1.0391, 1.1328, 1.1094, 0.9492, 1.1875, 0.8906, 0.8828, 1.2422, 1.0859,\n",
       "                      1.2500, 0.9141, 0.9648, 1.0156, 1.2266, 0.8867, 0.8594, 1.1719, 0.8125,\n",
       "                      1.0859, 1.0078, 0.8711, 1.0000, 0.9531, 1.0625, 0.8164, 1.0938, 0.9414,\n",
       "                      1.5234, 1.4297, 0.8516, 1.4609, 1.1094, 1.3438, 1.3047, 0.9609, 3.0469,\n",
       "                      0.9648, 1.1406, 0.8945, 1.1406, 1.0781, 1.2812, 1.0547, 1.6328, 0.9023,\n",
       "                      0.9844, 1.0156, 1.1875, 0.9531, 0.9180, 0.9531, 1.1641, 0.7852, 1.0781,\n",
       "                      0.8984, 0.9922, 1.4766, 0.9805, 0.9922, 1.4141, 0.9961, 0.9727, 1.3984,\n",
       "                      0.9414, 1.0156, 0.9180, 0.9453, 1.3125, 1.0156, 0.8945, 1.0312, 0.8516,\n",
       "                      1.0625, 1.0312, 1.3047, 0.9570, 1.0547, 1.4609, 1.0547, 1.0391, 0.9688,\n",
       "                      1.1562, 0.8711, 0.8750, 1.1484, 0.9492, 0.8711, 0.8984, 0.8867, 0.8711,\n",
       "                      0.8828, 1.1328, 0.9688, 1.2266, 0.9141, 0.8867, 1.5312, 0.9883, 0.9766,\n",
       "                      0.8398, 1.6406, 1.1797, 0.9609, 0.9180, 1.0391, 0.9727, 0.9023, 1.0000,\n",
       "                      0.8555, 1.5078, 0.9062, 0.7734, 0.8984, 1.3125, 1.0078, 1.1719, 1.0781,\n",
       "                      0.9883, 0.9570, 1.0078, 1.1406, 1.5469, 1.0312, 0.8164, 0.8359, 0.9258,\n",
       "                      1.1875, 1.2266, 1.0312, 1.1484, 1.1094, 0.8945, 1.3359, 1.2734, 1.2031,\n",
       "                      0.9688, 1.2656, 0.9648, 1.1328, 0.8906, 1.1406, 0.9453, 1.0156, 1.0625,\n",
       "                      1.3672, 0.9531, 0.8203, 1.0938, 0.9258, 1.2031, 0.9648, 0.8125, 0.9180,\n",
       "                      0.9766, 2.9531, 0.8633, 0.8438, 0.9766, 1.1172, 1.0234, 1.2656, 1.3125,\n",
       "                      0.9219, 0.9844, 1.4375, 0.9805, 1.0469, 1.1250, 0.7852, 1.2969, 0.3086,\n",
       "                      0.9375, 0.9102, 1.0547, 0.8398, 0.9844, 1.2891, 0.8906, 1.2188, 1.0703,\n",
       "                      1.1484, 0.9180, 0.8984, 0.7812, 0.9414, 0.9297, 0.5664, 0.9492, 1.1328,\n",
       "                      0.9414, 0.8125, 0.8789, 1.0703, 0.8828, 0.9492, 0.9062, 1.2422, 1.0859,\n",
       "                      0.9688, 0.8945, 0.9453, 1.2188, 1.0000, 1.1016, 1.1406, 0.9688, 1.0938,\n",
       "                      0.9492, 1.0234, 1.1172, 2.7031, 0.9648, 1.0078, 0.8672, 1.0312, 1.1406,\n",
       "                      0.9766, 1.2969, 1.3984, 0.9219, 1.0625, 0.9805, 1.1094, 0.6445, 0.8203,\n",
       "                      0.8555, 0.6641, 0.4434, 1.1094, 1.2109, 1.2969, 1.4453, 1.2031, 0.9844,\n",
       "                      1.0156, 0.8516, 0.8906, 0.9609, 1.0547, 0.8398, 0.9844, 2.9375, 0.8789,\n",
       "                      0.8867, 0.9258, 0.8398, 0.9180, 0.9102, 0.8945, 1.1641, 0.9609, 1.3047,\n",
       "                      0.4375, 0.9023, 0.9297, 0.8828, 0.9570, 0.8867, 0.9375, 0.9219, 0.9531,\n",
       "                      0.8711, 1.2969, 1.4844, 1.6719, 1.2656, 1.1094, 0.9609, 1.2422, 1.0469,\n",
       "                      1.0078, 1.4297, 1.3203, 0.8711, 1.2969, 1.1406, 1.2969, 0.8477, 0.8633,\n",
       "                      1.0234, 1.9219, 0.8789, 0.9258, 0.8750, 0.7969, 0.9414, 0.8633, 1.1719,\n",
       "                      0.8477, 0.9297, 1.2344, 1.1641, 0.9297, 1.4375, 0.9805, 1.0312, 0.8281,\n",
       "                      0.9414, 0.9102, 1.0547, 1.1094, 0.9414, 1.2188, 0.9180, 0.8750, 1.1562,\n",
       "                      0.9688, 0.8984, 1.3438, 1.2734, 0.9023, 0.8086, 1.2422, 1.0156, 0.8945,\n",
       "                      1.2656, 0.9258, 1.2734, 1.3984, 0.9570, 0.9570, 0.9336, 1.3516, 1.1406,\n",
       "                      0.9297, 0.8672, 1.0625, 0.7812, 0.8164, 1.0625, 1.4297, 0.9805, 1.1016,\n",
       "                      1.0938, 0.9492, 1.0469, 1.0469, 1.3672, 1.0469, 1.1797, 1.8516, 1.1875,\n",
       "                      0.8633, 0.9961, 0.9609, 1.1719, 0.9648, 1.0547, 1.2891, 0.9102, 1.1641,\n",
       "                      0.9023, 1.2500, 1.4453, 0.7344, 0.8398, 1.0156, 1.1953, 1.1641, 0.9414,\n",
       "                      1.0938, 1.1328, 1.1641, 0.1631, 0.9336, 1.3750, 0.8672, 1.2344, 0.9727,\n",
       "                      1.3516, 1.3281, 0.9609, 1.0703, 1.3281, 0.9727, 0.9453, 1.3203, 0.8867,\n",
       "                      1.2188, 0.7344, 1.1094, 0.8867, 1.9141, 1.0156, 1.2734, 1.1953, 0.9883,\n",
       "                      1.1875, 0.9141, 1.0312, 0.8906, 0.7812, 0.6641, 0.9219, 0.8320, 0.8242,\n",
       "                      0.9258, 1.2422, 1.0000, 0.8477, 0.8867, 0.8867, 0.8633, 1.0859, 1.0859,\n",
       "                      0.8906, 1.0391, 0.8867, 0.8750, 0.3633, 1.4375, 1.7656, 1.0625, 0.8711,\n",
       "                      0.4102, 1.0938, 0.8438, 1.1250, 0.9258, 1.0781, 1.1016, 1.1406, 0.9414,\n",
       "                      1.4453, 0.8633, 1.2969, 0.8672, 1.0156, 0.9102, 1.1094, 1.2109, 1.1406,\n",
       "                      1.0312, 1.1250, 1.2578, 0.9375, 1.0312, 0.8828, 1.0703, 1.2891, 1.0391,\n",
       "                      1.0234, 0.7930, 0.9375, 0.8672, 1.1484, 1.2734, 1.0938, 1.0078, 1.1172,\n",
       "                      0.8594, 0.8359, 0.9062, 0.9062, 1.0469, 1.0000, 1.0391, 0.8203, 0.8711,\n",
       "                      0.8125, 0.8867, 0.8516, 0.8711, 0.7656, 0.8555, 1.0469, 0.9219, 0.8555,\n",
       "                      0.8906, 1.2344, 1.0156, 1.0625, 1.9844, 1.0547, 0.9727, 1.4844, 0.9688,\n",
       "                      0.8711, 1.2188, 0.7695, 1.0547, 1.0078, 1.1328, 1.1562, 0.8398, 1.0859,\n",
       "                      1.2266, 1.0078, 0.9492, 0.9336, 0.8320, 0.9219, 1.1484, 1.2266, 1.5859,\n",
       "                      0.9336, 1.0469, 1.0156, 1.1094, 0.8477, 1.0938, 1.0781, 0.8047, 2.5156,\n",
       "                      0.8438, 1.2266, 0.9336, 1.0156, 0.9727, 1.5469, 1.1562, 0.8203, 1.0391,\n",
       "                      0.7148, 0.8828, 1.0469, 2.0156, 1.1328, 1.4141, 0.9961, 1.0234, 1.2344,\n",
       "                      0.9727, 1.0000, 1.0938, 1.1016, 1.1328, 0.9883, 0.9883, 1.2578, 1.0547,\n",
       "                      1.2031, 0.9062, 1.1016, 1.3750, 0.9531, 0.9531, 0.9844, 0.9609, 0.9141,\n",
       "                      0.6992, 1.3281, 1.3516, 0.8242, 1.3281, 0.9492, 1.0312, 1.1953, 0.9453,\n",
       "                      1.0938, 1.0469, 1.2500, 0.7891, 1.3906, 1.3906, 0.9531, 1.0547, 0.0124,\n",
       "                      1.0078, 1.1641, 0.9023, 1.0391, 0.2354, 0.8008, 0.9102, 0.9883, 1.0859,\n",
       "                      1.0000, 1.0781, 0.8867, 1.4844, 0.9336, 0.8672, 0.9961, 0.8906, 0.8164,\n",
       "                      0.7383, 0.9688, 0.9219, 0.9141, 0.8789, 0.9570, 1.1016, 0.9023, 0.9531,\n",
       "                      0.9375, 0.9961, 0.9883, 1.0703, 1.4141, 0.9258, 1.3047, 0.9258, 0.9766,\n",
       "                      0.6602, 1.0234, 0.8203, 0.8984, 1.0938, 1.2734, 0.9805, 1.0781, 0.8867,\n",
       "                      1.1250, 0.9102, 1.0234, 1.2188, 1.0391, 1.1250, 0.9531, 0.9805, 1.1797,\n",
       "                      0.8320, 1.0078, 1.3281, 0.9570, 0.9062, 1.0703, 1.2422, 0.9922, 1.1875,\n",
       "                      1.1562, 0.9531, 1.1250, 1.4297, 0.8789, 0.9727, 1.0781, 1.0391, 1.2812,\n",
       "                      0.8789, 1.1562, 1.1328, 0.8867, 1.0859, 1.0547, 1.1172, 1.1016, 0.7852,\n",
       "                      0.9336, 0.9297, 0.9492, 0.9727, 0.9141, 1.0000, 1.1484, 1.3359, 0.7656,\n",
       "                      1.0547, 1.0625, 1.2344, 1.0156, 0.0903, 1.0391, 1.4219, 0.9531, 1.1328,\n",
       "                      0.9844, 0.8828, 0.9336, 0.9805, 1.0938, 1.5234, 0.9219, 0.9336, 0.9609,\n",
       "                      0.8945, 0.9492, 0.9961, 0.9297, 1.0078, 1.0156, 0.9648, 1.0547, 0.7812,\n",
       "                      1.0547, 1.5000, 1.0312, 1.1484, 0.7734, 1.0781, 0.9844, 1.1562, 1.4844,\n",
       "                      0.9023, 0.9062, 0.8828, 1.1719, 0.8164, 1.0078, 0.9844, 1.0156, 0.8125,\n",
       "                      0.9805, 0.9609, 1.0781, 0.8594, 0.9141, 1.5469, 1.3594, 0.8672, 2.6250,\n",
       "                      0.9922, 0.9453, 0.9336, 0.4277, 1.0859, 1.0859, 1.2734, 1.0469, 1.2109,\n",
       "                      0.9336, 1.0781, 1.1953, 0.9023, 1.0859, 1.0469, 1.1250, 0.9180, 0.9180,\n",
       "                      1.2344, 1.2109, 0.9688, 0.8867, 0.7422, 1.0234, 0.9883, 1.0312, 0.9883,\n",
       "                      0.8984, 1.0547, 1.0469, 1.2891, 1.2188, 1.1328, 0.9453, 1.1406, 0.9805,\n",
       "                      0.8281, 1.2109, 1.3672, 1.0078, 0.9609, 1.4141, 0.9844, 1.0703, 0.9844,\n",
       "                      1.0547, 0.8672, 1.0938, 1.0938, 1.0625, 0.7500, 1.0156, 0.9453, 0.9805,\n",
       "                      1.0391, 0.9336, 1.2578])),\n",
       "             ('0.auto_model.encoder.block.5.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0554, -0.0227, -0.0021,  ...,  0.0439,  0.0376,  0.0342],\n",
       "                      [ 0.0330,  0.0447, -0.0108,  ..., -0.0640, -0.0038,  0.0679],\n",
       "                      [ 0.0361,  0.0289, -0.0544,  ...,  0.0184,  0.0488,  0.0532],\n",
       "                      ...,\n",
       "                      [-0.0674, -0.0283, -0.0075,  ...,  0.0430,  0.0522, -0.0210],\n",
       "                      [-0.0085, -0.0583,  0.0396,  ...,  0.0737,  0.0201, -0.0027],\n",
       "                      [ 0.0300,  0.0413, -0.0391,  ..., -0.0260, -0.0610, -0.0065]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.3691,  0.2695, -0.2061,  ...,  0.2910,  0.5234,  0.3359],\n",
       "                      [ 0.3652,  0.1602, -0.6367,  ..., -0.0918,  0.0742,  0.3516],\n",
       "                      [ 0.0854, -0.5195, -0.5273,  ..., -0.0481,  0.3379,  0.4844],\n",
       "                      ...,\n",
       "                      [-0.2871, -0.1426,  0.3164,  ...,  0.4355,  0.4473,  0.0815],\n",
       "                      [-0.0077,  0.1562, -0.1060,  ..., -0.0957,  0.2432, -0.0786],\n",
       "                      [ 0.0540,  0.0505, -0.0713,  ...,  0.3301, -0.5000,  0.0664]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.1650,  0.5391,  0.3984,  ..., -0.1465,  0.0537,  0.3711],\n",
       "                      [ 0.0952,  0.3984, -0.6211,  ..., -0.3457,  0.0520,  0.4863],\n",
       "                      [ 0.3535, -1.1094,  0.0786,  ...,  0.4316,  0.9375, -0.6367],\n",
       "                      ...,\n",
       "                      [ 0.2461, -0.5352,  0.2432,  ...,  0.1064, -0.9609, -1.0156],\n",
       "                      [ 0.1650, -0.0161,  0.8633,  ...,  0.0776, -0.3047,  0.0776],\n",
       "                      [-0.4570,  0.4922, -0.0317,  ..., -0.4453,  0.1011,  0.1475]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.3926, -0.2109, -0.6484,  ..., -0.2422, -0.9844, -0.1245],\n",
       "                      [-0.3125, -0.0444,  1.8828,  ..., -0.3789, -0.0315,  0.4668],\n",
       "                      [-0.4258,  0.9883, -0.1494,  ...,  0.8008,  1.0781, -0.7070],\n",
       "                      ...,\n",
       "                      [ 0.0415, -0.1084, -0.3867,  ..., -0.3945, -0.0134, -0.4121],\n",
       "                      [ 0.3320, -0.2793, -0.4004,  ...,  0.0815, -0.1074, -0.2207],\n",
       "                      [ 0.1816,  0.0535,  0.4824,  ...,  0.5430,  0.2637,  0.6445]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1670,  0.1592,  0.1738,  0.1846,  0.1270,  0.1387,  0.0476,  0.1875,\n",
       "                       0.0991,  0.1582,  0.1885,  0.1699,  0.1050,  0.1592,  0.1279,  0.1699,\n",
       "                       0.1514,  0.1699,  0.1787,  0.1465,  0.1279,  0.1689,  0.0530,  0.1436,\n",
       "                       0.1514,  0.1846,  0.1426,  0.1562,  0.1650,  0.1436,  0.1396,  0.1436,\n",
       "                       0.1660,  0.1318,  0.1787,  0.1523,  0.1367,  0.2178,  0.1250,  0.1963,\n",
       "                       0.1631,  0.1738,  0.1719,  0.1650, -0.0757,  0.1514,  0.1123,  0.1533,\n",
       "                       0.1699,  0.0564,  0.2002,  0.1650,  0.1406,  0.1357, -0.0204,  0.1387,\n",
       "                       0.0349,  0.1406,  0.1768,  0.1484,  0.1699,  0.1270,  0.1719,  0.1396,\n",
       "                       0.1445,  0.0503,  0.1475,  0.1357,  0.1924,  0.1426,  0.1553,  0.1895,\n",
       "                       0.1270,  0.1670,  0.1514,  0.1484,  0.1426,  0.1748,  0.1377,  0.1904,\n",
       "                       0.1533,  0.1562,  0.1572,  0.1924,  0.1553,  0.1572,  0.2090,  0.1758,\n",
       "                       0.1670,  0.1367,  0.1553,  0.1309,  0.1328,  0.1436,  0.1240,  0.1406,\n",
       "                       0.1719,  0.1367,  0.1387,  0.1357,  0.1895,  0.1445,  0.1260,  0.1357,\n",
       "                       0.1533,  0.1475,  0.1484,  0.1621,  0.1406,  0.0986,  0.1611,  0.1562,\n",
       "                       0.1416,  0.1240,  0.1406,  0.1387,  0.1660,  0.1299,  0.1787,  0.1445,\n",
       "                       0.1182,  0.1602,  0.1670,  0.1484,  0.1777,  0.1523,  0.1709,  0.1572,\n",
       "                       0.1475,  0.1758,  0.0947,  0.1797,  0.1279,  0.1079,  0.1484,  0.1475,\n",
       "                       0.1768,  0.1562,  0.1533,  0.1699,  0.1357,  0.2109,  0.1816,  0.1582,\n",
       "                       0.1562,  0.1807,  0.1338,  0.1748,  0.1543,  0.1621,  0.1592,  0.1504,\n",
       "                       0.1631,  0.1973,  0.1299,  0.1396,  0.1562, -0.1318,  0.1553,  0.1367,\n",
       "                       0.1387,  0.1465,  0.1016,  0.0762,  0.1309,  0.1270,  0.1396,  0.1748,\n",
       "                       0.1699,  0.2109,  0.2090,  0.1729,  0.1621,  0.1348,  0.1631,  0.1426,\n",
       "                       0.1660,  0.1089,  0.1543,  0.0220,  0.1445,  0.1455,  0.1553,  0.1357,\n",
       "                       0.1895,  0.1973,  0.1289,  0.1797,  0.1553,  0.1650,  0.1348,  0.1406,\n",
       "                       0.1138,  0.1475,  0.1523,  0.0493,  0.1572,  0.1885,  0.1670,  0.0986,\n",
       "                       0.1289,  0.1621,  0.1562,  0.1406,  0.1631,  0.2002,  0.1416,  0.1426,\n",
       "                       0.1387,  0.1592,  0.1719,  0.1299,  0.1738,  0.1719,  0.1396,  0.1738,\n",
       "                       0.1592,  0.1592,  0.1797,  0.1514,  0.1079,  0.1592,  0.1348,  0.1670,\n",
       "                       0.1260,  0.1367,  0.1934,  0.1914,  0.1592,  0.1914,  0.1289,  0.1553,\n",
       "                       0.1025,  0.1318,  0.1396,  0.0869,  0.0903,  0.1777,  0.1641,  0.1553,\n",
       "                       0.1904,  0.1816,  0.1494,  0.1680,  0.1270,  0.1475,  0.1494,  0.1592,\n",
       "                       0.1299,  0.1514,  0.1196,  0.1250,  0.1357, -0.1299,  0.1216,  0.1250,\n",
       "                       0.1396,  0.1279,  0.0815,  0.1514,  0.1807,  0.1270,  0.1484,  0.1504,\n",
       "                       0.1348,  0.1406,  0.1621,  0.1543,  0.1523,  0.1484,  0.0410,  0.2139,\n",
       "                       0.1621,  0.0320,  0.2080,  0.1836,  0.1572,  0.1494,  0.1455,  0.1484,\n",
       "                       0.2129,  0.1924,  0.1484,  0.1543,  0.1670,  0.1426,  0.1191,  0.1621,\n",
       "                       0.1367,  0.1348,  0.1201,  0.1631,  0.1377,  0.1357,  0.1514,  0.1162,\n",
       "                       0.1875,  0.1572,  0.1206,  0.1797,  0.1914,  0.1406,  0.2051,  0.1631,\n",
       "                       0.1631,  0.1299,  0.1582,  0.1514,  0.1543,  0.1562,  0.1582,  0.1875,\n",
       "                       0.1357,  0.1152,  0.1768,  0.1543,  0.1094,  0.2207,  0.1187,  0.1455,\n",
       "                       0.1230,  0.1602,  0.1455,  0.1367,  0.1777,  0.1289,  0.1846,  0.2070,\n",
       "                       0.1670,  0.1602,  0.1387,  0.1455,  0.1523,  0.1465,  0.1455,  0.1553,\n",
       "                       0.1367,  0.1270,  0.1611,  0.2012,  0.1807,  0.1826,  0.1533,  0.1406,\n",
       "                       0.1680,  0.1699,  0.1416,  0.1582,  0.1475,  0.1216,  0.1807,  0.1387,\n",
       "                       0.1377,  0.1514,  0.1514,  0.1738,  0.1650,  0.1846,  0.1436,  0.1641,\n",
       "                       0.1543,  0.1816,  0.1631,  0.1025,  0.1436,  0.1357,  0.1060,  0.1475,\n",
       "                       0.1328,  0.1572,  0.1455,  0.1895,  0.0410,  0.1279,  0.1885,  0.1328,\n",
       "                       0.1553,  0.1494,  0.2002,  0.1289,  0.1494,  0.1592,  0.1924,  0.1592,\n",
       "                       0.1514,  0.0464,  0.1504,  0.1475,  0.1187,  0.1729,  0.1406,  0.1123,\n",
       "                       0.1611,  0.1973,  0.1855,  0.1338,  0.1680,  0.1484,  0.1445,  0.1250,\n",
       "                       0.1377,  0.0160,  0.1245,  0.1592,  0.1367,  0.1357,  0.1738,  0.1436,\n",
       "                       0.1533,  0.1338,  0.1523,  0.1230,  0.1484,  0.1475,  0.1348,  0.1523,\n",
       "                       0.1177,  0.1416,  0.0459,  0.1572,  0.1924,  0.1699,  0.1348,  0.0498,\n",
       "                       0.1719,  0.1553,  0.1621,  0.1387,  0.1592,  0.1748,  0.1836,  0.1523,\n",
       "                       0.1387,  0.1348,  0.1680,  0.1445,  0.1455,  0.1455,  0.1807,  0.1963,\n",
       "                       0.1729,  0.1250,  0.1855,  0.1982,  0.1396,  0.1719,  0.1387,  0.1689,\n",
       "                       0.1973,  0.1621,  0.1533,  0.1387,  0.1396,  0.1523,  0.1631,  0.1826,\n",
       "                       0.1787,  0.1670,  0.1660,  0.1250,  0.1348,  0.1387,  0.1406,  0.1768,\n",
       "                       0.1719,  0.1680,  0.1123,  0.1465,  0.1230,  0.1406,  0.1309,  0.1445,\n",
       "                       0.1196,  0.1270,  0.1670,  0.1699,  0.1504,  0.1416,  0.0972,  0.1465,\n",
       "                       0.1895,  0.1099,  0.1543,  0.1455,  0.2295,  0.1426,  0.1250,  0.1582,\n",
       "                       0.1387,  0.1777,  0.1514,  0.1738,  0.0452,  0.1543,  0.1533,  0.1797,\n",
       "                       0.1592,  0.1523,  0.1377,  0.1328,  0.1260,  0.1836,  0.1475,  0.0405,\n",
       "                       0.1416,  0.1807,  0.1543,  0.1729,  0.1250,  0.1748,  0.1650,  0.1494,\n",
       "                       0.0820,  0.1504,  0.1768,  0.1426,  0.1348,  0.1338,  0.1484,  0.1836,\n",
       "                       0.1157,  0.1279,  0.1270,  0.1289,  0.1641,  0.1338,  0.1797,  0.1865,\n",
       "                       0.1367,  0.1797,  0.1807,  0.1533,  0.1514,  0.1572,  0.1611,  0.1455,\n",
       "                       0.1621,  0.1582,  0.1943,  0.1523,  0.1699,  0.1514,  0.1748,  0.1904,\n",
       "                       0.1387,  0.1543,  0.1650,  0.1484,  0.1357,  0.0718,  0.1865,  0.2158,\n",
       "                       0.1196,  0.2188,  0.1377,  0.1514,  0.1631,  0.1504,  0.1572,  0.1543,\n",
       "                       0.1680,  0.1104,  0.1787,  0.1846,  0.1582,  0.1758,  0.0217,  0.1572,\n",
       "                       0.1689,  0.1445,  0.1816,  0.0703,  0.1162,  0.1455,  0.1504,  0.1670,\n",
       "                       0.1602,  0.1816,  0.1396,  0.1826,  0.1797,  0.1416,  0.1523,  0.1235,\n",
       "                       0.1201,  0.1338,  0.1177,  0.1299,  0.1406,  0.1377,  0.1523,  0.1709,\n",
       "                       0.1650,  0.1670,  0.1602,  0.1582,  0.1436,  0.1572,  0.1670,  0.1543,\n",
       "                       0.2070,  0.1475,  0.1670,  0.0874,  0.1836,  0.1260,  0.1475,  0.1641,\n",
       "                       0.1797,  0.1484,  0.1631,  0.1406,  0.1650,  0.1553,  0.1543,  0.1904,\n",
       "                       0.1387,  0.1953,  0.1602,  0.1523,  0.1904,  0.1270,  0.1436,  0.1787,\n",
       "                       0.1738,  0.1387,  0.1650,  0.1465,  0.1592,  0.1250,  0.1631,  0.1553,\n",
       "                       0.1748,  0.2217,  0.1338,  0.1250,  0.1807,  0.1699,  0.2109,  0.1621,\n",
       "                       0.1533,  0.1660,  0.1377,  0.1514,  0.1885,  0.1748,  0.1641,  0.1289,\n",
       "                       0.1309,  0.1387,  0.1348,  0.1641,  0.1582,  0.1299,  0.1562,  0.1885,\n",
       "                       0.1494,  0.1553,  0.1738,  0.1787,  0.1543,  0.0292,  0.1475,  0.2178,\n",
       "                       0.1475,  0.1611,  0.1289,  0.1309,  0.1846,  0.1338,  0.1572,  0.1309,\n",
       "                       0.1484,  0.1475,  0.1699,  0.1533,  0.1484,  0.1289,  0.1465,  0.1680,\n",
       "                       0.1650,  0.1357,  0.1562,  0.1299,  0.1455,  0.0869,  0.1387,  0.1836,\n",
       "                       0.1367,  0.1807,  0.1182,  0.1729,  0.1699,  0.1377,  0.1367,  0.1348,\n",
       "                       0.1904,  0.1250,  0.1680,  0.1611,  0.1592,  0.1089,  0.1572,  0.1328,\n",
       "                       0.1553,  0.1299,  0.1406,  0.1670,  0.1982,  0.1406,  0.0718,  0.1562,\n",
       "                       0.1562,  0.1455,  0.0430,  0.1562,  0.1562,  0.1875,  0.1553,  0.1953,\n",
       "                       0.1533,  0.1182,  0.1777,  0.1416,  0.1758,  0.1602,  0.1729,  0.1279,\n",
       "                       0.1533,  0.1641,  0.1641,  0.1523,  0.1318,  0.1074,  0.1377,  0.1523,\n",
       "                       0.1641,  0.1562,  0.1562,  0.1777,  0.1660,  0.1729,  0.1914,  0.1816,\n",
       "                       0.1631,  0.1807,  0.1738,  0.1289,  0.1826,  0.1631,  0.1504,  0.1592,\n",
       "                       0.2031,  0.1631,  0.1729,  0.1543,  0.1699,  0.1416,  0.1807,  0.1836,\n",
       "                       0.1553,  0.1206,  0.1748,  0.1226,  0.1572,  0.1719,  0.1436,  0.2021])),\n",
       "             ('0.auto_model.encoder.block.5.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.6406,  1.0547,  0.0698,  ..., -0.6172, -0.2793,  0.3906],\n",
       "                      [-0.9102,  1.0469,  0.6602,  ...,  0.1260, -0.7031,  0.6484],\n",
       "                      [ 0.1357, -0.2207, -0.6016,  ...,  1.0547, -0.0187,  0.9609],\n",
       "                      ...,\n",
       "                      [ 0.1245,  0.3477, -0.0771,  ..., -0.1270,  0.0388, -0.5430],\n",
       "                      [ 0.2383,  0.1484, -0.3262,  ...,  0.1631, -0.2637,  0.2969],\n",
       "                      [-0.2324, -0.2676,  0.1436,  ...,  0.1396,  0.0177,  0.2832]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.0410, -0.1807, -0.1865,  ...,  0.1465, -0.5195, -0.3867],\n",
       "                      [ 0.0625, -0.2559, -0.4824,  ..., -0.0610,  0.1055, -0.0029],\n",
       "                      [ 0.1040, -0.5156,  0.2637,  ...,  0.2354,  0.0449,  0.2051],\n",
       "                      ...,\n",
       "                      [-0.6016, -0.1055,  0.0476,  ..., -0.0201,  0.4043, -0.2207],\n",
       "                      [-0.1582, -0.2910,  0.7578,  ..., -0.1162, -0.2578, -0.1299],\n",
       "                      [ 0.1992,  0.2275,  0.3203,  ...,  0.0124, -0.0266,  0.2041]])),\n",
       "             ('0.auto_model.encoder.block.5.layer.1.layer_norm.weight',\n",
       "              tensor([1.0781, 1.1641, 1.0469, 1.1172, 0.9844, 1.0078, 0.4258, 1.2656, 2.2500,\n",
       "                      1.0234, 1.1172, 1.2344, 1.0234, 1.1797, 0.8633, 0.9570, 1.1484, 1.1250,\n",
       "                      1.1641, 0.9141, 1.0469, 1.0781, 1.4375, 0.9805, 0.9805, 1.0859, 0.8906,\n",
       "                      1.0156, 1.1484, 0.8945, 0.9453, 0.9258, 1.0781, 1.0078, 1.1484, 1.0078,\n",
       "                      1.3828, 1.3047, 0.8086, 1.4531, 0.9961, 1.4453, 1.1406, 1.0234, 2.8906,\n",
       "                      1.0000, 1.1172, 1.0469, 1.0312, 0.9062, 1.2422, 1.1016, 1.6719, 0.8906,\n",
       "                      0.9805, 1.0625, 1.1641, 1.0625, 0.9648, 0.9141, 1.0234, 0.8867, 1.0469,\n",
       "                      0.9258, 0.9922, 1.5156, 0.9297, 0.9102, 1.3125, 0.8828, 0.9570, 1.3984,\n",
       "                      0.9453, 1.0781, 0.9375, 0.9102, 1.2656, 1.1094, 0.9531, 1.0078, 0.9492,\n",
       "                      0.9844, 0.9961, 1.2109, 0.9492, 1.0781, 1.2188, 1.0547, 1.0078, 0.9258,\n",
       "                      1.0469, 0.8281, 0.8867, 1.1562, 0.9141, 0.9023, 0.9648, 1.0703, 0.9727,\n",
       "                      0.9023, 1.0859, 1.0625, 1.0703, 0.8867, 1.0312, 1.4766, 0.9570, 0.9766,\n",
       "                      0.9766, 1.6641, 1.0859, 0.9805, 0.9648, 0.9648, 1.0469, 0.9531, 0.9414,\n",
       "                      0.8984, 1.5000, 0.9180, 0.8359, 0.9492, 1.3750, 0.9531, 1.1328, 1.0547,\n",
       "                      0.9922, 0.9688, 1.0156, 1.1484, 1.4375, 1.0156, 0.9375, 0.9609, 1.0234,\n",
       "                      1.2422, 1.2500, 0.9492, 1.0938, 1.0938, 0.9531, 1.2031, 1.3047, 1.2031,\n",
       "                      1.0078, 1.2266, 1.0078, 1.1328, 0.9492, 1.0781, 0.8906, 1.0938, 0.9727,\n",
       "                      1.2812, 0.9297, 0.9297, 1.0547, 0.9375, 1.1562, 1.0781, 0.9180, 1.1328,\n",
       "                      0.8750, 2.8594, 0.9648, 0.7969, 0.9297, 1.1719, 0.9883, 1.2422, 1.1953,\n",
       "                      0.9922, 0.9492, 1.3047, 1.0859, 1.0938, 1.0391, 0.6875, 1.2422, 0.3438,\n",
       "                      0.8828, 0.9453, 1.0156, 0.8594, 0.9844, 1.2734, 0.9805, 1.1797, 1.1562,\n",
       "                      1.1328, 0.9375, 0.9492, 0.9062, 0.9961, 0.9258, 0.5781, 0.9766, 1.0938,\n",
       "                      1.0625, 0.8828, 0.8594, 1.0703, 0.9570, 1.0703, 0.9102, 1.1172, 1.0000,\n",
       "                      0.9492, 0.9180, 0.9609, 1.2344, 0.9961, 1.0781, 1.1797, 1.0234, 1.1719,\n",
       "                      1.0547, 1.0156, 1.1797, 2.4531, 0.9570, 1.0312, 0.9805, 1.0156, 1.1250,\n",
       "                      0.9414, 1.2812, 1.3125, 1.0781, 1.1016, 0.9180, 1.0703, 0.6758, 0.8906,\n",
       "                      0.8828, 0.6953, 0.5195, 1.1016, 1.0938, 1.1875, 1.3438, 1.0938, 1.0156,\n",
       "                      0.9688, 0.9141, 0.8945, 1.0625, 1.0703, 0.8594, 1.0859, 3.0312, 0.8750,\n",
       "                      0.8867, 0.9883, 0.9141, 0.9492, 0.9570, 0.9375, 0.9688, 0.9961, 1.1797,\n",
       "                      0.4395, 1.0312, 0.9531, 1.0078, 1.0156, 0.9453, 0.9961, 1.0469, 0.8906,\n",
       "                      0.7461, 1.2812, 1.4219, 1.5156, 1.1797, 1.0938, 0.9219, 1.1250, 1.0000,\n",
       "                      0.9688, 1.3828, 1.3672, 0.9258, 1.2344, 1.0703, 1.1719, 0.8750, 1.0078,\n",
       "                      0.9766, 1.8438, 0.9258, 1.0000, 0.9492, 0.8633, 1.0625, 0.9141, 1.2031,\n",
       "                      1.0156, 0.8984, 1.1641, 1.1484, 0.9492, 1.2578, 1.0781, 1.0000, 0.9336,\n",
       "                      0.9609, 0.9727, 1.0625, 1.1250, 1.0000, 1.2891, 1.0078, 0.8555, 1.0859,\n",
       "                      1.0859, 0.8711, 1.1797, 1.2578, 0.9141, 0.8906, 1.2656, 1.0703, 0.8438,\n",
       "                      1.2734, 0.9297, 1.2656, 1.2031, 1.0234, 1.0781, 1.0078, 1.2734, 1.1797,\n",
       "                      0.9531, 0.9570, 1.0469, 0.8672, 0.8984, 1.0078, 1.2812, 1.1094, 1.1406,\n",
       "                      1.1328, 1.0391, 0.9961, 1.0469, 1.2422, 1.0156, 1.1094, 1.7109, 1.1484,\n",
       "                      0.8750, 1.0938, 0.9609, 1.1172, 1.0156, 1.0859, 1.2031, 0.9453, 1.2344,\n",
       "                      1.0781, 1.2891, 1.2188, 0.8242, 0.9102, 0.9766, 0.9727, 1.2500, 0.9414,\n",
       "                      1.1406, 1.1250, 1.0781, 0.2012, 1.0312, 1.2422, 0.8242, 1.2109, 0.9805,\n",
       "                      1.3125, 1.2500, 1.0312, 1.0469, 1.2578, 1.0547, 0.9375, 1.2656, 0.9531,\n",
       "                      1.1016, 0.8242, 1.1484, 0.9727, 2.0938, 1.0156, 1.1484, 1.1172, 1.1016,\n",
       "                      1.2031, 0.9922, 1.0078, 0.8906, 0.9219, 0.5312, 0.9141, 0.9258, 0.9297,\n",
       "                      1.0547, 1.1250, 0.9219, 0.9375, 0.9062, 0.9414, 0.8672, 1.1250, 1.0156,\n",
       "                      0.8984, 1.0234, 0.9648, 0.9414, 0.3340, 1.3984, 1.5000, 1.1016, 0.9297,\n",
       "                      0.4316, 1.0547, 0.9414, 1.0781, 0.9180, 1.1562, 1.0312, 1.1797, 1.0000,\n",
       "                      1.3438, 0.8711, 1.2500, 0.8789, 1.0859, 0.9648, 1.1484, 1.2266, 1.0781,\n",
       "                      1.0547, 1.1016, 1.1641, 0.8867, 0.9570, 0.9922, 1.1172, 1.1172, 1.0703,\n",
       "                      1.0156, 0.9258, 0.9492, 0.9219, 1.1953, 1.2344, 1.0156, 1.0234, 1.1172,\n",
       "                      0.8945, 0.9766, 0.8828, 0.9453, 1.0625, 0.9961, 1.0156, 0.8203, 1.0078,\n",
       "                      0.8867, 0.9570, 0.9336, 0.8945, 0.8672, 0.8906, 1.0391, 0.9375, 1.0156,\n",
       "                      1.0078, 1.2656, 1.1172, 1.2188, 1.9219, 1.0547, 1.0078, 1.4219, 0.9883,\n",
       "                      0.9492, 1.1797, 0.8398, 1.0703, 0.9648, 1.1562, 1.1328, 0.9336, 1.0938,\n",
       "                      1.2188, 1.1250, 0.9688, 0.8867, 0.9453, 0.9961, 1.1016, 1.1328, 1.6953,\n",
       "                      0.9336, 1.1094, 1.0625, 1.1016, 0.9766, 1.0703, 1.1641, 0.9219, 2.2656,\n",
       "                      0.9570, 1.0703, 0.9414, 0.9648, 0.9688, 1.2500, 1.0625, 0.8633, 1.0938,\n",
       "                      0.8516, 0.9258, 0.9727, 1.9609, 1.1016, 1.3203, 0.8945, 1.0547, 1.1875,\n",
       "                      0.9961, 1.0156, 1.0547, 1.1094, 1.0547, 0.9961, 1.0000, 1.2891, 1.0078,\n",
       "                      1.0391, 0.9688, 1.1172, 1.1562, 1.0391, 0.9531, 1.1328, 1.0469, 1.0156,\n",
       "                      0.5547, 1.3281, 1.2188, 0.9141, 1.2422, 0.9023, 1.0469, 1.1250, 0.9531,\n",
       "                      1.0781, 1.1094, 1.1875, 0.8398, 1.1797, 1.3906, 0.9766, 1.1406, 0.0085,\n",
       "                      1.0703, 1.1719, 0.9492, 1.0938, 0.2773, 0.8398, 0.9805, 0.9961, 1.0469,\n",
       "                      0.9883, 1.1250, 0.9062, 1.2891, 1.0000, 0.9688, 0.9883, 1.0000, 0.8828,\n",
       "                      0.8086, 0.8516, 0.9375, 0.9492, 0.9258, 0.9531, 1.0312, 0.9141, 1.0625,\n",
       "                      1.0000, 1.0234, 1.0156, 1.1406, 1.2266, 0.9258, 1.2266, 0.9453, 1.0391,\n",
       "                      0.7109, 1.0234, 0.8945, 0.8867, 1.0234, 1.2578, 1.0391, 1.0078, 0.9258,\n",
       "                      1.1562, 0.9375, 1.0000, 1.2969, 1.0625, 1.1016, 0.9688, 0.9727, 1.1641,\n",
       "                      1.0078, 1.0469, 1.1797, 1.0625, 0.9961, 1.0000, 1.2422, 0.9844, 1.1562,\n",
       "                      1.1719, 1.0078, 1.1797, 1.3125, 0.9883, 0.8164, 1.0625, 1.0625, 1.1953,\n",
       "                      0.9102, 1.1562, 1.0391, 0.9414, 1.0703, 1.1094, 1.0703, 1.0547, 0.8711,\n",
       "                      1.0391, 0.9883, 0.9805, 0.9375, 0.9961, 1.0234, 0.9961, 1.1484, 0.8828,\n",
       "                      1.1016, 0.9766, 1.1016, 0.9141, 0.1338, 0.9336, 1.2422, 1.0234, 1.1641,\n",
       "                      1.0078, 0.9414, 1.0547, 0.9609, 1.1406, 1.5469, 0.9297, 1.0000, 1.0703,\n",
       "                      1.0234, 0.9453, 0.9688, 0.9609, 0.9414, 0.9844, 0.9922, 1.0859, 0.8242,\n",
       "                      0.9844, 1.4375, 1.0156, 1.2109, 0.9531, 1.0859, 0.8789, 1.0469, 1.3594,\n",
       "                      0.9727, 0.9492, 1.0156, 1.1719, 0.9414, 1.0391, 1.1328, 1.1484, 0.8594,\n",
       "                      1.0781, 0.9453, 1.1484, 0.9297, 0.9922, 1.4297, 1.3125, 0.9023, 2.3594,\n",
       "                      1.0469, 0.8867, 0.9727, 0.4336, 1.0859, 1.2734, 1.2812, 0.9688, 1.2031,\n",
       "                      0.9062, 1.1250, 1.1875, 0.9844, 1.0625, 0.9961, 1.0469, 0.8398, 0.9258,\n",
       "                      1.1719, 1.0469, 0.9766, 0.9180, 0.7852, 1.0312, 1.0625, 1.0234, 0.9375,\n",
       "                      1.0000, 1.0703, 1.0625, 1.1797, 1.1953, 1.1953, 1.0312, 1.1719, 0.9883,\n",
       "                      0.9336, 1.1328, 1.2812, 1.1016, 1.0703, 1.3984, 1.0625, 1.0859, 0.9922,\n",
       "                      1.0781, 0.8750, 1.1484, 1.1328, 1.0156, 0.7656, 1.0703, 0.9375, 0.9609,\n",
       "                      1.1562, 0.9766, 1.1172])),\n",
       "             ('0.auto_model.encoder.block.6.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0776,  0.0228, -0.0048,  ...,  0.0198, -0.0535,  0.0315],\n",
       "                      [ 0.0591, -0.0024,  0.0330,  ...,  0.0498,  0.0625, -0.0170],\n",
       "                      [-0.0400,  0.0107,  0.0095,  ...,  0.0052, -0.0496, -0.0175],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0154, -0.0479,  ..., -0.0197,  0.0522, -0.0311],\n",
       "                      [ 0.0071,  0.0210, -0.0302,  ..., -0.0009,  0.0574,  0.0034],\n",
       "                      [-0.0189, -0.0408,  0.0160,  ...,  0.0713, -0.0361, -0.0356]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.2148,  0.0106,  0.4707,  ..., -0.2217, -0.1094,  0.0096],\n",
       "                      [ 0.3203, -0.0889,  0.3867,  ..., -0.0147, -0.2773,  0.1182],\n",
       "                      [ 0.4824,  0.3750, -0.0217,  ..., -0.2988,  0.3398,  0.2246],\n",
       "                      ...,\n",
       "                      [-0.3516, -0.0713, -0.1250,  ..., -0.2676,  0.2656, -0.3340],\n",
       "                      [ 0.2070,  0.3711, -0.4688,  ...,  0.2637,  0.3105, -0.2031],\n",
       "                      [ 0.1099, -0.0483, -0.0447,  ...,  0.3965, -0.2266, -0.0403]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.3223, -0.8164, -0.0486,  ..., -0.1582,  0.0212,  0.8359],\n",
       "                      [ 0.4492, -0.1108,  0.2676,  ..., -0.9297,  0.6797,  0.0048],\n",
       "                      [ 1.1016,  0.0154,  0.8906,  ..., -0.3184,  0.6055,  0.0840],\n",
       "                      ...,\n",
       "                      [-0.5117,  0.0349,  0.2930,  ...,  0.0737, -0.2275,  0.9492],\n",
       "                      [-0.4707, -0.3730, -0.2930,  ..., -0.6328,  0.0234, -0.2256],\n",
       "                      [-0.7266,  0.6641, -0.6719,  ..., -0.0154,  0.0201, -0.3320]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.2520, -0.4727, -1.4375,  ...,  0.3828,  0.1689, -1.1016],\n",
       "                      [ 0.7305,  0.1108, -0.5469,  ...,  1.9609, -0.5703,  0.4609],\n",
       "                      [ 0.1475,  0.1523, -0.8086,  ..., -0.7305,  0.3770,  0.5859],\n",
       "                      ...,\n",
       "                      [ 0.0294,  0.7656,  0.2500,  ...,  0.3789, -0.5352,  0.6250],\n",
       "                      [-0.1787, -0.3340, -0.8633,  ..., -0.1943,  0.1709, -0.9766],\n",
       "                      [-1.1406, -0.0908,  0.3867,  ..., -0.8477, -0.1875, -0.0923]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1719,  0.1592,  0.1641,  0.1846,  0.1582,  0.1592,  0.0403,  0.2002,\n",
       "                      -0.0840,  0.1670,  0.1914,  0.1709,  0.1138,  0.1602,  0.1328,  0.1729,\n",
       "                       0.1553,  0.1680,  0.1670,  0.1514,  0.1206,  0.1592, -0.0288,  0.1533,\n",
       "                       0.1504,  0.1699,  0.1514,  0.1504,  0.1523,  0.1455,  0.1436,  0.1406,\n",
       "                       0.1650,  0.1465,  0.1660,  0.1709,  0.1328,  0.2090,  0.1318,  0.1787,\n",
       "                       0.1572,  0.1406,  0.1650,  0.1660, -0.0728,  0.1543,  0.1152,  0.1416,\n",
       "                       0.1826,  0.0398,  0.1836,  0.1650,  0.1270,  0.1494,  0.0295,  0.1436,\n",
       "                       0.0344, -0.1689,  0.1748,  0.1475,  0.1855,  0.1436,  0.1602,  0.1475,\n",
       "                       0.1562,  0.0483,  0.1406,  0.1406,  0.1689,  0.1621,  0.1562,  0.1680,\n",
       "                       0.1406,  0.1709,  0.1494,  0.1650,  0.1406,  0.1738,  0.1523,  0.1885,\n",
       "                       0.1475,  0.1611,  0.1455,  0.1904,  0.1504,  0.1709,  0.1914,  0.1631,\n",
       "                       0.1699,  0.1504,  0.1572,  0.1328,  0.1533,  0.1426,  0.1416,  0.1436,\n",
       "                       0.1572,  0.1582,  0.1494,  0.1221,  0.1758,  0.1670,  0.1201,  0.1348,\n",
       "                       0.1572,  0.1426,  0.1611,  0.1572,  0.1406,  0.0796,  0.1709,  0.1602,\n",
       "                       0.1582,  0.1328,  0.1592,  0.1602,  0.1582,  0.1289,  0.1562,  0.1494,\n",
       "                       0.1436,  0.1562,  0.1602,  0.1572,  0.1543,  0.1582,  0.1748,  0.1572,\n",
       "                       0.1484,  0.1709,  0.0859,  0.1885,  0.1436,  0.1201,  0.1641, -0.1436,\n",
       "                       0.1777,  0.1611,  0.1748,  0.1768,  0.1514,  0.2031,  0.1768,  0.1660,\n",
       "                       0.1572,  0.1807,  0.1562,  0.1807,  0.1621,  0.1670,  0.1582,  0.1611,\n",
       "                       0.1523,  0.1924,  0.1553,  0.1582,  0.1572,  0.1436,  0.1709,  0.1367,\n",
       "                       0.1455,  0.1523,  0.1172,  0.0664,  0.1465,  0.1348,  0.1582,  0.1738,\n",
       "                       0.1758,  0.1865,  0.1943,  0.1650,  0.1797,  0.1250,  0.1611,  0.1387,\n",
       "                       0.1836,  0.0825,  0.1582,  0.0216,  0.1582,  0.1523,  0.1533,  0.1455,\n",
       "                       0.1748,  0.1836,  0.1387,  0.1758,  0.1729,  0.1611,  0.1455,  0.1562,\n",
       "                       0.1167,  0.1748,  0.1299,  0.0505,  0.1562,  0.1807,  0.1602, -0.1240,\n",
       "                       0.1357,  0.1709,  0.1514,  0.1455,  0.1582,  0.1904,  0.1426,  0.1562,\n",
       "                       0.1406,  0.1504,  0.1572,  0.1523,  0.1650,  0.1768,  0.1562,  0.1758,\n",
       "                       0.1562,  0.1592,  0.1719,  0.1230,  0.1011,  0.1797,  0.1387,  0.1699,\n",
       "                       0.1338,  0.1611,  0.1855,  0.1670,  0.1719,  0.1816,  0.1289,  0.1475,\n",
       "                       0.1099,  0.1338,  0.1426,  0.1006,  0.1060,  0.1934,  0.1650,  0.1523,\n",
       "                       0.1719,  0.1777,  0.1582,  0.1631,  0.1279,  0.1543,  0.1611,  0.1689,\n",
       "                       0.1289,  0.1621,  0.1035,  0.1445,  0.1445,  0.1436,  0.1338,  0.1523,\n",
       "                       0.1631,  0.1118,  0.0635,  0.1514,  0.1826,  0.1084,  0.1602,  0.1650,\n",
       "                       0.1465,  0.1553,  0.1631,  0.1514,  0.1611,  0.1523,  0.0383,  0.1982,\n",
       "                       0.1455,  0.0284,  0.1982,  0.1729,  0.1709,  0.1455,  0.1504,  0.1504,\n",
       "                       0.1885,  0.1885,  0.1611,  0.1504,  0.1621,  0.1348,  0.1465,  0.1729,\n",
       "                       0.1396,  0.1289,  0.1396,  0.1670,  0.1562,  0.1533,  0.1670,  0.1367,\n",
       "                       0.1719,  0.1719,  0.1465,  0.1729,  0.1865,  0.1445,  0.2041,  0.1729,\n",
       "                       0.1621,  0.1387,  0.1777,  0.1631,  0.1641,  0.1543,  0.1689,  0.1797,\n",
       "                       0.1475,  0.1104,  0.1670,  0.1670,  0.1270,  0.1943,  0.1211,  0.1660,\n",
       "                       0.1387,  0.1748,  0.1562,  0.1562,  0.1758,  0.1387,  0.1660,  0.2051,\n",
       "                       0.1699,  0.1592,  0.1543,  0.1328,  0.1445,  0.1572,  0.1553,  0.1465,\n",
       "                       0.1367,  0.1533,  0.1562,  0.1836,  0.1904,  0.1738,  0.1602,  0.1465,\n",
       "                       0.1699,  0.1719,  0.1367,  0.1592,  0.1602,  0.1074,  0.1719,  0.1299,\n",
       "                       0.1475,  0.1768,  0.1523,  0.1777,  0.1602,  0.1846,  0.1631,  0.1660,\n",
       "                       0.1523,  0.1562,  0.1670,  0.1162,  0.1436,  0.1484,  0.0811,  0.1357,\n",
       "                       0.1514,  0.1680,  0.1436,  0.1816,  0.0449,  0.1279,  0.1865,  0.1396,\n",
       "                       0.1387,  0.1572,  0.1904,  0.1167,  0.1719,  0.1660,  0.1836,  0.1592,\n",
       "                       0.1611,  0.0393,  0.1455,  0.1377,  0.1250,  0.1777,  0.1582,  0.0884,\n",
       "                       0.1572,  0.2012,  0.1826,  0.1367,  0.1748,  0.1709,  0.1562,  0.1465,\n",
       "                       0.1602,  0.0105,  0.1299,  0.1621,  0.1465,  0.1504,  0.1719,  0.1455,\n",
       "                       0.1592,  0.1406,  0.1475,  0.1416,  0.1406,  0.1426,  0.1592,  0.1543,\n",
       "                      -0.1201,  0.1465,  0.0542,  0.1611,  0.1699,  0.1514,  0.1406,  0.0530,\n",
       "                       0.1689,  0.1514,  0.1699,  0.1572,  0.1631,  0.1719,  0.1729,  0.1504,\n",
       "                       0.1235,  0.1270,  0.1484,  0.1445,  0.1543,  0.1523,  0.1660,  0.1875,\n",
       "                       0.1680,  0.1172,  0.1758,  0.1836,  0.1377,  0.1738,  0.1455,  0.1670,\n",
       "                       0.1914,  0.1592,  0.1387,  0.1475,  0.1631,  0.1729,  0.1787,  0.1768,\n",
       "                       0.1709,  0.1621,  0.1592,  0.1455,  0.1318,  0.1377,  0.1533,  0.1621,\n",
       "                       0.1807,  0.1758,  0.1201,  0.1582,  0.1338,  0.1592,  0.1377,  0.1543,\n",
       "                      -0.1270,  0.1279,  0.1582,  0.1602,  0.1670,  0.1670,  0.0708,  0.1885,\n",
       "                       0.1846,  0.0986,  0.1484,  0.1514,  0.2178,  0.1523,  0.1475,  0.1641,\n",
       "                       0.1377,  0.1699,  0.1572,  0.1709,  0.0481,  0.1484,  0.1562,  0.1699,\n",
       "                       0.1494,  0.1465,  0.1396,  0.1279,  0.1406,  0.1689,  0.1494,  0.0398,\n",
       "                       0.1514,  0.1826,  0.1689,  0.1777,  0.1357,  0.1699,  0.1855,  0.1592,\n",
       "                       0.0737,  0.1699,  0.1777,  0.1436,  0.1367,  0.1289,  0.1494,  0.1689,\n",
       "                       0.1367,  0.1377,  0.1211,  0.1494,  0.1650,  0.1235,  0.1816,  0.1709,\n",
       "                       0.1455,  0.1875,  0.1807,  0.1660,  0.1650,  0.1689,  0.1680,  0.1689,\n",
       "                       0.1611,  0.1484,  0.1816,  0.1621,  0.1738,  0.1621,  0.1680,  0.1904,\n",
       "                       0.1494,  0.1553,  0.1621,  0.1680,  0.1523,  0.0618,  0.1768,  0.1816,\n",
       "                       0.1279,  0.2119,  0.1357,  0.1729,  0.1543,  0.1357,  0.1631,  0.1445,\n",
       "                       0.1621,  0.1240,  0.1602,  0.1875,  0.1758,  0.1787,  0.0203,  0.1592,\n",
       "                       0.1689,  0.1367,  0.1904,  0.0688,  0.1187,  0.1602,  0.1543,  0.1699,\n",
       "                       0.1523,  0.2061,  0.1543,  0.1582,  0.1611,  0.1553,  0.1582,  0.1553,\n",
       "                       0.1191,  0.1309,  0.1299,  0.1367,  0.1406,  0.1484,  0.1631,  0.1768,\n",
       "                       0.1631,  0.1680,  0.1729,  0.1611,  0.1475,  0.1465,  0.1592,  0.1572,\n",
       "                       0.1992,  0.1670,  0.1816,  0.0889,  0.1846,  0.1406,  0.1357,  0.1680,\n",
       "                       0.1680,  0.1514,  0.1748,  0.1279,  0.1709,  0.1611,  0.1582,  0.1807,\n",
       "                       0.1426,  0.1816,  0.1631,  0.1602,  0.1787,  0.1157,  0.1572,  0.1885,\n",
       "                       0.1719,  0.1582,  0.1592,  0.1533,  0.1465,  0.1060,  0.1768,  0.1494,\n",
       "                       0.1709,  0.2061,  0.1533,  0.1152,  0.1572,  0.1592,  0.1875,  0.1602,\n",
       "                       0.1553,  0.1816,  0.1367,  0.1660,  0.1895,  0.1621,  0.1660,  0.1465,\n",
       "                       0.1377,  0.1533,  0.1523,  0.1631,  0.1758,  0.1426,  0.1543,  0.1553,\n",
       "                       0.1465,  0.1504,  0.1875,  0.1562,  0.1299, -0.0178,  0.1504,  0.2188,\n",
       "                       0.1670,  0.1602,  0.1416,  0.1357,  0.1738,  0.1553,  0.1816,  0.1187,\n",
       "                       0.1475,  0.1377,  0.1758,  0.1641,  0.1562,  0.1611,  0.1357,  0.1699,\n",
       "                       0.1699,  0.1514,  0.1719,  0.1377,  0.1582,  0.0835,  0.1396,  0.1738,\n",
       "                       0.1494,  0.1855,  0.1299,  0.1738,  0.1631,  0.1357,  0.1611,  0.1582,\n",
       "                       0.1885,  0.1387,  0.1660,  0.1650,  0.1621,  0.1138,  0.1650,  0.1396,\n",
       "                       0.1631,  0.1357,  0.1621,  0.1582,  0.1875,  0.1299,  0.0669,  0.1611,\n",
       "                       0.1631,  0.1396,  0.0376,  0.1602,  0.1611,  0.1777,  0.1787,  0.1846,\n",
       "                       0.1387,  0.1152,  0.1699,  0.1396,  0.1689,  0.1602,  0.1719,  0.1406,\n",
       "                       0.1514,  0.1582,  0.1602,  0.1582,  0.1357,  0.1118,  0.1592,  0.1533,\n",
       "                       0.1719,  0.1465,  0.1562,  0.1631,  0.1611,  0.1787,  0.1826,  0.1836,\n",
       "                       0.1621,  0.1611,  0.1562,  0.1523,  0.1797,  0.1709,  0.1504,  0.1514,\n",
       "                       0.1904,  0.1426,  0.1689,  0.1689,  0.1689,  0.1553,  0.1719,  0.1865,\n",
       "                       0.1719,  0.1279,  0.1689,  0.1309,  0.1758,  0.1768,  0.1416,  0.1934])),\n",
       "             ('0.auto_model.encoder.block.6.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.3438,  0.0864,  1.1562,  ..., -0.9062,  0.5156,  0.1904],\n",
       "                      [-0.0491, -0.1196, -0.3145,  ..., -0.6680, -0.7539,  0.5625],\n",
       "                      [-0.8438,  0.0042, -0.2949,  ..., -1.1875,  1.5469, -0.2578],\n",
       "                      ...,\n",
       "                      [-0.0420,  0.1836,  1.7344,  ..., -0.5273, -0.3164, -0.5508],\n",
       "                      [ 0.0864, -0.3867,  1.0234,  ...,  0.0762, -0.6562, -0.1172],\n",
       "                      [ 0.1318, -0.2441, -0.6133,  ..., -0.7773, -1.1016,  0.8594]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.1230,  0.2246,  0.0825,  ..., -0.1846, -0.2178,  0.1846],\n",
       "                      [ 0.5312,  0.3477,  1.0156,  ...,  0.8594, -0.1348, -0.1016],\n",
       "                      [ 0.3906, -0.3730, -0.1953,  ..., -0.9453, -0.0070, -0.2148],\n",
       "                      ...,\n",
       "                      [-0.0581, -0.4316, -0.1011,  ...,  0.1650, -0.0123,  0.1084],\n",
       "                      [ 0.0344,  0.7812,  0.4023,  ...,  0.5234, -0.4824, -0.1885],\n",
       "                      [-0.1001, -0.3613, -0.0996,  ...,  0.7031,  0.6641, -0.4102]])),\n",
       "             ('0.auto_model.encoder.block.6.layer.1.layer_norm.weight',\n",
       "              tensor([ 1.2188,  1.1016,  1.1641,  1.1328,  0.9375,  1.1328,  0.3125,  1.1875,\n",
       "                       2.1406,  1.1641,  1.1953,  1.1484,  0.9883,  1.1797,  1.0625,  0.9844,\n",
       "                       1.1406,  1.0781,  1.1250,  1.1484,  1.1016,  1.0234,  1.5938,  1.0469,\n",
       "                       0.9688,  1.2188,  1.0312,  1.0859,  1.1484,  1.1094,  1.1172,  1.0391,\n",
       "                       1.0859,  1.0000,  1.1484,  1.0938,  1.2969,  1.2734,  0.9688,  1.5391,\n",
       "                       1.0859,  1.3828,  1.3516,  1.0938,  2.7969,  1.0469,  1.1875,  1.1641,\n",
       "                       1.1250,  0.7422,  1.1562,  1.1094,  1.5938,  1.0156,  1.0859,  1.0625,\n",
       "                       1.4062,  1.0625,  1.0625,  1.1094,  1.2188,  1.0000,  1.1250,  1.0000,\n",
       "                       1.0625,  1.5781,  1.1875,  1.0312,  1.3047,  1.0312,  1.1953,  1.4844,\n",
       "                       1.1016,  1.1328,  0.9844,  1.0469,  1.3906,  1.0781,  1.0469,  1.2188,\n",
       "                       1.0156,  1.1406,  1.1406,  1.2109,  1.0625,  1.1484,  1.3750,  1.0312,\n",
       "                       1.1250,  1.0078,  1.1797,  1.0234,  0.9805,  1.1875,  1.0391,  1.0234,\n",
       "                       1.1016,  1.0312,  1.0703,  1.0078,  1.1562,  1.1562,  1.1094,  0.9766,\n",
       "                       1.1328,  1.3984,  1.1016,  1.0469,  0.9805,  1.7656,  1.1953,  1.0781,\n",
       "                       1.0469,  1.0781,  1.0156,  1.1016,  1.0938,  0.9336,  1.3516,  1.0156,\n",
       "                       1.0234,  1.0859,  1.3438,  1.0469,  1.0859,  1.1797,  1.1016,  1.1250,\n",
       "                       0.9844,  1.1406,  1.4688,  1.1016,  0.9844,  0.9609,  1.0703,  1.1328,\n",
       "                       1.2891,  0.9531,  1.1719,  1.1484,  1.0469,  1.2188,  1.2812,  1.1953,\n",
       "                       1.0625,  1.2812,  1.1484,  1.0781,  1.1172,  1.0859,  1.1094,  1.1172,\n",
       "                       1.1172,  1.1797,  1.0078,  0.9414,  1.1953,  1.1406,  1.1406,  1.1172,\n",
       "                       1.0625,  1.0391,  1.0000,  3.0312,  1.0625,  0.9570,  1.1172,  1.1641,\n",
       "                       0.9961,  1.1562,  1.2422,  1.0859,  1.1328,  1.2891,  1.0312,  1.1562,\n",
       "                       1.1406,  0.6289,  1.1953,  0.4219,  1.0547,  1.0469,  1.0781,  1.0391,\n",
       "                       1.1562,  1.1797,  1.0703,  1.1484,  1.1484,  1.1797,  1.0859,  1.1484,\n",
       "                       0.9922,  1.0469,  0.9766,  0.5703,  1.0938,  1.2578,  1.1250,  1.0391,\n",
       "                       0.9531,  1.1328,  1.0938,  1.0703,  0.9844,  1.2344,  1.0469,  1.1172,\n",
       "                       1.0703,  1.0547,  1.2422,  1.1172,  1.1406,  1.1797,  1.0234,  1.1406,\n",
       "                       1.1172,  1.0938,  1.1328,  2.2969,  1.0547,  1.1094,  1.0156,  1.1406,\n",
       "                       1.1406,  1.1094,  1.3125,  1.1797,  1.0703,  1.2891,  1.1328,  1.1328,\n",
       "                       0.7070,  0.9766,  1.0156,  0.7695,  0.6367,  1.2109,  1.1797,  1.2109,\n",
       "                       1.3047,  1.1797,  1.1797,  1.0703,  0.9375,  1.0469,  1.1094,  1.0625,\n",
       "                       1.0469,  1.1562,  3.2031,  0.9375,  1.1250,  1.0625,  0.9883,  1.1172,\n",
       "                       1.0469,  0.9922,  0.8555,  1.0156,  1.1016,  0.5859,  1.0703,  0.9844,\n",
       "                       1.1250,  1.0547,  1.1094,  1.0938,  1.1094,  1.0234,  0.8789,  1.2891,\n",
       "                       1.3828,  1.4062,  1.1797,  1.2031,  1.0859,  1.1250,  1.0781,  1.0156,\n",
       "                       1.3281,  1.3438,  1.1094,  1.3125,  1.0938,  1.2266,  0.9766,  1.1562,\n",
       "                       1.0156,  1.8516,  1.0312,  1.2422,  1.0938,  0.9609,  1.0625,  1.0938,\n",
       "                       1.1562,  1.1172,  1.0469,  1.2266,  1.1562,  1.0234,  1.2734,  1.1406,\n",
       "                       1.0469,  1.0703,  1.1250,  1.0859,  1.1172,  1.1797,  1.0703,  1.3125,\n",
       "                       1.1094,  0.9766,  1.0781,  1.1250,  0.9570,  1.1562,  1.2969,  1.0391,\n",
       "                       0.9922,  1.2656,  1.1328,  1.0156,  1.2812,  1.0234,  1.3047,  1.2969,\n",
       "                       0.9531,  1.0703,  1.1094,  1.2344,  1.0781,  1.0000,  1.0312,  1.0703,\n",
       "                       0.9961,  1.0781,  1.0859,  1.2734,  1.2031,  1.2188,  1.2500,  1.1875,\n",
       "                       1.2344,  1.1094,  1.2422,  1.0859,  1.2031,  1.8594,  1.1875,  0.9023,\n",
       "                       1.0703,  1.0859,  1.1016,  1.0547,  1.0078,  1.1484,  1.0234,  1.2109,\n",
       "                       1.0703,  1.2188,  1.1953,  0.8750,  1.1406,  1.0625,  0.9492,  1.1953,\n",
       "                       1.0625,  1.1641,  1.2266,  1.2266,  0.2129,  1.0781,  1.2578,  1.0078,\n",
       "                       1.1719,  0.9844,  1.2578,  1.2109,  1.0000,  1.0625,  1.2266,  1.1250,\n",
       "                       1.0859,  1.1328,  1.0781,  1.0391,  0.8750,  1.2109,  1.1016,  2.3906,\n",
       "                       1.0938,  1.3359,  1.1250,  1.1406,  1.2188,  1.1094,  1.1484,  0.9844,\n",
       "                       1.0156,  0.5156,  1.0703,  0.9922,  0.9844,  1.0781,  1.3359,  1.0781,\n",
       "                       0.9883,  1.0547,  1.0859,  0.9531,  1.2188,  1.1797,  1.0625,  1.1406,\n",
       "                       1.1250,  1.1172,  0.4023,  1.3672,  1.4375,  1.2500,  1.1250,  0.4062,\n",
       "                       1.1562,  0.9766,  1.0469,  1.0156,  1.1562,  1.1406,  1.1641,  1.0156,\n",
       "                       1.2266,  1.0000,  1.3125,  1.0859,  1.0859,  0.9805,  1.2969,  1.1719,\n",
       "                       1.2578,  0.9414,  1.2031,  1.2344,  1.0391,  1.1016,  0.9922,  1.1094,\n",
       "                       1.1719,  1.0547,  1.1641,  1.0234,  1.0938,  1.0391,  1.2109,  1.1875,\n",
       "                       1.0703,  1.1016,  1.2109,  0.9844,  0.9961,  1.0391,  0.9883,  1.2578,\n",
       "                       1.1797,  1.1094,  0.9688,  1.0625,  1.0547,  1.0547,  1.0547,  1.0078,\n",
       "                       0.8867,  0.9336,  1.0547,  1.0547,  1.1172,  1.0312,  1.3281,  1.0781,\n",
       "                       1.2109,  1.8828,  1.1250,  1.0781,  1.2422,  1.0859,  1.0938,  1.1562,\n",
       "                       1.0547,  1.2344,  1.1406,  1.2656,  1.2578,  1.0547,  1.1406,  1.2500,\n",
       "                       1.1719,  0.9727,  1.0391,  1.0078,  1.0781,  1.2344,  1.1250,  1.9688,\n",
       "                       1.0781,  1.1562,  1.0938,  1.1797,  0.9648,  1.2031,  1.1016,  1.0391,\n",
       "                       2.0000,  0.9492,  1.2422,  1.0938,  1.0312,  1.0625,  1.2656,  1.1328,\n",
       "                       0.9922,  1.1953,  0.9102,  1.0469,  1.0859,  1.8906,  1.0781,  1.3906,\n",
       "                       1.0938,  1.2188,  1.2344,  1.0703,  0.9766,  1.1406,  1.1562,  1.0469,\n",
       "                       1.2031,  1.0469,  1.3047,  1.1641,  1.1719,  1.0859,  1.0469,  1.1875,\n",
       "                       0.9766,  1.0703,  1.1797,  1.1484,  1.0469,  0.4219,  1.2812,  1.1641,\n",
       "                      -1.0000,  1.2578,  1.0703,  1.1484,  1.1953,  0.9453,  1.0781,  1.0781,\n",
       "                       1.2500,  0.9023,  1.2031,  1.3438,  1.1094,  1.0938,  0.0287,  1.1016,\n",
       "                       1.2578,  1.0469,  1.0938,  0.3672,  0.9453,  1.0234,  1.0234,  1.1172,\n",
       "                       1.1328,  1.1172,  0.9961,  1.2109,  1.0078,  0.9727,  1.1094,  0.9844,\n",
       "                       0.9727,  1.0391,  1.0625,  0.9609,  1.0469,  1.1094,  1.0469,  1.1016,\n",
       "                       1.0625,  1.0234,  1.1016,  1.1016,  1.0625,  1.1562,  1.1484,  1.0000,\n",
       "                       1.2500,  1.0391,  1.0469,  0.7539,  1.0703,  0.9922,  0.9727,  1.0156,\n",
       "                       1.2266,  1.1016,  1.1719,  1.0781,  1.1797,  1.0391,  1.1094,  1.2109,\n",
       "                       1.0781,  1.1172,  1.1484,  1.0312,  1.1562,  1.0625,  1.0859,  1.2500,\n",
       "                       1.0625,  1.0312,  1.1016,  1.2188,  1.0781,  1.0469,  1.1484,  1.1172,\n",
       "                       1.1641,  1.3750,  1.0078,  0.8047,  1.1016,  1.1406,  1.3750,  1.0156,\n",
       "                       1.0781,  1.1953,  1.0625,  1.2500,  1.1797,  1.1328,  1.0859,  1.0000,\n",
       "                       1.1016,  1.1016,  0.9609,  1.1641,  1.1562,  1.0156,  1.1016,  1.2422,\n",
       "                       1.0000,  1.1641,  1.1094,  1.1875,  0.8555,  0.2441,  1.0625,  1.3125,\n",
       "                       1.0469,  1.1172,  1.0078,  1.0078,  1.0156,  1.1484,  1.2109,  1.5234,\n",
       "                       1.0234,  1.0625,  1.1641,  1.0703,  0.9727,  1.1562,  1.1016,  1.0703,\n",
       "                       1.1562,  1.1641,  1.0781,  1.0156,  1.1016,  1.4844,  1.0156,  1.1797,\n",
       "                       1.1641,  1.0781,  0.9961,  1.1016,  1.2578,  1.1250,  1.2344,  1.0781,\n",
       "                       1.1875,  1.0547,  1.1172,  1.1328,  1.1797,  0.9648,  1.1016,  0.9453,\n",
       "                       1.1719,  0.9688,  1.0391,  1.3125,  1.2500,  1.0859,  2.2500,  1.0469,\n",
       "                       0.9688,  0.9688,  0.5078,  1.1797,  1.3906,  1.1641,  1.1250,  1.2109,\n",
       "                       1.0469,  1.1094,  1.2891,  1.1562,  1.2344,  0.9883,  1.0781,  0.9727,\n",
       "                       1.0703,  1.2344,  1.1484,  1.0391,  1.0078,  0.7578,  1.0703,  1.0234,\n",
       "                       1.1094,  1.1016,  1.1797,  1.1484,  1.2109,  1.1016,  1.1953,  1.2500,\n",
       "                       1.0469,  1.2734,  0.9492,  0.9961,  1.1641,  1.2422,  1.1641,  1.0781,\n",
       "                       1.3828,  1.1094,  1.1797,  1.0469,  1.0938,  0.9648,  1.0234,  1.1484,\n",
       "                       1.0547,  0.8438,  1.1562,  1.0156,  1.0078,  1.0938,  1.0078,  1.1406])),\n",
       "             ('0.auto_model.encoder.block.7.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0222, -0.0591, -0.0737,  ..., -0.0171, -0.0569, -0.0294],\n",
       "                      [ 0.0087,  0.0383,  0.0245,  ...,  0.0334, -0.0037, -0.0454],\n",
       "                      [-0.0542,  0.0146,  0.0132,  ..., -0.0452, -0.0091,  0.0361],\n",
       "                      ...,\n",
       "                      [-0.0649,  0.0265,  0.0322,  ...,  0.0057,  0.0330,  0.0179],\n",
       "                      [-0.0128, -0.0059, -0.0073,  ..., -0.0791,  0.0117, -0.0123],\n",
       "                      [ 0.0359,  0.0171, -0.0014,  ..., -0.0045, -0.0148,  0.0488]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.1631, -0.3594, -0.4922,  ..., -0.2402,  0.2334, -0.3066],\n",
       "                      [ 0.0422,  0.0271, -0.2598,  ...,  0.0262,  0.2617, -0.0913],\n",
       "                      [ 0.0610,  0.0417,  0.0825,  ..., -0.2754, -0.2559,  0.1206],\n",
       "                      ...,\n",
       "                      [-0.7578,  0.0449,  0.1196,  ..., -0.3672,  0.2812, -0.0256],\n",
       "                      [-0.3574, -0.2676,  0.2539,  ...,  0.2197, -0.0019,  0.0427],\n",
       "                      [ 0.0859, -0.0518, -0.2715,  ...,  0.3770, -0.0718,  0.0074]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 1.7188,  0.0708,  0.9688,  ...,  0.1260, -0.2471, -0.5469],\n",
       "                      [-0.2285,  0.2266, -0.4277,  ...,  0.9766, -0.6406, -0.0427],\n",
       "                      [ 0.0069,  0.0160,  0.6680,  ..., -0.7031,  1.7422, -0.4238],\n",
       "                      ...,\n",
       "                      [-0.2178, -0.2246,  0.5195,  ..., -0.0752, -0.4961,  0.7969],\n",
       "                      [ 0.7227, -0.5586, -0.5039,  ...,  1.1797, -0.1011,  0.2275],\n",
       "                      [-0.8477,  0.3438,  0.4473,  ..., -0.3672, -0.0342,  0.0208]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-1.4531,  0.0447,  0.3809,  ...,  0.4609,  0.1201, -0.2598],\n",
       "                      [-0.8008,  0.0083,  0.0566,  ..., -0.1875,  1.2656,  0.7656],\n",
       "                      [-0.5039,  0.0103,  0.4004,  ..., -0.3203,  0.6250, -0.8281],\n",
       "                      ...,\n",
       "                      [-0.0449, -1.7734,  1.6328,  ..., -0.3613,  1.2109,  0.7422],\n",
       "                      [ 0.2793,  0.5820, -1.1406,  ..., -0.3555,  0.2100,  0.9414],\n",
       "                      [ 0.1523, -0.3379, -0.1602,  ...,  0.8789,  0.0398,  0.8125]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1533,  0.1582,  0.1465,  0.1572,  0.1465,  0.1455,  0.0449,  0.1660,\n",
       "                       0.0820,  0.1611,  0.1689,  0.1592,  0.1182,  0.1514,  0.1338,  0.1641,\n",
       "                       0.1660,  0.1553,  0.1592,  0.1465,  0.1235,  0.1680,  0.0442,  0.1406,\n",
       "                       0.1494,  0.1621,  0.1514,  0.1387,  0.1562,  0.1416,  0.1270,  0.1396,\n",
       "                       0.1680,  0.1465,  0.1543,  0.1689,  0.1230,  0.1963,  0.1406,  0.1611,\n",
       "                       0.1582,  0.1436,  0.1543,  0.1602, -0.0786,  0.1543,  0.1074,  0.1455,\n",
       "                       0.1826,  0.0322,  0.1621,  0.1641,  0.1299,  0.1504, -0.0153,  0.1377,\n",
       "                       0.0413,  0.1494,  0.1631,  0.1523,  0.1768,  0.1309,  0.1797,  0.1416,\n",
       "                       0.1338,  0.0454,  0.1426,  0.1426,  0.1553,  0.1611,  0.1572,  0.1494,\n",
       "                       0.1309,  0.1533,  0.1494,  0.1406,  0.1357,  0.1719,  0.1475,  0.1729,\n",
       "                       0.1553,  0.1650,  0.1582,  0.1641,  0.1445,  0.1523,  0.1699,  0.1543,\n",
       "                       0.1572,  0.1514,  0.1494,  0.1377,  0.1553,  0.1245,  0.1318,  0.1348,\n",
       "                       0.1475,  0.1494,  0.1426,  0.1279,  0.1641,  0.1699,  0.1143,  0.1318,\n",
       "                       0.1494,  0.1328,  0.1572,  0.1445,  0.1338,  0.0874,  0.1631,  0.1377,\n",
       "                       0.1484,  0.1279,  0.1582,  0.1357,  0.1611,  0.1348,  0.1426,  0.1543,\n",
       "                       0.1387,  0.1602,  0.1553,  0.1494,  0.1250,  0.1572,  0.1650,  0.1582,\n",
       "                       0.1455,  0.1787,  0.0898,  0.1787,  0.1445,  0.1289,  0.1436,  0.1328,\n",
       "                       0.1758,  0.1475,  0.1689,  0.1680,  0.1396,  0.1738,  0.1465,  0.1465,\n",
       "                       0.1543,  0.1689,  0.1475,  0.1826,  0.1465,  0.1689,  0.1523,  0.1582,\n",
       "                       0.1436,  0.1777,  0.1436,  0.1465,  0.1533,  0.1504,  0.1465,  0.1396,\n",
       "                       0.1445,  0.1396,  0.1279,  0.0649,  0.1289,  0.1318,  0.1592,  0.1709,\n",
       "                       0.1602,  0.1680,  0.1602,  0.1660,  0.1514,  0.1230,  0.1426,  0.1416,\n",
       "                       0.1582,  0.0791,  0.1475,  0.0208,  0.1455,  0.1494,  0.1504,  0.1445,\n",
       "                       0.1699,  0.1895,  0.1250,  0.1377,  0.1543,  0.1572,  0.1572,  0.1455,\n",
       "                       0.1147,  0.1602,  0.1572,  0.0417,  0.1631,  0.1689,  0.1592,  0.1191,\n",
       "                       0.1221,  0.1660,  0.1582,  0.1484,  0.1562,  0.1729,  0.1387,  0.1572,\n",
       "                       0.1465,  0.1602,  0.1631,  0.1416,  0.1797,  0.1602,  0.1387,  0.1699,\n",
       "                       0.1553,  0.1709,  0.1514,  0.1064,  0.0957,  0.1582,  0.1514,  0.1592,\n",
       "                       0.1260,  0.1689,  0.1807,  0.1436,  0.1582,  0.1670,  0.1270,  0.1494,\n",
       "                       0.1211,  0.1357,  0.1387,  0.1035,  0.1123,  0.1543,  0.1494,  0.1426,\n",
       "                       0.1475,  0.1680,  0.1562,  0.1494,  0.1348,  0.1406,  0.1514,  0.1465,\n",
       "                       0.1270,  0.1611,  0.0879,  0.1406,  0.1309,  0.1260,  0.1367,  0.1465,\n",
       "                       0.1504,  0.1289,  0.0649,  0.1406,  0.1670,  0.1177,  0.1562,  0.1582,\n",
       "                       0.1367,  0.1475,  0.1445,  0.1533,  0.1504,  0.1543,  0.0452,  0.1914,\n",
       "                       0.1416,  0.0240,  0.1572,  0.1641,  0.1562,  0.1465,  0.1611,  0.1582,\n",
       "                       0.1855,  0.1875,  0.1494,  0.1553,  0.1465,  0.1240,  0.1416,  0.1680,\n",
       "                       0.1279,  0.1138,  0.1387,  0.1641,  0.1387,  0.1396,  0.1436,  0.1475,\n",
       "                       0.1689,  0.1514,  0.1455,  0.1709,  0.1680,  0.1514,  0.1875,  0.1641,\n",
       "                       0.1582,  0.1318,  0.1533,  0.1562,  0.1631,  0.1543,  0.1562,  0.1758,\n",
       "                       0.1465,  0.1143,  0.1650,  0.1562,  0.1157,  0.1650,  0.1147,  0.1582,\n",
       "                       0.1426,  0.1611,  0.1494,  0.1406,  0.1543,  0.1348,  0.1475,  0.1709,\n",
       "                       0.1562,  0.1387,  0.1484,  0.1318,  0.1602,  0.1318,  0.1660,  0.1641,\n",
       "                       0.1318,  0.1533,  0.1553,  0.1572,  0.1729,  0.1719,  0.1523,  0.1514,\n",
       "                       0.1572,  0.1680,  0.1387,  0.1475,  0.1387,  0.1138,  0.1611,  0.1357,\n",
       "                       0.1426,  0.1602,  0.1377,  0.1650,  0.1738,  0.1738,  0.1621,  0.1562,\n",
       "                       0.1514,  0.1465,  0.1641,  0.1084,  0.1406,  0.1406,  0.0786,  0.1406,\n",
       "                       0.1514,  0.1572,  0.1504,  0.1758,  0.0491,  0.1260,  0.1748,  0.1387,\n",
       "                       0.1357,  0.1592,  0.1680,  0.1230,  0.1475,  0.1602,  0.1621,  0.1514,\n",
       "                       0.1592,  0.0427,  0.1514,  0.1099,  0.1436,  0.1709,  0.1523,  0.0752,\n",
       "                       0.1719,  0.1689,  0.1670,  0.1406,  0.1582,  0.1475,  0.1445,  0.1465,\n",
       "                       0.1436, -0.0175,  0.1387,  0.1748,  0.1396,  0.1484,  0.1641,  0.1455,\n",
       "                       0.1504,  0.1455,  0.1523,  0.1328,  0.1328,  0.1426,  0.1406,  0.1523,\n",
       "                       0.1279,  0.1245,  0.0544,  0.1504,  0.1465,  0.1523,  0.1348,  0.0598,\n",
       "                       0.1602,  0.1602,  0.1631,  0.1475,  0.1572,  0.1572,  0.1650,  0.1514,\n",
       "                       0.1143,  0.1206,  0.1475,  0.1562,  0.1602,  0.1475,  0.1533,  0.1709,\n",
       "                       0.1602,  0.1348,  0.1719,  0.1621,  0.1436,  0.1641,  0.1406,  0.1738,\n",
       "                       0.1660,  0.1650,  0.1416,  0.1553,  0.1533,  0.1553,  0.1621,  0.1709,\n",
       "                       0.1680,  0.1465,  0.1582,  0.1445,  0.1484,  0.1445,  0.1465,  0.1631,\n",
       "                       0.1797,  0.1543,  0.1079,  0.1426,  0.1377,  0.1387,  0.1367,  0.1387,\n",
       "                       0.1328,  0.1289,  0.1719,  0.1475,  0.1475,  0.1611,  0.0654,  0.1426,\n",
       "                       0.1777,  0.0908,  0.1699,  0.1504,  0.1875,  0.1318,  0.1416,  0.1445,\n",
       "                       0.1504,  0.1631,  0.1543,  0.1562,  0.0562,  0.1562,  0.1377,  0.1621,\n",
       "                       0.1602,  0.1504,  0.1309,  0.1270,  0.1406,  0.1680,  0.1387,  0.0415,\n",
       "                       0.1455,  0.1660,  0.1572,  0.1572,  0.1279,  0.1689,  0.1816,  0.1475,\n",
       "                       0.0776,  0.1455,  0.1709,  0.1348,  0.1445,  0.1230,  0.1309,  0.1768,\n",
       "                       0.1387,  0.1318,  0.1377,  0.1553,  0.1680,  0.1021,  0.1797,  0.1465,\n",
       "                       0.1533,  0.1699,  0.1533,  0.1572,  0.1494,  0.1670,  0.1553,  0.1592,\n",
       "                       0.1543,  0.1494,  0.1680,  0.1650,  0.1602,  0.1543,  0.1729,  0.1709,\n",
       "                       0.1338,  0.1523,  0.1689,  0.1611,  0.1455,  0.0742,  0.1670,  0.1777,\n",
       "                       0.1377,  0.2129,  0.1494,  0.1670,  0.1572,  0.1426,  0.1572,  0.1289,\n",
       "                       0.1621,  0.1245,  0.1592,  0.1719,  0.1621,  0.1650,  0.0194,  0.1562,\n",
       "                       0.1543,  0.1523,  0.1729,  0.0674,  0.1309,  0.1592,  0.1670,  0.1670,\n",
       "                       0.1553,  0.1865,  0.1465,  0.1592,  0.1484,  0.1416,  0.1377,  0.1494,\n",
       "                       0.1270,  0.1387,  0.1235,  0.1436,  0.1279,  0.1455,  0.1592,  0.1650,\n",
       "                       0.1533,  0.1650,  0.1494,  0.1523,  0.1367,  0.1650,  0.1523,  0.1533,\n",
       "                       0.1758,  0.1562,  0.1670,  0.0947,  0.1699,  0.1533,  0.1270,  0.1621,\n",
       "                       0.1621,  0.1484,  0.1602,  0.1357,  0.1768,  0.1465,  0.1660,  0.1807,\n",
       "                       0.1416,  0.1855,  0.1602,  0.1562,  0.1699,  0.1318,  0.1396,  0.1758,\n",
       "                       0.1572,  0.1387,  0.1475,  0.1504,  0.1611,  0.0986,  0.1572,  0.1348,\n",
       "                       0.1670,  0.1846,  0.1416,  0.0972,  0.1494,  0.1504,  0.1865,  0.1533,\n",
       "                       0.1455,  0.1650,  0.1445,  0.1514,  0.1826,  0.1641,  0.1533,  0.1494,\n",
       "                       0.1357,  0.1523,  0.1289,  0.1484,  0.1641,  0.1367,  0.1318,  0.1553,\n",
       "                       0.1475,  0.1572,  0.1680,  0.1523,  0.1211,  0.0204,  0.1514,  0.1963,\n",
       "                       0.1680,  0.1465,  0.1377,  0.1533,  0.1729,  0.1514,  0.1592,  0.1260,\n",
       "                       0.1465,  0.1357,  0.1572,  0.1494,  0.1514,  0.1377,  0.1416,  0.1553,\n",
       "                       0.1777,  0.1406,  0.1611,  0.1357,  0.1562,  0.0864,  0.1494,  0.1660,\n",
       "                       0.1475,  0.1699,  0.1504,  0.1641,  0.1426,  0.1436,  0.1562,  0.1416,\n",
       "                       0.1689,  0.1348,  0.1641,  0.1602,  0.1611,  0.1152,  0.1611,  0.1396,\n",
       "                       0.1621,  0.1387,  0.1494,  0.1445,  0.1768,  0.1357,  0.0532,  0.1543,\n",
       "                       0.1562,  0.1494,  0.0405,  0.1602,  0.1475,  0.1572,  0.1572,  0.1475,\n",
       "                       0.1504,  0.1138,  0.1729,  0.1523,  0.1719,  0.1572,  0.1592,  0.1367,\n",
       "                       0.1484,  0.1543,  0.1494,  0.1416,  0.1387,  0.1299,  0.1416,  0.1572,\n",
       "                       0.1621,  0.1572,  0.1738,  0.1484,  0.1631,  0.1357,  0.1846,  0.1670,\n",
       "                       0.1494,  0.1650,  0.1572,  0.1357,  0.1543,  0.1572,  0.1514,  0.1494,\n",
       "                       0.1709,  0.1602,  0.1426,  0.1592,  0.1523,  0.1641,  0.1768,  0.1895,\n",
       "                       0.1572,  0.1299,  0.1592,  0.1426,  0.1650,  0.1572,  0.1436,  0.1670])),\n",
       "             ('0.auto_model.encoder.block.7.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.5391,  0.4707,  0.0220,  ..., -0.0075,  0.4688, -0.3145],\n",
       "                      [ 0.1465, -0.0562, -0.1289,  ..., -0.5742,  0.1680, -0.1943],\n",
       "                      [-0.4102, -0.0898, -0.4395,  ..., -0.4551,  0.0251,  0.3457],\n",
       "                      ...,\n",
       "                      [-0.0952, -0.4004,  0.4590,  ...,  0.3242, -0.0928,  0.5312],\n",
       "                      [ 0.2363,  0.3691,  0.5977,  ..., -0.4785, -0.7031,  0.1523],\n",
       "                      [ 0.2266,  1.4531, -0.7461,  ...,  0.5430,  0.5820, -0.0791]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-1.4844e-01,  4.0234e-01,  1.9629e-01,  ...,  1.7871e-01,\n",
       "                       -1.2891e-01,  3.2031e-01],\n",
       "                      [ 1.5137e-01, -1.2500e-01, -7.1875e-01,  ...,  6.5918e-02,\n",
       "                        1.3184e-01,  3.8574e-02],\n",
       "                      [-1.0938e-01,  2.1484e-01,  2.5000e-01,  ...,  3.1055e-01,\n",
       "                        2.5391e-01,  1.8120e-04],\n",
       "                      ...,\n",
       "                      [-2.5586e-01,  2.1680e-01, -3.3789e-01,  ..., -1.7480e-01,\n",
       "                        1.0693e-01,  4.5898e-01],\n",
       "                      [ 7.2861e-04,  6.9922e-01,  3.9453e-01,  ..., -1.1572e-01,\n",
       "                        1.0693e-01,  4.0820e-01],\n",
       "                      [-4.8047e-01,  1.4160e-01,  5.3906e-01,  ..., -5.8984e-01,\n",
       "                       -2.1362e-02,  1.4453e-01]])),\n",
       "             ('0.auto_model.encoder.block.7.layer.1.layer_norm.weight',\n",
       "              tensor([1.1641, 1.1875, 1.0938, 1.1641, 1.1406, 1.1875, 0.2461, 1.1953, 2.1406,\n",
       "                      1.1641, 1.1875, 1.1797, 1.1797, 1.2422, 1.2031, 1.1562, 1.1953, 1.1562,\n",
       "                      1.0859, 1.1484, 1.1875, 1.2266, 1.9062, 1.1094, 1.2109, 1.2422, 1.1016,\n",
       "                      1.2344, 1.2578, 1.1641, 1.2188, 1.0938, 1.1328, 1.1875, 1.1797, 1.1484,\n",
       "                      1.3438, 1.2578, 1.0000, 1.8203, 1.1562, 1.3359, 1.0703, 1.1406, 2.9688,\n",
       "                      1.2031, 1.1797, 1.1875, 1.2109, 0.5312, 1.2891, 1.1953, 1.6016, 1.1094,\n",
       "                      1.1562, 1.0781, 1.7812, 1.1016, 1.1875, 1.2422, 1.1875, 1.1719, 1.3594,\n",
       "                      1.1250, 1.1484, 1.4453, 1.1484, 1.1094, 1.3125, 1.1250, 1.1328, 1.5078,\n",
       "                      1.1484, 1.2969, 1.0469, 1.1562, 1.3359, 1.1250, 1.2188, 1.2031, 1.1406,\n",
       "                      1.1953, 1.1172, 1.1953, 1.2109, 1.2656, 1.2266, 1.1328, 1.2109, 1.1328,\n",
       "                      1.3359, 1.2422, 1.1406, 1.2656, 1.0859, 1.1250, 1.2266, 1.2031, 1.1094,\n",
       "                      1.0156, 1.1719, 1.1406, 1.0312, 1.0859, 1.2969, 1.4531, 1.1250, 1.1953,\n",
       "                      1.1094, 1.8594, 1.2031, 1.1484, 1.2031, 1.1328, 1.0703, 1.1641, 1.1250,\n",
       "                      1.1797, 1.2891, 1.1797, 1.2031, 1.0547, 1.3047, 1.0781, 1.1484, 1.2656,\n",
       "                      1.1875, 1.1328, 1.1875, 1.1641, 1.4688, 1.1641, 1.2109, 1.0703, 1.1328,\n",
       "                      1.1641, 1.3125, 1.1484, 1.2656, 1.1484, 1.1484, 1.2812, 1.2734, 1.2422,\n",
       "                      1.1328, 1.2734, 1.1641, 1.0938, 1.1719, 1.2031, 1.1797, 1.2031, 1.1328,\n",
       "                      1.1953, 1.2344, 1.2266, 1.1484, 1.2266, 1.2031, 1.1953, 1.1094, 1.1328,\n",
       "                      1.0078, 3.3594, 1.1719, 1.0781, 1.1172, 1.2266, 1.0000, 1.1562, 1.1328,\n",
       "                      1.1484, 1.1562, 1.2969, 1.1484, 1.2031, 1.2031, 0.4727, 1.2031, 0.4395,\n",
       "                      1.1562, 1.1641, 1.0391, 1.0156, 1.1094, 1.1797, 1.0859, 1.2266, 1.2109,\n",
       "                      1.2031, 1.2188, 1.2500, 1.1406, 1.1562, 1.1406, 0.5664, 1.2188, 1.3203,\n",
       "                      1.2578, 1.0938, 1.0781, 1.2266, 1.1797, 1.1641, 1.1562, 1.0859, 1.1328,\n",
       "                      1.0703, 1.1328, 1.0625, 1.2500, 1.1797, 1.3047, 1.3203, 1.1406, 1.1797,\n",
       "                      1.1484, 1.1875, 1.2734, 2.0469, 1.0625, 1.1562, 1.1641, 1.0859, 1.2812,\n",
       "                      1.2031, 1.2344, 1.1641, 1.2500, 1.1875, 1.0703, 1.1562, 0.8008, 1.0000,\n",
       "                      1.1250, 0.9102, 0.8359, 1.2031, 1.2422, 1.1719, 1.3281, 1.1406, 1.1719,\n",
       "                      1.1406, 1.1094, 1.1641, 1.1250, 1.1719, 1.0156, 1.1875, 3.2031, 1.0312,\n",
       "                      1.1406, 1.2109, 1.1094, 1.1953, 1.1562, 1.0859, 0.7812, 1.0781, 1.1250,\n",
       "                      0.7305, 1.2422, 1.1172, 1.1016, 1.1406, 1.1641, 1.1094, 1.1797, 1.1484,\n",
       "                      0.9414, 1.3281, 1.4375, 1.3281, 1.2891, 1.3906, 1.2031, 1.1016, 1.0859,\n",
       "                      1.1172, 1.2578, 1.3672, 1.1875, 1.4141, 1.2031, 1.2891, 1.1875, 1.1094,\n",
       "                      1.1094, 1.8203, 1.0859, 1.2656, 1.0859, 1.0391, 1.1328, 1.2188, 1.2109,\n",
       "                      1.1719, 1.0859, 1.2266, 1.2969, 1.1797, 1.1953, 1.1719, 1.1953, 1.1016,\n",
       "                      1.1016, 1.2656, 1.2188, 1.3203, 1.1484, 1.3516, 1.2109, 1.0312, 1.1797,\n",
       "                      1.0703, 1.0156, 1.2344, 1.4375, 1.1172, 1.1250, 1.2656, 1.2031, 1.1797,\n",
       "                      1.2812, 1.2422, 1.3203, 1.2812, 1.1172, 1.1016, 1.1953, 1.2656, 1.2109,\n",
       "                      1.1641, 1.1719, 1.1328, 1.1484, 1.0781, 1.0391, 1.2500, 1.2578, 1.3516,\n",
       "                      1.1797, 1.2578, 1.1641, 1.0859, 1.1484, 1.1172, 1.0859, 2.0000, 1.1250,\n",
       "                      1.1797, 1.2969, 1.2188, 1.1719, 1.2109, 1.1250, 1.1562, 1.1641, 1.2188,\n",
       "                      1.0703, 1.2734, 1.1875, 1.0469, 1.1484, 1.1406, 0.7812, 1.3125, 1.1172,\n",
       "                      1.2656, 1.1484, 1.0781, 0.2539, 1.1641, 1.2812, 1.1953, 1.1953, 1.1719,\n",
       "                      1.0156, 1.2266, 1.2266, 1.1562, 1.2578, 1.1797, 1.2031, 1.1406, 1.1562,\n",
       "                      0.9141, 1.1016, 1.3516, 1.2188, 2.6562, 1.1406, 1.2500, 1.1641, 1.2266,\n",
       "                      1.2578, 1.1484, 1.1953, 1.1250, 1.1641, 0.4980, 1.2031, 1.1406, 1.0938,\n",
       "                      1.2344, 1.3594, 1.1719, 1.1406, 1.0469, 1.2109, 1.1406, 1.2188, 1.2266,\n",
       "                      1.0938, 1.1172, 1.1484, 1.2344, 0.5117, 1.4688, 1.3516, 1.2969, 1.1641,\n",
       "                      0.4199, 1.2969, 1.2422, 1.1719, 1.0859, 1.2031, 1.1562, 1.2734, 1.2109,\n",
       "                      1.1797, 1.0781, 1.3672, 1.0938, 1.2188, 1.0938, 1.2422, 1.1719, 1.2969,\n",
       "                      1.0781, 1.2656, 1.1953, 1.1172, 1.1406, 1.0625, 1.2266, 1.0547, 1.1641,\n",
       "                      1.1953, 1.0234, 1.0938, 1.0938, 1.3281, 1.2578, 1.1484, 1.0625, 1.1641,\n",
       "                      1.1953, 1.1250, 1.1250, 1.1797, 1.1250, 1.2891, 1.1484, 1.0234, 1.0312,\n",
       "                      1.1250, 1.2734, 1.1484, 1.1719, 0.9922, 0.9453, 1.1562, 1.1250, 1.1562,\n",
       "                      1.1797, 1.2969, 1.1328, 1.2578, 1.9766, 1.3047, 1.0781, 1.2500, 1.1484,\n",
       "                      1.1953, 1.2891, 1.0469, 1.1719, 1.1172, 1.2891, 1.4609, 1.1797, 1.2656,\n",
       "                      1.2031, 1.1953, 1.0234, 1.0703, 1.0234, 1.0312, 1.1562, 1.1875, 2.0312,\n",
       "                      1.1484, 1.1953, 1.0938, 1.1172, 1.1875, 1.2266, 1.2266, 1.1328, 1.8516,\n",
       "                      1.2344, 1.2656, 1.1094, 1.0234, 1.1484, 1.1641, 1.2188, 1.1562, 1.2812,\n",
       "                      1.0469, 1.1641, 1.1328, 1.9375, 1.2188, 1.4375, 1.2031, 1.3047, 1.1953,\n",
       "                      1.2109, 1.1641, 1.1406, 1.2031, 1.1719, 1.1641, 1.1328, 1.3438, 1.1797,\n",
       "                      1.1719, 1.1016, 1.1719, 1.1875, 1.0938, 1.1953, 1.2266, 1.2656, 1.1953,\n",
       "                      0.4082, 1.2578, 1.2500, 1.0312, 1.3125, 1.1953, 1.1875, 1.1719, 1.0859,\n",
       "                      1.1641, 1.0312, 1.3281, 1.0547, 1.2812, 1.3594, 1.1328, 1.2109, 0.2344,\n",
       "                      1.1016, 1.2188, 1.1562, 1.2422, 0.4941, 1.0781, 1.1172, 1.2109, 1.2109,\n",
       "                      1.1406, 1.2344, 1.2422, 1.2109, 1.2266, 1.1094, 1.1016, 1.2031, 1.0312,\n",
       "                      1.1250, 1.1094, 1.0859, 1.1250, 1.0625, 1.2500, 1.0781, 1.0625, 1.1484,\n",
       "                      1.0391, 1.1719, 1.0938, 1.3047, 1.2031, 1.1406, 1.1719, 0.9766, 1.1953,\n",
       "                      0.8359, 1.1953, 1.0703, 1.0547, 1.1797, 1.3047, 1.1250, 1.2422, 1.1797,\n",
       "                      1.2500, 1.0781, 1.0781, 1.2656, 1.1562, 1.1172, 1.1172, 1.1328, 1.2734,\n",
       "                      1.1641, 1.0938, 1.2188, 1.0938, 1.1172, 1.1719, 1.2500, 1.0859, 1.1172,\n",
       "                      1.1172, 0.9883, 1.1172, 1.3125, 1.1328, 0.8281, 1.1875, 1.1172, 1.3203,\n",
       "                      1.1641, 1.1641, 1.1484, 1.0625, 1.2031, 1.2109, 1.2266, 1.0859, 1.0703,\n",
       "                      1.2656, 1.1250, 1.0781, 1.1250, 1.1641, 1.0859, 1.1094, 1.3047, 1.0312,\n",
       "                      1.1953, 1.0625, 1.2266, 0.9297, 0.3320, 1.0234, 1.2422, 1.1172, 1.1875,\n",
       "                      1.1719, 1.1875, 1.1562, 1.2578, 1.2266, 1.5703, 1.1172, 1.1250, 1.0156,\n",
       "                      1.1719, 1.2031, 1.2266, 1.2344, 1.1953, 1.1562, 1.1172, 1.1875, 1.2500,\n",
       "                      1.1797, 1.5469, 1.0625, 1.2266, 1.1328, 1.1484, 1.1562, 1.2109, 1.0859,\n",
       "                      1.1719, 1.1484, 1.1562, 1.2266, 1.2031, 1.2344, 1.0625, 1.2500, 1.0469,\n",
       "                      1.1016, 1.1719, 1.2188, 1.1875, 1.2812, 1.3750, 1.2656, 1.1328, 2.2656,\n",
       "                      1.1484, 1.1484, 1.2109, 0.5000, 1.2578, 1.2578, 1.2266, 1.2031, 1.1719,\n",
       "                      1.0703, 1.2266, 1.1250, 1.1719, 1.1562, 1.0312, 1.0938, 1.0547, 1.1719,\n",
       "                      1.2734, 1.0703, 1.1172, 1.0859, 0.9570, 1.0469, 1.0547, 1.2031, 1.1094,\n",
       "                      1.2734, 1.1953, 1.1953, 1.2188, 1.1641, 1.1562, 1.1250, 1.3359, 1.1016,\n",
       "                      1.0781, 1.2734, 1.3125, 1.2891, 1.2422, 1.5469, 1.0781, 1.1797, 1.1406,\n",
       "                      1.2578, 1.0859, 1.0859, 1.1875, 1.0781, 0.8984, 1.1875, 1.1641, 1.0703,\n",
       "                      1.1328, 1.2031, 1.1406])),\n",
       "             ('0.auto_model.encoder.block.8.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0537,  0.0192, -0.0311,  ..., -0.0092, -0.0286,  0.0664],\n",
       "                      [ 0.0114, -0.0369, -0.0613,  ..., -0.0106, -0.0176,  0.0264],\n",
       "                      [ 0.0091, -0.0415, -0.0162,  ...,  0.0383, -0.0579,  0.0669],\n",
       "                      ...,\n",
       "                      [ 0.0114, -0.0317, -0.0109,  ...,  0.0220,  0.0184,  0.0136],\n",
       "                      [-0.0242,  0.0762,  0.0071,  ..., -0.0062, -0.0457,  0.0078],\n",
       "                      [-0.0356,  0.0020, -0.0070,  ...,  0.0238, -0.0330, -0.0162]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.5000, -0.0361, -0.0060,  ..., -0.0898, -0.2129,  0.1016],\n",
       "                      [-0.0077,  0.4668, -0.3340,  ...,  0.1147,  0.1235, -0.2334],\n",
       "                      [-0.0942,  0.3496, -0.0308,  ..., -0.1309, -0.1689,  0.3223],\n",
       "                      ...,\n",
       "                      [ 0.0986,  0.3652,  0.4238,  ...,  0.1387, -0.3359,  0.3398],\n",
       "                      [ 0.1934,  0.1543, -0.3828,  ..., -0.0850, -0.2734, -0.1030],\n",
       "                      [-0.1875, -0.2100, -0.1533,  ...,  0.2246,  0.1260, -0.2402]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.8203,  0.4785, -0.9766,  ..., -0.4805,  1.4219,  0.6641],\n",
       "                      [-0.8477, -0.3301,  0.1533,  ...,  0.9102,  0.8398,  0.0215],\n",
       "                      [-0.7539,  0.4863, -0.0942,  ...,  0.0024, -0.1514, -0.6094],\n",
       "                      ...,\n",
       "                      [-0.2617, -0.6484, -0.0223,  ..., -1.1953,  0.0425, -0.7461],\n",
       "                      [ 1.0078, -0.3398,  0.0299,  ...,  0.2871, -1.0234, -1.2812],\n",
       "                      [-0.4238, -0.1104, -0.4590,  ...,  0.9609, -0.7227,  0.5703]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.6016,  0.5352,  0.7227,  ..., -0.0292, -0.6719,  0.6914],\n",
       "                      [-0.5430, -0.6133, -1.2578,  ...,  0.1152,  1.1406,  0.4688],\n",
       "                      [ 1.3203, -0.8359,  1.0703,  ..., -0.8789,  0.8359,  0.6719],\n",
       "                      ...,\n",
       "                      [ 0.1807, -0.7305, -0.4688,  ..., -1.6250,  0.6953, -1.7656],\n",
       "                      [-1.2344, -1.3828, -0.1079,  ..., -0.1670, -1.2188,  0.8359],\n",
       "                      [-0.8555,  0.2734,  0.1270,  ..., -0.1069, -0.2139,  0.0640]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.0.layer_norm.weight',\n",
       "              tensor([0.1465, 0.1562, 0.1562, 0.1465, 0.1533, 0.1572, 0.0304, 0.1797, 0.0820,\n",
       "                      0.1543, 0.1406, 0.1387, 0.1138, 0.1348, 0.1406, 0.1729, 0.1455, 0.1543,\n",
       "                      0.1465, 0.1445, 0.1260, 0.1582, 0.0444, 0.1475, 0.1533, 0.1562, 0.1426,\n",
       "                      0.1367, 0.1533, 0.1445, 0.1436, 0.1436, 0.1523, 0.1504, 0.1699, 0.1621,\n",
       "                      0.1279, 0.1611, 0.1475, 0.1602, 0.1367, 0.1406, 0.1396, 0.1523, 0.1001,\n",
       "                      0.1494, 0.1167, 0.1445, 0.1611, 0.0320, 0.1777, 0.1650, 0.1328, 0.1406,\n",
       "                      0.0287, 0.1309, 0.0605, 0.1377, 0.1562, 0.1455, 0.1758, 0.1270, 0.1562,\n",
       "                      0.1455, 0.1406, 0.0398, 0.1553, 0.1396, 0.1592, 0.1494, 0.1543, 0.1357,\n",
       "                      0.1514, 0.1455, 0.1465, 0.1494, 0.1270, 0.1543, 0.1279, 0.1660, 0.1543,\n",
       "                      0.1396, 0.1572, 0.1562, 0.1455, 0.1514, 0.1562, 0.1572, 0.1455, 0.1377,\n",
       "                      0.1523, 0.1436, 0.1553, 0.1406, 0.1523, 0.1592, 0.1699, 0.1650, 0.1523,\n",
       "                      0.1348, 0.1641, 0.1592, 0.0947, 0.1357, 0.1494, 0.1182, 0.1465, 0.1523,\n",
       "                      0.1338, 0.0825, 0.1523, 0.1650, 0.1436, 0.1387, 0.1465, 0.1533, 0.1514,\n",
       "                      0.1309, 0.1387, 0.1582, 0.1406, 0.1504, 0.1416, 0.1357, 0.1279, 0.1426,\n",
       "                      0.1631, 0.1621, 0.1543, 0.1494, 0.0938, 0.1602, 0.1436, 0.1436, 0.1436,\n",
       "                      0.1348, 0.1631, 0.1553, 0.1611, 0.1475, 0.1436, 0.1787, 0.1396, 0.1396,\n",
       "                      0.1436, 0.1582, 0.1592, 0.1680, 0.1543, 0.1582, 0.1475, 0.1523, 0.1484,\n",
       "                      0.1621, 0.1455, 0.1611, 0.1484, 0.1504, 0.1514, 0.1348, 0.1445, 0.1514,\n",
       "                      0.1226, 0.0747, 0.1465, 0.1299, 0.1504, 0.1523, 0.1602, 0.1523, 0.1641,\n",
       "                      0.1689, 0.1641, 0.1196, 0.1602, 0.1514, 0.1602, 0.0566, 0.1455, 0.0195,\n",
       "                      0.1455, 0.1494, 0.1465, 0.1533, 0.1631, 0.1729, 0.1328, 0.1553, 0.1553,\n",
       "                      0.1465, 0.1494, 0.1494, 0.1328, 0.1455, 0.1572, 0.0554, 0.1533, 0.1484,\n",
       "                      0.1504, 0.1299, 0.1445, 0.1465, 0.1553, 0.1426, 0.1455, 0.1494, 0.1523,\n",
       "                      0.1367, 0.1377, 0.1523, 0.1455, 0.1504, 0.1572, 0.1592, 0.1484, 0.1650,\n",
       "                      0.1465, 0.1572, 0.1494, 0.0962, 0.1064, 0.1670, 0.1387, 0.1543, 0.1279,\n",
       "                      0.1455, 0.1631, 0.1348, 0.1621, 0.1621, 0.1206, 0.1494, 0.1235, 0.1328,\n",
       "                      0.1348, 0.1201, 0.1123, 0.1553, 0.1553, 0.1196, 0.1436, 0.1621, 0.1641,\n",
       "                      0.1465, 0.1348, 0.1455, 0.1602, 0.1465, 0.1348, 0.1592, 0.1016, 0.1328,\n",
       "                      0.1377, 0.1289, 0.1406, 0.1543, 0.1504, 0.1289, 0.0613, 0.1504, 0.1562,\n",
       "                      0.1030, 0.1436, 0.1592, 0.1436, 0.1377, 0.1494, 0.1465, 0.1562, 0.1377,\n",
       "                      0.0515, 0.1660, 0.1455, 0.0140, 0.1660, 0.1484, 0.1631, 0.1201, 0.1475,\n",
       "                      0.1592, 0.1602, 0.1719, 0.1475, 0.1543, 0.1611, 0.1211, 0.1445, 0.1543,\n",
       "                      0.1309, 0.1260, 0.1396, 0.1582, 0.1387, 0.1455, 0.1582, 0.1406, 0.1514,\n",
       "                      0.1426, 0.1553, 0.1582, 0.1748, 0.1602, 0.1729, 0.1475, 0.1602, 0.1250,\n",
       "                      0.1523, 0.1553, 0.1641, 0.1504, 0.1553, 0.1562, 0.1689, 0.1104, 0.1660,\n",
       "                      0.1543, 0.1250, 0.1494, 0.1206, 0.1377, 0.1396, 0.1533, 0.1367, 0.1396,\n",
       "                      0.1572, 0.1426, 0.1377, 0.1719, 0.1621, 0.1582, 0.1562, 0.1143, 0.1523,\n",
       "                      0.1494, 0.1631, 0.1465, 0.1533, 0.1416, 0.1377, 0.1484, 0.1709, 0.1699,\n",
       "                      0.1602, 0.1367, 0.1533, 0.1592, 0.1436, 0.1348, 0.1328, 0.1064, 0.1572,\n",
       "                      0.1494, 0.1533, 0.1572, 0.1426, 0.1582, 0.1543, 0.1504, 0.1562, 0.1377,\n",
       "                      0.1533, 0.1436, 0.1309, 0.1211, 0.1523, 0.1357, 0.0698, 0.1318, 0.1445,\n",
       "                      0.1602, 0.1436, 0.1709, 0.0466, 0.1318, 0.1611, 0.1514, 0.1309, 0.1514,\n",
       "                      0.1562, 0.1260, 0.1572, 0.1553, 0.1475, 0.1660, 0.1406, 0.0415, 0.1572,\n",
       "                      0.1045, 0.1357, 0.1553, 0.1494, 0.0713, 0.1436, 0.1572, 0.1504, 0.1436,\n",
       "                      0.1592, 0.1504, 0.1504, 0.1367, 0.1465, 0.0110, 0.1387, 0.1523, 0.1465,\n",
       "                      0.1494, 0.1562, 0.1377, 0.1377, 0.1475, 0.1416, 0.1416, 0.1387, 0.1426,\n",
       "                      0.1533, 0.1533, 0.1309, 0.1318, 0.0376, 0.1562, 0.1543, 0.1670, 0.1494,\n",
       "                      0.0583, 0.1621, 0.1562, 0.1562, 0.1484, 0.1504, 0.1592, 0.1631, 0.1611,\n",
       "                      0.0991, 0.1270, 0.1387, 0.1396, 0.1621, 0.1514, 0.1396, 0.1572, 0.1641,\n",
       "                      0.1157, 0.1641, 0.1455, 0.1396, 0.1572, 0.1377, 0.1504, 0.1494, 0.1602,\n",
       "                      0.1426, 0.1562, 0.1553, 0.1572, 0.1602, 0.1611, 0.1523, 0.1338, 0.1484,\n",
       "                      0.1387, 0.1514, 0.1396, 0.1592, 0.1641, 0.1572, 0.1533, 0.1191, 0.1484,\n",
       "                      0.1299, 0.1494, 0.1436, 0.1523, 0.1387, 0.1475, 0.1504, 0.1621, 0.1475,\n",
       "                      0.1367, 0.0459, 0.1611, 0.1719, 0.0928, 0.1484, 0.1416, 0.1562, 0.1455,\n",
       "                      0.1494, 0.1387, 0.1465, 0.1543, 0.1426, 0.1670, 0.0552, 0.1494, 0.1357,\n",
       "                      0.1523, 0.1562, 0.1562, 0.1299, 0.1338, 0.1514, 0.1533, 0.1572, 0.0354,\n",
       "                      0.1416, 0.1475, 0.1416, 0.1504, 0.1338, 0.1680, 0.1650, 0.1445, 0.0752,\n",
       "                      0.1699, 0.1582, 0.1504, 0.1416, 0.1357, 0.1118, 0.1523, 0.1377, 0.1406,\n",
       "                      0.1279, 0.1406, 0.1475, 0.1230, 0.1602, 0.1289, 0.1396, 0.1670, 0.1475,\n",
       "                      0.1484, 0.1631, 0.1426, 0.1523, 0.1572, 0.1680, 0.1309, 0.1504, 0.1445,\n",
       "                      0.1484, 0.1416, 0.1484, 0.1611, 0.1562, 0.1582, 0.1621, 0.1582, 0.1406,\n",
       "                      0.0559, 0.1445, 0.1641, 0.1172, 0.1855, 0.1553, 0.1611, 0.1270, 0.1387,\n",
       "                      0.1533, 0.1191, 0.1514, 0.1289, 0.1602, 0.1641, 0.1465, 0.1641, 0.0208,\n",
       "                      0.1484, 0.1504, 0.1494, 0.1729, 0.0723, 0.1318, 0.1484, 0.1533, 0.1553,\n",
       "                      0.1572, 0.1836, 0.1494, 0.1328, 0.1523, 0.1475, 0.1553, 0.1455, 0.1279,\n",
       "                      0.1338, 0.1270, 0.1309, 0.1494, 0.1328, 0.1514, 0.1445, 0.1631, 0.1719,\n",
       "                      0.1504, 0.1562, 0.1416, 0.1562, 0.1338, 0.1426, 0.1748, 0.1562, 0.1562,\n",
       "                      0.1064, 0.1504, 0.1416, 0.1445, 0.1553, 0.1445, 0.1475, 0.1514, 0.1338,\n",
       "                      0.1631, 0.1455, 0.1494, 0.1533, 0.1426, 0.1895, 0.1602, 0.1494, 0.1719,\n",
       "                      0.1523, 0.1455, 0.1719, 0.1631, 0.1465, 0.1455, 0.1367, 0.1523, 0.1104,\n",
       "                      0.1611, 0.1514, 0.1582, 0.1797, 0.1514, 0.0962, 0.1465, 0.1455, 0.1719,\n",
       "                      0.1680, 0.1396, 0.1621, 0.1436, 0.1660, 0.1699, 0.1436, 0.1445, 0.1533,\n",
       "                      0.1484, 0.1484, 0.1426, 0.1533, 0.1553, 0.1387, 0.1299, 0.1309, 0.1494,\n",
       "                      0.1562, 0.1543, 0.1523, 0.1016, 0.0166, 0.1455, 0.1660, 0.1455, 0.1494,\n",
       "                      0.1416, 0.1631, 0.1611, 0.1396, 0.1436, 0.1279, 0.1631, 0.1426, 0.1641,\n",
       "                      0.1582, 0.1436, 0.1357, 0.1455, 0.1699, 0.1533, 0.1553, 0.1504, 0.1504,\n",
       "                      0.1396, 0.0830, 0.1553, 0.1631, 0.1406, 0.1621, 0.1367, 0.1553, 0.1318,\n",
       "                      0.1260, 0.1406, 0.1445, 0.1816, 0.1504, 0.1602, 0.1455, 0.1533, 0.1123,\n",
       "                      0.1621, 0.1318, 0.1387, 0.1377, 0.1504, 0.1367, 0.1689, 0.1387, 0.0503,\n",
       "                      0.1631, 0.1514, 0.1572, 0.0486, 0.1484, 0.1572, 0.1660, 0.1426, 0.1328,\n",
       "                      0.1436, 0.1157, 0.1582, 0.1504, 0.1572, 0.1533, 0.1592, 0.1387, 0.1543,\n",
       "                      0.1562, 0.1533, 0.1406, 0.1367, 0.1099, 0.1699, 0.1533, 0.1514, 0.1377,\n",
       "                      0.1465, 0.1592, 0.1602, 0.1484, 0.1572, 0.1641, 0.1631, 0.1377, 0.1562,\n",
       "                      0.1436, 0.1709, 0.1729, 0.1504, 0.1523, 0.1426, 0.1504, 0.1602, 0.1445,\n",
       "                      0.1543, 0.1523, 0.1582, 0.1621, 0.1553, 0.1260, 0.1514, 0.1201, 0.1699,\n",
       "                      0.1455, 0.1523, 0.1729])),\n",
       "             ('0.auto_model.encoder.block.8.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.6445,  0.9219, -0.0586,  ..., -0.6680, -0.5430,  0.0889],\n",
       "                      [-0.4023, -0.4824, -0.0217,  ...,  0.9141, -0.5195,  0.1143],\n",
       "                      [ 0.5547, -0.0359,  0.1060,  ..., -0.6680, -0.2812,  0.5938],\n",
       "                      ...,\n",
       "                      [ 0.2695, -0.1562,  0.2598,  ..., -0.8906, -0.2637, -0.3750],\n",
       "                      [ 0.1553, -0.1807,  0.3418,  ..., -0.8281,  0.8633,  0.2197],\n",
       "                      [-0.9023, -1.2188,  0.3496,  ..., -0.5664,  0.2256, -0.4746]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.0894,  0.1748, -0.0674,  ...,  0.0261, -0.3125,  0.1270],\n",
       "                      [-0.0728, -0.0391, -0.1553,  ...,  0.0034, -0.2490, -0.2617],\n",
       "                      [ 0.0684,  0.2754, -0.0425,  ..., -0.1689, -0.2832, -0.3359],\n",
       "                      ...,\n",
       "                      [ 0.0233,  0.4922,  0.1816,  ..., -0.3594,  0.5039, -0.5156],\n",
       "                      [-0.0581,  0.7031, -0.1934,  ..., -0.1045,  0.3125, -0.2676],\n",
       "                      [ 0.0518, -0.4648, -0.1196,  ..., -0.3887, -0.4219, -0.3691]])),\n",
       "             ('0.auto_model.encoder.block.8.layer.1.layer_norm.weight',\n",
       "              tensor([1.2891, 1.1406, 1.1406, 1.1641, 1.1875, 1.2188, 0.1748, 1.2969, 2.0781,\n",
       "                      1.2734, 1.1406, 1.2656, 1.2031, 1.3203, 1.3281, 1.1562, 1.3594, 1.1484,\n",
       "                      1.0547, 1.2266, 1.3125, 1.2344, 2.2656, 1.2031, 1.3281, 1.2656, 1.1484,\n",
       "                      1.2422, 1.2812, 1.3203, 1.2031, 1.1328, 1.2578, 1.2891, 1.2500, 1.1406,\n",
       "                      1.4141, 1.3672, 1.2031, 1.9453, 1.2188, 1.3906, 1.1016, 1.2109, 3.0938,\n",
       "                      1.2422, 1.3594, 1.2266, 1.2109, 0.5781, 1.3125, 1.3516, 1.4844, 1.2891,\n",
       "                      1.2578, 1.2578, 2.2188, 1.2578, 1.2500, 1.3438, 1.2891, 1.2344, 1.3047,\n",
       "                      1.0781, 1.2969, 1.3203, 1.2891, 1.1719, 1.2891, 1.2109, 1.2812, 1.2500,\n",
       "                      1.2109, 1.2656, 1.2266, 1.3125, 1.3828, 1.2578, 1.3438, 1.3438, 1.1406,\n",
       "                      1.0391, 1.2812, 1.1641, 1.1562, 1.3594, 1.1406, 1.1875, 1.2422, 1.2266,\n",
       "                      1.2656, 1.3125, 1.1328, 1.3438, 1.2656, 1.2266, 1.3203, 1.3984, 1.1719,\n",
       "                      1.2500, 1.2266, 1.1953, 1.0312, 1.2812, 1.2500, 1.5703, 1.2812, 1.3281,\n",
       "                      1.1875, 1.9609, 1.2031, 1.1953, 1.1562, 1.3984, 1.2344, 1.2500, 1.1250,\n",
       "                      1.1797, 1.3594, 1.2031, 1.2734, 1.2578, 1.4141, 1.2188, 1.1094, 1.3359,\n",
       "                      1.1562, 1.3125, 1.1172, 1.2109, 1.6016, 1.2344, 1.2266, 1.1719, 1.2188,\n",
       "                      1.1094, 1.4062, 1.2969, 1.2188, 1.2344, 1.1797, 1.2422, 1.1641, 1.2344,\n",
       "                      1.1797, 1.4141, 1.3359, 1.1719, 1.3047, 1.2812, 1.2734, 1.2891, 1.1328,\n",
       "                      1.2578, 1.3359, 1.2656, 1.2734, 1.4375, 1.1953, 1.2188, 1.3281, 1.2344,\n",
       "                      1.1406, 3.7344, 1.2578, 1.1797, 1.2422, 1.1953, 1.0078, 1.2344, 1.1875,\n",
       "                      1.2812, 1.2500, 1.3438, 1.2891, 1.3281, 1.2578, 0.4512, 1.1484, 0.3516,\n",
       "                      1.3203, 1.3828, 1.1875, 1.3906, 1.2812, 1.1797, 1.2344, 1.2578, 1.2734,\n",
       "                      1.2734, 1.2500, 1.3125, 1.4062, 1.1562, 1.3203, 0.4824, 1.3125, 1.2969,\n",
       "                      1.3516, 1.2344, 1.1875, 1.2344, 1.2266, 1.2266, 1.1719, 1.1484, 1.2812,\n",
       "                      1.1094, 1.2031, 1.2812, 1.2891, 1.1719, 1.3125, 1.2266, 1.1875, 1.0703,\n",
       "                      1.1406, 1.3203, 1.3203, 1.8438, 1.2031, 1.3438, 1.1562, 1.1172, 1.2812,\n",
       "                      1.2188, 1.1875, 1.1250, 1.2109, 1.2422, 1.2734, 1.3516, 0.9648, 1.2891,\n",
       "                      1.3047, 1.1953, 1.0859, 1.1953, 1.2578, 1.0938, 1.4297, 1.2969, 1.2109,\n",
       "                      1.2578, 1.1406, 1.2344, 1.1719, 1.2734, 1.2031, 1.2969, 3.0625, 1.2500,\n",
       "                      1.1719, 1.3516, 1.2266, 1.3594, 1.3516, 1.0625, 0.7617, 1.2031, 1.2109,\n",
       "                      0.8164, 1.2266, 1.2578, 1.1484, 1.1797, 1.2656, 1.2031, 1.2734, 1.2109,\n",
       "                      1.0312, 1.1562, 1.5000, 1.1797, 1.2109, 1.2734, 1.2969, 1.1094, 1.2188,\n",
       "                      1.1953, 1.2500, 1.3984, 1.3281, 1.3125, 1.3203, 1.1875, 1.1797, 1.3594,\n",
       "                      1.2500, 1.9688, 1.2344, 1.3359, 1.2734, 1.2422, 1.3047, 1.3672, 1.2266,\n",
       "                      1.2188, 1.2969, 1.2969, 1.2578, 1.2734, 1.1016, 1.2109, 1.2656, 1.1875,\n",
       "                      1.2500, 1.2656, 1.1953, 1.3750, 1.2344, 1.3281, 1.3438, 1.1016, 1.3047,\n",
       "                      1.1562, 1.1797, 1.1641, 1.4844, 1.1562, 1.3047, 1.2109, 1.2109, 1.1406,\n",
       "                      1.3281, 1.2969, 1.2969, 1.3281, 1.0625, 1.2109, 1.2266, 1.3203, 1.1797,\n",
       "                      1.2422, 1.3359, 1.2422, 1.1953, 1.2031, 1.1641, 1.4453, 1.2812, 1.3984,\n",
       "                      1.2422, 1.3047, 1.2109, 1.3594, 1.1328, 1.2422, 1.1484, 2.0938, 1.2969,\n",
       "                      1.2188, 1.3906, 1.3359, 1.1719, 1.1875, 1.1875, 1.1797, 1.3125, 1.2578,\n",
       "                      1.1875, 1.2891, 1.1328, 1.1641, 1.1641, 1.3047, 0.7461, 1.3672, 1.2109,\n",
       "                      1.2344, 1.3281, 1.2344, 0.3906, 1.2500, 1.2500, 1.3125, 1.1484, 1.1172,\n",
       "                      1.0625, 1.3750, 1.2422, 1.3438, 1.1797, 1.2969, 1.3047, 1.1406, 1.3203,\n",
       "                      0.9180, 1.2344, 1.2812, 1.3047, 2.7656, 1.2734, 1.1797, 1.2109, 1.2891,\n",
       "                      1.3203, 1.1953, 1.1797, 1.1875, 1.1016, 0.5156, 1.1797, 1.1406, 1.2422,\n",
       "                      1.3516, 1.3672, 1.3516, 1.1406, 1.2344, 1.2969, 1.2344, 1.2969, 1.3828,\n",
       "                      1.2812, 1.1406, 1.2344, 1.1484, 0.6641, 1.5000, 1.4609, 1.3750, 1.3594,\n",
       "                      0.4395, 1.2578, 1.1875, 1.2031, 1.3281, 1.2344, 1.2656, 1.3438, 1.2812,\n",
       "                      1.1328, 1.2188, 1.3203, 1.2656, 1.2891, 1.1953, 1.2266, 1.2109, 1.3672,\n",
       "                      1.0938, 1.4219, 1.2188, 1.1953, 1.1484, 1.2188, 1.2812, 1.2344, 1.2812,\n",
       "                      1.2422, 1.3125, 1.3281, 1.1406, 1.4688, 1.2500, 1.3125, 1.1797, 1.3438,\n",
       "                      1.2500, 1.2656, 1.3750, 1.2500, 1.2734, 1.1484, 1.2109, 1.1641, 1.2188,\n",
       "                      1.1797, 1.2734, 1.2500, 1.1562, 0.9570, 1.2109, 1.3906, 1.2812, 1.1875,\n",
       "                      1.2969, 1.0312, 1.2188, 1.3047, 1.9375, 1.2266, 1.2969, 1.2578, 1.4375,\n",
       "                      1.2109, 1.3203, 1.1328, 1.2969, 1.1641, 1.3750, 1.6016, 1.3906, 1.3125,\n",
       "                      1.2031, 1.2578, 1.1953, 1.2969, 1.2578, 1.2578, 1.2266, 1.2031, 2.0938,\n",
       "                      1.2734, 1.2500, 1.1250, 1.1719, 1.2422, 1.2422, 1.2188, 1.3906, 1.9922,\n",
       "                      1.2266, 1.2656, 1.3047, 1.1719, 1.2266, 1.1484, 1.1719, 1.1875, 1.2578,\n",
       "                      1.1719, 1.2344, 1.1094, 2.0156, 1.1172, 1.2188, 1.3438, 1.3828, 1.2109,\n",
       "                      1.2656, 1.2656, 1.2109, 1.3672, 1.2578, 1.1484, 1.1641, 1.2109, 1.1875,\n",
       "                      1.2188, 1.3203, 1.2031, 1.1797, 1.2266, 1.1953, 1.2891, 1.1875, 1.2266,\n",
       "                      0.3809, 1.2344, 1.2578, 1.3047, 1.3594, 1.1719, 1.2031, 1.2812, 1.3125,\n",
       "                      1.2500, 1.0625, 1.3438, 1.2109, 1.2656, 1.3359, 1.2188, 1.1484, 0.3672,\n",
       "                      1.1875, 1.1562, 1.2266, 1.1719, 0.5742, 1.2266, 1.3203, 1.1797, 1.2188,\n",
       "                      1.3125, 1.2344, 1.2266, 1.1406, 1.3828, 1.3594, 1.1641, 1.2891, 1.2344,\n",
       "                      1.3047, 1.2500, 1.1875, 1.2344, 1.2266, 1.3594, 1.2109, 1.1641, 1.2266,\n",
       "                      1.1953, 1.3203, 1.1641, 1.3359, 1.2109, 1.1953, 1.2656, 1.3125, 1.2734,\n",
       "                      0.8945, 1.2031, 1.2266, 1.1484, 1.1250, 1.1719, 1.2344, 1.3359, 1.3438,\n",
       "                      1.3438, 1.1016, 1.2656, 1.2969, 1.2344, 1.1875, 1.2734, 1.0312, 1.2578,\n",
       "                      1.1797, 1.2656, 1.1953, 1.0938, 1.1562, 1.1719, 1.1953, 1.2812, 1.1641,\n",
       "                      1.2031, 1.2578, 1.2344, 1.2969, 1.3984, 0.7461, 1.1719, 1.2734, 1.3672,\n",
       "                      1.2422, 1.1484, 1.2422, 1.0781, 1.3672, 1.2109, 1.0781, 1.2266, 1.2188,\n",
       "                      1.2109, 1.3672, 1.2500, 1.2500, 1.2422, 1.3359, 1.1016, 1.2031, 1.2344,\n",
       "                      1.2422, 1.1484, 1.1641, 0.7227, 0.4297, 1.1953, 1.3125, 1.1328, 1.2188,\n",
       "                      1.1094, 1.2734, 1.1719, 1.2109, 1.2656, 1.5859, 1.2500, 1.3594, 1.1016,\n",
       "                      1.5234, 1.2734, 1.2188, 1.2734, 1.3750, 1.2109, 1.2344, 1.1250, 1.1875,\n",
       "                      1.2734, 1.6484, 1.2734, 1.3359, 1.3047, 1.1953, 1.2891, 1.1719, 1.2656,\n",
       "                      1.2500, 1.3125, 1.1641, 1.1484, 1.2109, 1.3281, 1.2422, 1.3359, 1.1641,\n",
       "                      1.2500, 1.2812, 1.2969, 1.2969, 1.4062, 1.4531, 1.1797, 1.2266, 2.1562,\n",
       "                      1.2734, 1.0938, 1.1406, 0.5742, 1.2734, 1.3828, 1.2031, 1.2891, 1.1406,\n",
       "                      1.2734, 1.2266, 1.3594, 1.1953, 1.1953, 1.1641, 1.0625, 1.2109, 1.2422,\n",
       "                      1.1797, 1.1328, 1.1875, 1.2734, 1.0938, 1.2188, 1.1250, 1.3203, 1.1875,\n",
       "                      1.3203, 1.1094, 1.2578, 1.1719, 1.1328, 1.2578, 1.1797, 1.2422, 1.0859,\n",
       "                      1.2656, 1.3281, 1.3516, 1.2344, 1.3438, 1.4609, 1.3125, 1.1875, 1.3516,\n",
       "                      1.2344, 1.1953, 1.1641, 1.2031, 1.1562, 0.9688, 1.2500, 1.2578, 1.2266,\n",
       "                      1.1094, 1.2031, 1.1875])),\n",
       "             ('0.auto_model.encoder.block.9.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0378, -0.0747,  0.0332,  ..., -0.0278, -0.0118, -0.0126],\n",
       "                      [-0.0354, -0.0040,  0.0025,  ..., -0.0219, -0.0439,  0.0028],\n",
       "                      [ 0.0444,  0.0236,  0.0850,  ...,  0.0354,  0.0542, -0.0053],\n",
       "                      ...,\n",
       "                      [-0.0056,  0.0168, -0.0150,  ..., -0.0096,  0.0294,  0.0062],\n",
       "                      [ 0.0132,  0.0037, -0.0225,  ...,  0.0067,  0.0143,  0.0003],\n",
       "                      [-0.0083, -0.0028,  0.0242,  ...,  0.0035, -0.0112,  0.0226]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.4062, -0.4336,  0.4180,  ..., -0.0496, -0.0762, -0.0398],\n",
       "                      [-0.2656, -0.0574, -0.1436,  ..., -0.0408,  0.0801, -0.1030],\n",
       "                      [-0.0674, -0.1514,  0.4355,  ..., -0.2559,  0.2695, -0.2100],\n",
       "                      ...,\n",
       "                      [ 0.3379, -0.4414, -0.1621,  ...,  0.1196,  0.1719, -0.2500],\n",
       "                      [ 0.1436, -0.0513, -0.0854,  ..., -0.4668, -0.0098, -0.1035],\n",
       "                      [ 0.1021, -0.2852,  0.1123,  ..., -0.0620, -0.0134,  0.0469]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 1.2344, -0.9102,  0.0776,  ..., -0.3535, -0.3203, -0.9766],\n",
       "                      [-0.7461,  0.1875, -0.5703,  ..., -0.8359, -0.0598,  0.0664],\n",
       "                      [-0.7070,  0.3652, -0.0728,  ..., -0.5625,  0.5898, -0.8750],\n",
       "                      ...,\n",
       "                      [-0.4043, -0.8242,  1.7188,  ...,  0.3906, -0.4473, -0.5039],\n",
       "                      [ 0.2363, -0.1924,  0.4590,  ...,  0.3340, -0.2832, -0.5703],\n",
       "                      [ 0.2451,  0.1338,  0.1992,  ...,  2.7344, -1.7969, -0.1089]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.5859,  0.6641,  0.2383,  ...,  2.4844, -0.5703, -0.1475],\n",
       "                      [ 1.4219,  0.1221,  1.0078,  ..., -1.5469,  1.3359,  1.4219],\n",
       "                      [-0.2383, -0.4062, -0.3262,  ..., -2.2344,  0.3535,  0.9531],\n",
       "                      ...,\n",
       "                      [ 0.5195,  1.2422, -0.6055,  ..., -1.5078, -0.9844, -0.5859],\n",
       "                      [ 0.6016, -0.2363, -0.4277,  ...,  0.9414,  0.7969,  1.3828],\n",
       "                      [ 0.9492, -0.0254,  1.2656,  ...,  0.8867, -1.7266,  0.4355]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1582,  0.1504,  0.1543,  0.1543,  0.1533,  0.1592,  0.0352,  0.1670,\n",
       "                       0.0913,  0.1523,  0.1484,  0.1787,  0.1196,  0.1436,  0.1582,  0.1592,\n",
       "                       0.1582,  0.1445,  0.1523,  0.1592,  0.1436,  0.1611,  0.0549,  0.1533,\n",
       "                       0.1826,  0.1660,  0.1660,  0.1367,  0.1572,  0.1572,  0.1602,  0.1621,\n",
       "                       0.1641,  0.1611,  0.1582,  0.1797,  0.1289,  0.1641,  0.1699,  0.1299,\n",
       "                       0.1328,  0.1387,  0.1523,  0.1621,  0.0894,  0.1650,  0.1396,  0.1523,\n",
       "                       0.1592,  0.0344,  0.1660,  0.1777,  0.1289,  0.1846, -0.0427,  0.1455,\n",
       "                       0.0962,  0.1416,  0.1807,  0.1523,  0.1729,  0.1768,  0.1650,  0.1621,\n",
       "                       0.1709,  0.0447,  0.1504,  0.1631,  0.1484,  0.1660,  0.1631,  0.1250,\n",
       "                       0.1514,  0.1631,  0.1670,  0.1709,  0.1201,  0.1611,  0.1367,  0.1738,\n",
       "                       0.1602,  0.1729,  0.1709,  0.1719,  0.1689,  0.1816,  0.1436,  0.1562,\n",
       "                       0.1602,  0.1553,  0.1680,  0.1484,  0.1504,  0.1328,  0.1680,  0.1670,\n",
       "                       0.1504,  0.1641,  0.1602,  0.1494,  0.1484,  0.1699,  0.1133,  0.1504,\n",
       "                       0.1494,  0.1260,  0.1641,  0.1670,  0.1621,  0.0972,  0.1621,  0.1543,\n",
       "                       0.1582,  0.1348,  0.1592,  0.1787,  0.1592,  0.1484,  0.1279,  0.1582,\n",
       "                       0.1680,  0.1719,  0.1484,  0.1475,  0.1240,  0.1504,  0.1797,  0.1514,\n",
       "                       0.1455,  0.1562,  0.1128,  0.1748,  0.1484,  0.1475,  0.1533,  0.1270,\n",
       "                       0.1650,  0.1553,  0.1631,  0.1582,  0.1553,  0.1758,  0.1260,  0.1621,\n",
       "                       0.1484,  0.1611,  0.1553,  0.1650,  0.1650,  0.1562,  0.1631,  0.1572,\n",
       "                       0.1436,  0.1660,  0.1680,  0.1650,  0.1602,  0.1455,  0.1504,  0.1484,\n",
       "                       0.1631,  0.1445,  0.1211, -0.0713,  0.1494,  0.1465,  0.1543,  0.1445,\n",
       "                       0.1572,  0.1416,  0.1621,  0.1855,  0.1797,  0.1172,  0.1611,  0.1621,\n",
       "                       0.1611,  0.0620,  0.1226,  0.0248,  0.1562,  0.1523,  0.1641,  0.1602,\n",
       "                       0.1670,  0.1699,  0.1357,  0.1357,  0.1670,  0.1582,  0.1553,  0.1670,\n",
       "                       0.1621,  0.1572,  0.1494,  0.0603,  0.1738,  0.1660,  0.1611,  0.1318,\n",
       "                       0.1670,  0.1582,  0.1484,  0.1523,  0.1680,  0.1387,  0.1729,  0.1631,\n",
       "                       0.1465,  0.1719,  0.1602,  0.1592,  0.1543,  0.1514,  0.1484,  0.1611,\n",
       "                       0.1416,  0.1699,  0.1465,  0.0991,  0.1167,  0.1709,  0.1533,  0.1768,\n",
       "                       0.1270,  0.1553,  0.1533,  0.1445,  0.1758,  0.1875,  0.1445,  0.1523,\n",
       "                       0.1426,  0.1621,  0.1689,  0.1279,  0.1504,  0.1660,  0.1494,  0.1177,\n",
       "                       0.1504,  0.1787,  0.1865,  0.1611,  0.1631,  0.1611,  0.1572,  0.1660,\n",
       "                       0.1553,  0.1582,  0.1006,  0.1572,  0.1611,  0.1621,  0.1562,  0.1670,\n",
       "                       0.1592,  0.1309,  0.0649,  0.1426,  0.1455,  0.1064,  0.1572,  0.1807,\n",
       "                       0.1465,  0.1582,  0.1689,  0.1582,  0.1436,  0.1348,  0.0610,  0.1738,\n",
       "                       0.1245,  0.0215,  0.1670,  0.1592,  0.1650,  0.1338,  0.1523,  0.1699,\n",
       "                       0.1611,  0.1592,  0.1523,  0.1641,  0.1602,  0.1172,  0.1504,  0.1621,\n",
       "                       0.1494,  0.1182,  0.1582,  0.1768,  0.1602,  0.1709,  0.1543,  0.1641,\n",
       "                       0.1572,  0.1689,  0.1621,  0.1660,  0.1514,  0.1406,  0.1572,  0.1553,\n",
       "                       0.1758,  0.1445,  0.1621,  0.1729,  0.1709,  0.1572,  0.1602,  0.1494,\n",
       "                       0.1602,  0.1201,  0.1514,  0.1543,  0.1328,  0.1416,  0.1484,  0.1602,\n",
       "                       0.1396,  0.1475,  0.1699,  0.1533,  0.1602,  0.1670,  0.1523,  0.1758,\n",
       "                       0.1729,  0.1494,  0.1680,  0.1182,  0.1572,  0.1650,  0.1680,  0.1445,\n",
       "                       0.1465,  0.1748,  0.1650,  0.1592,  0.1523,  0.1699,  0.1680,  0.1514,\n",
       "                       0.1689,  0.1621,  0.1348,  0.1523,  0.1260,  0.1104,  0.1494,  0.1699,\n",
       "                       0.1631,  0.1582,  0.1504,  0.1699,  0.1689,  0.1523,  0.1797,  0.1191,\n",
       "                       0.1650,  0.1426,  0.1426,  0.1396,  0.1543,  0.1543,  0.0693,  0.1484,\n",
       "                       0.1572,  0.1514,  0.1562,  0.1777,  0.0703,  0.1465,  0.1650,  0.1689,\n",
       "                       0.1177,  0.1680,  0.1514,  0.1396,  0.1445,  0.1621,  0.1514,  0.1572,\n",
       "                       0.1660,  0.0615,  0.1621,  0.0996,  0.1504,  0.1621,  0.1445,  0.0674,\n",
       "                       0.1504,  0.1650,  0.1621,  0.1523,  0.1650,  0.1621,  0.1641,  0.1797,\n",
       "                       0.1689,  0.0320,  0.1602,  0.1611,  0.1445,  0.1660,  0.1562,  0.1553,\n",
       "                       0.1445,  0.1660,  0.1445,  0.1729,  0.1504,  0.1582,  0.1592,  0.1562,\n",
       "                       0.1484,  0.1621,  0.0669,  0.1494,  0.1367,  0.1660,  0.1572,  0.0532,\n",
       "                       0.1699,  0.1709,  0.1650,  0.1641,  0.1523,  0.1650,  0.1611,  0.1699,\n",
       "                       0.1138,  0.1543,  0.1377,  0.1748,  0.1797,  0.1582,  0.1523,  0.1582,\n",
       "                       0.1670,  0.1289,  0.1455,  0.1416,  0.1475,  0.1768,  0.1445,  0.1738,\n",
       "                       0.1475,  0.1738,  0.1484,  0.1611,  0.1631,  0.1572,  0.1660,  0.1670,\n",
       "                       0.1611,  0.1494,  0.1484,  0.1475,  0.1631,  0.1572,  0.1650,  0.1699,\n",
       "                       0.1699,  0.1719,  0.1240,  0.1387,  0.1533,  0.1641,  0.1553,  0.1553,\n",
       "                       0.1367,  0.1523,  0.1650,  0.1689,  0.1709,  0.1465,  0.0557,  0.1670,\n",
       "                       0.1494,  0.0977,  0.1465,  0.1650,  0.1729,  0.1445,  0.1504,  0.1416,\n",
       "                       0.1592,  0.1855,  0.1514,  0.1553,  0.0703,  0.1592,  0.1494,  0.1592,\n",
       "                       0.1670,  0.1592,  0.1475,  0.1592,  0.1641,  0.1494,  0.1416,  0.0408,\n",
       "                       0.1602,  0.1611,  0.1406,  0.1533,  0.1504,  0.1572,  0.1660,  0.1631,\n",
       "                       0.1030,  0.1729,  0.1572,  0.1572,  0.1377,  0.1494,  0.1367,  0.1621,\n",
       "                       0.1631,  0.1523,  0.1514,  0.1465,  0.1641,  0.1069,  0.1602,  0.1309,\n",
       "                       0.1777,  0.1611,  0.1465,  0.1641,  0.1504,  0.1670,  0.1445,  0.1621,\n",
       "                       0.1719,  0.1543,  0.1465,  0.1582,  0.1592,  0.1641,  0.1602,  0.1543,\n",
       "                       0.1475,  0.1680,  0.1650,  0.1650,  0.1689,  0.0535,  0.1484,  0.1465,\n",
       "                       0.1631,  0.2002,  0.1631,  0.1738,  0.1533,  0.1631,  0.1514,  0.1226,\n",
       "                       0.1533,  0.1523,  0.1660,  0.1660,  0.1709,  0.1611,  0.0258,  0.1709,\n",
       "                       0.1641,  0.1611,  0.1680,  0.0938,  0.1592,  0.1826,  0.1533,  0.1484,\n",
       "                       0.1680,  0.1514,  0.1680,  0.1289,  0.1621,  0.1924,  0.1689,  0.1582,\n",
       "                       0.1475,  0.1592,  0.1484,  0.1406,  0.1514,  0.1699,  0.1572,  0.1553,\n",
       "                       0.1709,  0.1719,  0.1602,  0.1621,  0.1494,  0.1455,  0.1475,  0.1758,\n",
       "                       0.1484,  0.1543,  0.1641,  0.1108,  0.1592,  0.1680,  0.1680,  0.1514,\n",
       "                       0.1416,  0.1543,  0.1562,  0.1426,  0.1523,  0.1611,  0.1504,  0.1533,\n",
       "                       0.1484,  0.1807,  0.1660,  0.1514,  0.1719,  0.1465,  0.1475,  0.1562,\n",
       "                       0.1602,  0.1494,  0.1504,  0.1318,  0.1494,  0.0991,  0.1504,  0.1719,\n",
       "                       0.1572,  0.1504,  0.1826,  0.0957,  0.1475,  0.1514,  0.1807,  0.1592,\n",
       "                       0.1523,  0.1670,  0.1514,  0.1660,  0.1455,  0.1592,  0.1514,  0.1650,\n",
       "                       0.1396,  0.1602,  0.1484,  0.1602,  0.1465,  0.1445,  0.1367,  0.1543,\n",
       "                       0.1680,  0.1553,  0.1621,  0.1533,  0.1001,  0.0250,  0.1543,  0.1689,\n",
       "                       0.1504,  0.1309,  0.1426,  0.1660,  0.1670,  0.1816,  0.1611,  0.1221,\n",
       "                       0.1533,  0.1533,  0.1670,  0.1621,  0.1533,  0.1465,  0.1377,  0.1611,\n",
       "                       0.1699,  0.1592,  0.1748,  0.1631,  0.1602,  0.0845,  0.1406,  0.1670,\n",
       "                       0.1631,  0.1543,  0.1602,  0.1748,  0.1235,  0.1426,  0.1660,  0.1729,\n",
       "                       0.1689,  0.1543,  0.1553,  0.1475,  0.1729,  0.1445,  0.1611,  0.1338,\n",
       "                       0.1621,  0.1543,  0.1777,  0.1436,  0.1660,  0.1650,  0.0728,  0.1670,\n",
       "                       0.1504,  0.1621,  0.0649,  0.1465,  0.1514,  0.1709,  0.1592,  0.1367,\n",
       "                       0.1602,  0.1279,  0.1572,  0.1553,  0.1533,  0.1602,  0.1748,  0.1348,\n",
       "                       0.1738,  0.1699,  0.1406,  0.1484,  0.1660,  0.1367,  0.1582,  0.1689,\n",
       "                       0.1758,  0.1562,  0.1611,  0.1660,  0.1768,  0.1436,  0.1357,  0.1572,\n",
       "                       0.1709,  0.1504,  0.1504,  0.1709,  0.1729,  0.1592,  0.1445,  0.1670,\n",
       "                       0.1504,  0.1484,  0.1758,  0.1533,  0.1602,  0.1553,  0.1680,  0.1729,\n",
       "                       0.1621,  0.1533,  0.1680,  0.1367,  0.1689,  0.1562,  0.1484,  0.1680])),\n",
       "             ('0.auto_model.encoder.block.9.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.1191,  0.2695,  0.6328,  ..., -0.0684, -0.9023,  0.0659],\n",
       "                      [ 1.0938, -0.1494, -0.7969,  ...,  0.7930, -0.1357,  1.7656],\n",
       "                      [ 1.0781,  0.1260, -0.0884,  ...,  0.2500,  0.1611, -0.5195],\n",
       "                      ...,\n",
       "                      [ 2.1719, -0.4043, -0.1426,  ..., -1.0938, -0.0356, -0.0952],\n",
       "                      [ 0.0115, -0.7656,  0.0859,  ..., -1.4922, -0.7773,  1.0703],\n",
       "                      [-0.1709, -0.1855, -0.1055,  ..., -0.6211,  0.1855, -0.3867]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.1387,  0.5195, -0.5039,  ...,  0.4512, -0.0408,  0.1846],\n",
       "                      [-0.9258, -0.3965, -0.0019,  ...,  0.5273,  0.4648, -0.2285],\n",
       "                      [-0.1738,  0.0630, -0.0576,  ...,  0.5312, -0.3047,  0.1006],\n",
       "                      ...,\n",
       "                      [ 0.2295,  0.2812, -0.6602,  ...,  0.2891, -0.2168,  0.3145],\n",
       "                      [ 0.2598, -0.1143, -0.3320,  ...,  0.2891, -0.3789,  0.0835],\n",
       "                      [ 0.0737, -0.1406,  0.3301,  ...,  0.3438,  0.4551,  0.0444]])),\n",
       "             ('0.auto_model.encoder.block.9.layer.1.layer_norm.weight',\n",
       "              tensor([1.5078, 1.3359, 1.3828, 1.3672, 1.5781, 1.3750, 0.1846, 1.4297, 2.0000,\n",
       "                      1.3516, 1.2812, 1.3359, 1.4453, 1.3828, 1.4531, 1.3672, 1.3906, 1.2891,\n",
       "                      1.0625, 1.3906, 1.5547, 1.3672, 3.0469, 1.4219, 1.6250, 1.5000, 1.6094,\n",
       "                      1.4141, 1.5000, 1.3359, 1.3438, 1.2891, 1.4609, 1.5000, 1.2656, 1.4375,\n",
       "                      1.5781, 1.4297, 1.6016, 2.0156, 1.2578, 1.6328, 1.1562, 1.4062, 3.6719,\n",
       "                      1.5312, 1.3750, 1.3281, 1.4297, 0.6523, 1.2578, 1.4922, 1.4688, 1.5703,\n",
       "                      1.5000, 1.1953, 3.0312, 1.4219, 1.5078, 1.4219, 1.5312, 1.6875, 1.5547,\n",
       "                      1.4141, 1.5781, 1.2812, 1.5000, 1.2891, 1.3750, 1.5234, 1.4453, 1.1875,\n",
       "                      1.3516, 1.5781, 1.3594, 1.5625, 1.5312, 1.4766, 1.4297, 1.5000, 1.4531,\n",
       "                      1.3203, 1.3438, 1.4062, 1.5156, 1.5391, 1.3047, 1.4219, 1.5859, 1.4219,\n",
       "                      1.5312, 1.4844, 1.3906, 1.5234, 1.5312, 1.4766, 1.3203, 1.3672, 1.4453,\n",
       "                      1.4531, 1.2578, 1.3984, 1.0000, 1.4766, 1.4922, 1.6484, 1.5000, 1.4844,\n",
       "                      1.4844, 2.1719, 1.4141, 1.3906, 1.3906, 1.4609, 1.5312, 1.5469, 1.4609,\n",
       "                      1.3750, 1.3594, 1.4219, 1.6016, 1.4141, 1.5781, 1.3516, 1.0703, 1.5703,\n",
       "                      1.3750, 1.4922, 1.4375, 1.4609, 1.8906, 1.3516, 1.6641, 1.4766, 1.3984,\n",
       "                      1.2031, 1.5859, 1.4062, 1.3984, 1.2812, 1.4297, 1.4141, 1.3125, 1.4922,\n",
       "                      1.2969, 1.6562, 1.5469, 1.3125, 1.4062, 1.3750, 1.3672, 1.2891, 1.3281,\n",
       "                      1.4219, 1.5000, 1.5156, 1.3828, 1.5234, 1.2969, 1.3203, 1.5312, 1.4609,\n",
       "                      1.3359, 4.5938, 1.3672, 1.2969, 1.3828, 1.3047, 1.2344, 1.5156, 1.2969,\n",
       "                      1.4531, 1.4062, 1.4297, 1.5078, 1.3828, 1.3672, 0.4434, 1.3125, 0.4219,\n",
       "                      1.4688, 1.5391, 1.3984, 1.5703, 1.3750, 1.3750, 1.4766, 1.1875, 1.3828,\n",
       "                      1.3750, 1.4375, 1.5469, 1.4766, 1.3047, 1.4688, 0.5938, 1.4453, 1.3516,\n",
       "                      1.4375, 1.4766, 1.3047, 1.3906, 1.3438, 1.5391, 1.4141, 1.2422, 1.3594,\n",
       "                      1.3984, 1.4531, 1.3984, 1.4688, 1.4922, 1.3516, 1.3438, 1.3750, 1.3203,\n",
       "                      1.4766, 1.3906, 1.3203, 1.7734, 1.3516, 1.3828, 1.4141, 1.3203, 1.4531,\n",
       "                      1.3359, 1.4297, 1.3672, 1.3984, 1.4141, 1.5000, 1.4609, 1.2578, 1.5469,\n",
       "                      1.4688, 1.3672, 1.3281, 1.4531, 1.3281, 1.2344, 1.3828, 1.3516, 1.5781,\n",
       "                      1.4922, 1.4609, 1.4922, 1.3438, 1.5000, 1.4062, 1.4766, 3.4375, 1.5312,\n",
       "                      1.4219, 1.5781, 1.4609, 1.5156, 1.4766, 1.3359, 0.8398, 1.2812, 1.2656,\n",
       "                      1.3125, 1.5000, 1.5078, 1.5469, 1.5781, 1.5781, 1.4062, 1.4219, 1.3906,\n",
       "                      1.2188, 1.2812, 1.4844, 1.1094, 1.4688, 1.4297, 1.4453, 1.2812, 1.3672,\n",
       "                      1.3828, 1.4844, 1.4062, 1.6250, 1.5156, 1.4922, 1.3125, 1.4375, 1.5156,\n",
       "                      1.4531, 2.0781, 1.4844, 1.5469, 1.4219, 1.4688, 1.3594, 1.6328, 1.3672,\n",
       "                      1.5078, 1.4219, 1.5156, 1.4922, 1.4297, 1.1484, 1.4609, 1.5234, 1.4609,\n",
       "                      1.3438, 1.4219, 1.3047, 1.5469, 1.5391, 1.4766, 1.5703, 1.2188, 1.3359,\n",
       "                      1.3984, 1.3984, 1.5078, 1.4219, 1.4297, 1.5859, 1.3359, 1.3438, 1.7031,\n",
       "                      1.4531, 1.6094, 1.3750, 1.5938, 1.3984, 1.3359, 1.4297, 1.2578, 1.3281,\n",
       "                      1.4766, 1.5391, 1.3203, 1.5234, 1.4531, 1.3906, 1.6094, 1.5391, 1.4688,\n",
       "                      1.5156, 1.6328, 1.6328, 1.3359, 1.3047, 1.3516, 1.3672, 2.5469, 1.3516,\n",
       "                      1.5391, 1.6641, 1.5469, 1.3438, 1.4297, 1.3594, 1.3594, 1.5000, 1.3438,\n",
       "                      1.4219, 1.3750, 1.2578, 1.3984, 1.4375, 1.3203, 0.8281, 1.4531, 1.3984,\n",
       "                      1.3828, 1.5547, 1.2500, 0.4219, 1.5156, 1.3359, 1.4375, 1.2969, 1.3438,\n",
       "                      1.2422, 1.3906, 1.4922, 1.3750, 1.2500, 1.4219, 1.3828, 1.1328, 1.4609,\n",
       "                      0.9961, 1.4297, 1.5078, 1.5547, 3.2656, 1.3672, 1.3125, 1.2969, 1.5547,\n",
       "                      1.5469, 1.5781, 1.5078, 1.3828, 1.4141, 0.8789, 1.5859, 1.4453, 1.3594,\n",
       "                      1.6094, 1.3359, 1.4844, 1.4297, 1.4219, 1.4297, 1.4141, 1.5781, 1.4219,\n",
       "                      1.5000, 1.3281, 1.5938, 1.3594, 0.6562, 1.5078, 1.4297, 1.4922, 1.5625,\n",
       "                      0.5430, 1.4297, 1.4844, 1.4141, 1.5859, 1.5234, 1.2500, 1.5234, 1.5312,\n",
       "                      1.1641, 1.4453, 1.4297, 1.4375, 1.4375, 1.4844, 1.2891, 1.2812, 1.3828,\n",
       "                      1.1406, 1.3359, 1.3125, 1.3672, 1.3750, 1.5625, 1.3281, 1.3438, 1.3672,\n",
       "                      1.4219, 1.4297, 1.3281, 1.4297, 1.6641, 1.4688, 1.2578, 1.3594, 1.4219,\n",
       "                      1.4688, 1.4219, 1.5312, 1.4453, 1.4297, 1.4297, 1.6094, 1.3438, 1.3984,\n",
       "                      1.4531, 1.5938, 1.5547, 1.5312, 1.2266, 1.5391, 1.3438, 1.4219, 1.3828,\n",
       "                      1.4141, 0.7344, 1.3203, 1.3125, 2.0938, 1.3672, 1.3438, 1.4453, 1.5547,\n",
       "                      1.5000, 1.6250, 1.4844, 1.3750, 1.4219, 1.5234, 1.9766, 1.5938, 1.5156,\n",
       "                      1.4062, 1.4141, 1.3516, 1.5469, 1.4453, 1.5781, 1.3281, 1.3203, 2.3906,\n",
       "                      1.2422, 1.4531, 1.3594, 1.1641, 1.5391, 1.2969, 1.4453, 1.6016, 2.4531,\n",
       "                      1.5469, 1.4297, 1.4688, 1.2969, 1.3984, 1.1094, 1.3906, 1.5078, 1.4609,\n",
       "                      1.4531, 1.5625, 1.3125, 2.3281, 1.3359, 1.2578, 1.3984, 1.5234, 1.3281,\n",
       "                      1.4766, 1.4688, 1.4062, 1.5078, 1.2578, 1.3828, 1.3594, 1.3281, 1.3516,\n",
       "                      1.3672, 1.3594, 1.4453, 1.2969, 1.4453, 1.4453, 1.5156, 1.4922, 1.5391,\n",
       "                      0.4043, 1.3203, 1.3516, 1.6250, 1.5703, 1.5859, 1.4531, 1.3203, 1.5312,\n",
       "                      1.4375, 1.0312, 1.5469, 1.2812, 1.4062, 1.4375, 1.4531, 1.2266, 0.4551,\n",
       "                      1.3125, 1.3984, 1.5078, 1.4922, 0.7383, 1.4531, 1.3281, 1.4141, 1.4453,\n",
       "                      1.3984, 1.2891, 1.3984, 1.3281, 1.5312, 1.6016, 1.4219, 1.5391, 1.4531,\n",
       "                      1.5000, 1.4609, 1.3906, 1.5234, 1.3672, 1.2812, 1.2109, 1.2344, 1.3594,\n",
       "                      1.3594, 1.5000, 1.3125, 1.5938, 1.3516, 1.5000, 1.4453, 1.5234, 1.3906,\n",
       "                      1.0156, 1.2969, 1.3984, 1.6172, 1.1719, 1.3750, 1.5078, 1.5234, 1.4766,\n",
       "                      1.3594, 1.5312, 1.4844, 1.3359, 1.4922, 1.4062, 1.4609, 1.2969, 1.3906,\n",
       "                      1.6172, 1.3828, 1.3203, 1.4688, 1.4531, 1.3984, 1.3828, 1.3359, 1.1797,\n",
       "                      1.3281, 1.5391, 1.5234, 1.4766, 1.6172, 0.8242, 1.5547, 1.3281, 1.4688,\n",
       "                      1.5078, 1.3359, 1.4375, 1.3906, 1.5312, 1.3984, 1.2031, 1.2422, 1.4844,\n",
       "                      1.4062, 1.5078, 1.4766, 1.6016, 1.5312, 1.5000, 1.3125, 1.3047, 1.4062,\n",
       "                      1.3516, 1.2344, 1.4062, 0.9102, 0.5742, 1.3438, 1.4375, 1.4375, 1.5312,\n",
       "                      1.4453, 1.5391, 1.3125, 1.4219, 1.5000, 1.7891, 1.3438, 1.5703, 1.4531,\n",
       "                      1.5391, 1.5938, 1.5547, 1.4453, 1.5859, 1.4453, 1.4531, 1.3828, 1.4531,\n",
       "                      1.2734, 1.7422, 1.4531, 1.5234, 1.5469, 1.3672, 1.4844, 1.4297, 1.1484,\n",
       "                      1.3594, 1.5625, 1.5078, 1.2578, 1.4453, 1.4688, 1.3203, 1.3906, 1.4688,\n",
       "                      1.3828, 1.3750, 1.4219, 1.4766, 1.3359, 1.5859, 1.2969, 1.5781, 2.5469,\n",
       "                      1.4766, 1.3906, 1.6172, 0.5781, 1.4141, 1.3906, 1.4453, 1.5391, 1.0625,\n",
       "                      1.4141, 1.2812, 1.4844, 1.4688, 1.4844, 1.3672, 1.2188, 1.3203, 1.6250,\n",
       "                      1.4531, 1.3750, 1.5078, 1.5000, 1.2812, 1.3594, 1.3750, 1.4844, 1.4453,\n",
       "                      1.6016, 1.5703, 1.4688, 1.3750, 1.3984, 1.4062, 1.4531, 1.4219, 1.3594,\n",
       "                      1.5234, 1.4688, 1.6641, 1.4688, 1.5000, 1.6406, 1.3750, 1.3203, 1.5391,\n",
       "                      1.4141, 1.5391, 1.3750, 1.3906, 1.1875, 1.4141, 1.3750, 1.4688, 1.4219,\n",
       "                      1.3359, 1.5000, 1.3828])),\n",
       "             ('0.auto_model.encoder.block.10.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0337, -0.0136,  0.0371,  ...,  0.0767,  0.0147,  0.0116],\n",
       "                      [-0.0283, -0.0167, -0.0347,  ...,  0.0479,  0.0459,  0.0194],\n",
       "                      [ 0.0270,  0.0104, -0.0137,  ...,  0.0466,  0.0532,  0.0311],\n",
       "                      ...,\n",
       "                      [-0.0293, -0.0518,  0.0535,  ...,  0.0332, -0.0091,  0.0189],\n",
       "                      [ 0.0072,  0.0820,  0.0071,  ...,  0.0337, -0.0176, -0.0219],\n",
       "                      [ 0.0134, -0.0122,  0.0559,  ..., -0.0300, -0.0344, -0.0167]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.2559,  0.0864,  0.0150,  ..., -0.1338, -0.0437, -0.1807],\n",
       "                      [ 0.0815,  0.0649, -0.0304,  ...,  0.2354, -0.3242, -0.1533],\n",
       "                      [ 0.1582,  0.0366,  0.5352,  ..., -0.4453, -0.0747,  0.0330],\n",
       "                      ...,\n",
       "                      [ 0.1689, -0.4199,  0.1069,  ...,  0.1001, -0.1182, -0.0747],\n",
       "                      [-0.0430, -0.1118, -0.3828,  ...,  0.1934,  0.2598, -0.3340],\n",
       "                      [-0.0028, -0.1660,  0.2812,  ...,  0.2188,  0.2812, -0.1709]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.7070, -0.5117, -0.5469,  ..., -0.9375,  0.9180,  0.3457],\n",
       "                      [-0.1689,  0.6602, -0.3047,  ..., -1.2578,  2.8281,  0.4395],\n",
       "                      [-0.4629, -0.0170,  0.0082,  ..., -0.2500,  0.6758, -0.1455],\n",
       "                      ...,\n",
       "                      [ 0.3418,  1.0938,  0.5000,  ...,  0.9570, -0.0310, -0.1221],\n",
       "                      [-0.3672, -0.8945,  0.3848,  ..., -0.2363,  0.1572,  1.1953],\n",
       "                      [-0.5273,  0.0708, -0.0869,  ...,  0.2373, -1.4453,  0.4512]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.1904, -0.6641,  0.3613,  ..., -0.3027,  1.3203,  0.1270],\n",
       "                      [ 1.5781,  0.4902,  0.1035,  ...,  1.3281,  0.5469, -0.0889],\n",
       "                      [ 1.3594,  0.6367, -1.0391,  ..., -1.8672, -0.4121, -1.7891],\n",
       "                      ...,\n",
       "                      [ 0.4121,  2.3281, -0.2715,  ...,  0.0184, -0.1934, -0.8867],\n",
       "                      [ 0.9570, -1.0547, -1.0703,  ...,  0.3008,  1.3438,  1.4453],\n",
       "                      [ 0.2520, -1.0781, -0.1006,  ...,  0.5391, -0.8086, -2.4062]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.0.layer_norm.weight',\n",
       "              tensor([0.1670, 0.1602, 0.1377, 0.1602, 0.1709, 0.1543, 0.0388, 0.1357, 0.0669,\n",
       "                      0.1523, 0.1494, 0.1641, 0.1216, 0.1328, 0.1670, 0.1572, 0.1650, 0.1582,\n",
       "                      0.1240, 0.1709, 0.1514, 0.1631, 0.0688, 0.1709, 0.1729, 0.1699, 0.1826,\n",
       "                      0.1475, 0.1602, 0.1797, 0.1406, 0.1602, 0.1855, 0.1650, 0.1523, 0.1523,\n",
       "                      0.1045, 0.1719, 0.1855, 0.1084, 0.1328, 0.1348, 0.1196, 0.1230, 0.0771,\n",
       "                      0.1650, 0.1250, 0.1680, 0.1943, 0.0247, 0.1377, 0.1611, 0.1289, 0.1621,\n",
       "                      0.0562, 0.1553, 0.0972, 0.1484, 0.1768, 0.1719, 0.1514, 0.1650, 0.1514,\n",
       "                      0.1729, 0.1836, 0.0322, 0.1514, 0.1602, 0.1396, 0.1621, 0.1641, 0.0942,\n",
       "                      0.1758, 0.1699, 0.1631, 0.1729, 0.1328, 0.1748, 0.1475, 0.1787, 0.1553,\n",
       "                      0.1709, 0.1699, 0.1484, 0.1660, 0.1748, 0.1465, 0.1572, 0.1709, 0.1709,\n",
       "                      0.1826, 0.1602, 0.1738, 0.1338, 0.1641, 0.1582, 0.1670, 0.1553, 0.1650,\n",
       "                      0.1328, 0.1494, 0.1660, 0.1060, 0.1475, 0.1611, 0.0908, 0.1504, 0.1719,\n",
       "                      0.1670, 0.0864, 0.1729, 0.1523, 0.1406, 0.1396, 0.1553, 0.1846, 0.1562,\n",
       "                      0.1650, 0.1079, 0.1670, 0.1406, 0.1768, 0.1338, 0.1855, 0.1113, 0.1592,\n",
       "                      0.1699, 0.1582, 0.1445, 0.1494, 0.0835, 0.1680, 0.1650, 0.1553, 0.1621,\n",
       "                      0.1250, 0.1699, 0.1621, 0.1592, 0.1543, 0.1514, 0.1611, 0.1235, 0.1592,\n",
       "                      0.1504, 0.1436, 0.1621, 0.1631, 0.1797, 0.1807, 0.1689, 0.1709, 0.1387,\n",
       "                      0.1641, 0.1582, 0.1670, 0.1543, 0.1611, 0.1377, 0.1465, 0.1602, 0.1387,\n",
       "                      0.1279, 0.0879, 0.1680, 0.1592, 0.1631, 0.1455, 0.1660, 0.0962, 0.1689,\n",
       "                      0.1621, 0.1621, 0.1060, 0.1846, 0.1611, 0.1611, 0.0491, 0.1455, 0.0179,\n",
       "                      0.1748, 0.1826, 0.1631, 0.1699, 0.1621, 0.1650, 0.1621, 0.1436, 0.1660,\n",
       "                      0.1543, 0.1436, 0.1631, 0.1504, 0.1807, 0.1650, 0.0491, 0.1895, 0.1475,\n",
       "                      0.1494, 0.1582, 0.1592, 0.1436, 0.1631, 0.1553, 0.1592, 0.1357, 0.1562,\n",
       "                      0.1699, 0.1631, 0.1650, 0.1553, 0.1504, 0.1602, 0.1465, 0.1699, 0.1426,\n",
       "                      0.1465, 0.1611, 0.1367, 0.0591, 0.1084, 0.1592, 0.1504, 0.1650, 0.1426,\n",
       "                      0.1582, 0.1504, 0.1230, 0.1729, 0.1680, 0.1475, 0.1602, 0.1543, 0.1572,\n",
       "                      0.1504, 0.1426, 0.1543, 0.1465, 0.1426, 0.0986, 0.1504, 0.1641, 0.1641,\n",
       "                      0.1543, 0.1533, 0.1553, 0.1523, 0.1445, 0.1689, 0.1494, 0.0767, 0.1562,\n",
       "                      0.1738, 0.1641, 0.1709, 0.1729, 0.1680, 0.1494, 0.0703, 0.1543, 0.1348,\n",
       "                      0.1011, 0.1533, 0.1689, 0.1572, 0.1709, 0.1631, 0.1680, 0.1611, 0.1572,\n",
       "                      0.0649, 0.1562, 0.1143, 0.0178, 0.1348, 0.1504, 0.1562, 0.1157, 0.1504,\n",
       "                      0.1602, 0.1455, 0.1553, 0.1602, 0.1523, 0.1758, 0.1221, 0.1709, 0.1738,\n",
       "                      0.1504, 0.0864, 0.1641, 0.1729, 0.1729, 0.1689, 0.1572, 0.1709, 0.1523,\n",
       "                      0.1611, 0.1709, 0.1543, 0.1582, 0.1660, 0.1445, 0.1426, 0.1650, 0.1816,\n",
       "                      0.1641, 0.1650, 0.1816, 0.1611, 0.1592, 0.1562, 0.1719, 0.1299, 0.1709,\n",
       "                      0.1650, 0.1387, 0.1680, 0.1514, 0.1602, 0.1514, 0.1416, 0.1592, 0.1729,\n",
       "                      0.1582, 0.1807, 0.1318, 0.1543, 0.1553, 0.1484, 0.1660, 0.0889, 0.1582,\n",
       "                      0.1533, 0.1611, 0.1699, 0.1680, 0.1514, 0.1709, 0.1475, 0.1797, 0.1670,\n",
       "                      0.1768, 0.1602, 0.1670, 0.1543, 0.1328, 0.1377, 0.1279, 0.0850, 0.1445,\n",
       "                      0.1787, 0.1582, 0.1748, 0.1455, 0.1533, 0.1582, 0.1621, 0.1689, 0.1016,\n",
       "                      0.1660, 0.1299, 0.1196, 0.1484, 0.1797, 0.1416, 0.0566, 0.1377, 0.1572,\n",
       "                      0.1680, 0.1357, 0.1885, 0.0693, 0.1494, 0.1416, 0.1650, 0.1216, 0.1816,\n",
       "                      0.1143, 0.1338, 0.1738, 0.1582, 0.1309, 0.1504, 0.1787, 0.0554, 0.1709,\n",
       "                      0.0825, 0.1738, 0.1523, 0.1582, 0.0654, 0.1367, 0.1641, 0.1562, 0.1445,\n",
       "                      0.1689, 0.1729, 0.1543, 0.1699, 0.1797, 0.0359, 0.1738, 0.1689, 0.1719,\n",
       "                      0.1826, 0.1494, 0.1621, 0.1865, 0.1699, 0.1533, 0.1787, 0.1445, 0.1670,\n",
       "                      0.1836, 0.1768, 0.1338, 0.1719, 0.0571, 0.1436, 0.1235, 0.1611, 0.1592,\n",
       "                      0.0547, 0.1562, 0.1846, 0.1582, 0.1494, 0.1562, 0.1689, 0.1562, 0.1680,\n",
       "                      0.0864, 0.1406, 0.1279, 0.1660, 0.1611, 0.1719, 0.1396, 0.1621, 0.1582,\n",
       "                      0.1270, 0.1592, 0.1289, 0.1816, 0.1641, 0.1475, 0.1709, 0.1562, 0.1768,\n",
       "                      0.1533, 0.1689, 0.1621, 0.1729, 0.1680, 0.1729, 0.1650, 0.1523, 0.1494,\n",
       "                      0.1562, 0.1562, 0.1650, 0.1719, 0.1875, 0.1582, 0.1504, 0.1562, 0.1475,\n",
       "                      0.1582, 0.1719, 0.1533, 0.1660, 0.1484, 0.1797, 0.1650, 0.1904, 0.1602,\n",
       "                      0.1484, 0.0557, 0.1592, 0.1543, 0.0674, 0.1553, 0.1475, 0.1562, 0.1748,\n",
       "                      0.1592, 0.1177, 0.1836, 0.1484, 0.1748, 0.1699, 0.0640, 0.1748, 0.1465,\n",
       "                      0.1680, 0.1592, 0.1582, 0.1572, 0.1602, 0.1592, 0.1523, 0.1504, 0.0317,\n",
       "                      0.1641, 0.1689, 0.1719, 0.1455, 0.1641, 0.1455, 0.1650, 0.1865, 0.0903,\n",
       "                      0.1836, 0.1670, 0.1582, 0.1641, 0.1543, 0.1108, 0.1592, 0.1660, 0.1338,\n",
       "                      0.1660, 0.1475, 0.1709, 0.0757, 0.1396, 0.1118, 0.1562, 0.1602, 0.1377,\n",
       "                      0.1738, 0.1494, 0.1543, 0.1338, 0.1650, 0.1562, 0.1543, 0.1250, 0.1699,\n",
       "                      0.1484, 0.1406, 0.1582, 0.1299, 0.1650, 0.1738, 0.1504, 0.1396, 0.1611,\n",
       "                      0.0613, 0.1309, 0.1719, 0.1621, 0.1846, 0.1719, 0.1738, 0.1396, 0.1816,\n",
       "                      0.1445, 0.1108, 0.1572, 0.1475, 0.1436, 0.1504, 0.1846, 0.1650, 0.0161,\n",
       "                      0.1641, 0.1533, 0.1641, 0.1650, 0.1050, 0.1846, 0.1719, 0.1592, 0.1455,\n",
       "                      0.1650, 0.1562, 0.1484, 0.1221, 0.1514, 0.1650, 0.1621, 0.1768, 0.1533,\n",
       "                      0.1523, 0.1465, 0.1572, 0.1631, 0.1768, 0.1748, 0.1426, 0.1709, 0.1768,\n",
       "                      0.1680, 0.1660, 0.1289, 0.1680, 0.1250, 0.1748, 0.1680, 0.1582, 0.1719,\n",
       "                      0.0972, 0.1592, 0.1689, 0.1562, 0.1465, 0.1494, 0.1641, 0.1719, 0.1523,\n",
       "                      0.1572, 0.1611, 0.1543, 0.1592, 0.1348, 0.1621, 0.1826, 0.1592, 0.1611,\n",
       "                      0.1553, 0.1582, 0.1650, 0.1699, 0.1455, 0.1592, 0.1328, 0.1562, 0.0884,\n",
       "                      0.1484, 0.1602, 0.1582, 0.1465, 0.1592, 0.0928, 0.1729, 0.1523, 0.1260,\n",
       "                      0.1758, 0.1436, 0.1758, 0.1650, 0.1768, 0.1553, 0.1592, 0.1455, 0.1631,\n",
       "                      0.1543, 0.1787, 0.1543, 0.1572, 0.1582, 0.1416, 0.1240, 0.1289, 0.1660,\n",
       "                      0.1475, 0.1406, 0.1455, 0.0850, 0.0337, 0.1641, 0.1709, 0.1602, 0.1484,\n",
       "                      0.1572, 0.1768, 0.1816, 0.1787, 0.1504, 0.1270, 0.1797, 0.1533, 0.1621,\n",
       "                      0.1709, 0.1611, 0.1631, 0.1592, 0.1670, 0.1494, 0.1602, 0.1602, 0.1582,\n",
       "                      0.1650, 0.0708, 0.1631, 0.1660, 0.1738, 0.1641, 0.1562, 0.1768, 0.1069,\n",
       "                      0.1484, 0.1719, 0.1689, 0.1572, 0.1631, 0.1562, 0.1523, 0.1719, 0.1367,\n",
       "                      0.1582, 0.1318, 0.1367, 0.1670, 0.1709, 0.1289, 0.1484, 0.1738, 0.0786,\n",
       "                      0.1670, 0.1572, 0.1670, 0.0483, 0.1416, 0.1396, 0.1650, 0.1553, 0.1182,\n",
       "                      0.1670, 0.1260, 0.1475, 0.1650, 0.1621, 0.1631, 0.1641, 0.1377, 0.1934,\n",
       "                      0.1514, 0.1465, 0.1602, 0.1533, 0.1592, 0.1748, 0.1748, 0.1514, 0.1328,\n",
       "                      0.1680, 0.1562, 0.1621, 0.1309, 0.1504, 0.1660, 0.1670, 0.1406, 0.1562,\n",
       "                      0.1758, 0.1533, 0.1250, 0.1631, 0.1709, 0.1426, 0.1602, 0.1592, 0.1680,\n",
       "                      0.1572, 0.1689, 0.1699, 0.1680, 0.1543, 0.1660, 0.1553, 0.1494, 0.1484,\n",
       "                      0.1436, 0.1387, 0.1504])),\n",
       "             ('0.auto_model.encoder.block.10.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.1777, -0.0055,  0.2715,  ..., -1.2109,  0.3164,  0.0066],\n",
       "                      [-0.3477, -2.1562, -0.0513,  ..., -0.2559, -0.9883, -1.6328],\n",
       "                      [-0.1396,  0.2676, -0.7070,  ...,  1.1094,  0.6250, -0.4609],\n",
       "                      ...,\n",
       "                      [ 0.5508,  0.1943, -1.6875,  ...,  0.6719,  1.1797,  1.6406],\n",
       "                      [-2.2500, -0.0025,  0.1582,  ...,  0.1152,  0.0284, -0.4551],\n",
       "                      [ 0.5859,  0.5547,  0.4707,  ..., -1.5625, -1.7266,  0.6016]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.1826,  0.2041, -0.2393,  ..., -0.1641, -0.0732, -0.1533],\n",
       "                      [ 0.0479,  0.0023,  0.2910,  ...,  0.6055,  0.3477,  0.2461],\n",
       "                      [ 0.2129,  1.0000,  0.6875,  ..., -0.2969,  0.0062,  0.5508],\n",
       "                      ...,\n",
       "                      [ 0.3730,  0.3984, -0.0928,  ...,  0.3262,  0.9023, -0.7891],\n",
       "                      [ 0.4844, -1.1719,  0.2949,  ...,  0.6562, -0.2773, -0.2930],\n",
       "                      [ 0.0098,  0.2051, -0.2578,  ..., -0.8086,  0.3398,  0.0713]])),\n",
       "             ('0.auto_model.encoder.block.10.layer.1.layer_norm.weight',\n",
       "              tensor([1.6406, 1.4844, 1.2500, 1.4219, 1.6250, 1.5391, 0.2812, 1.3906, 1.3672,\n",
       "                      1.6250, 1.2344, 1.3359, 1.4922, 1.3906, 1.7031, 1.4375, 1.3672, 1.3828,\n",
       "                      1.0391, 1.6328, 1.4609, 1.5781, 3.5000, 1.4766, 1.6641, 1.4141, 1.4844,\n",
       "                      1.3828, 1.5078, 1.5859, 1.4375, 1.4219, 1.4766, 1.5234, 1.3516, 1.5000,\n",
       "                      1.4219, 1.3828, 1.6328, 1.7969, 1.3594, 1.4531, 1.0781, 1.5000, 3.1406,\n",
       "                      1.6797, 1.3281, 1.5312, 1.5234, 0.5977, 1.3125, 1.5469, 1.5312, 1.5156,\n",
       "                      1.5156, 1.4062, 3.1875, 1.4922, 1.6094, 1.5156, 1.4922, 1.6406, 1.8047,\n",
       "                      1.6250, 1.8125, 1.1641, 1.6406, 1.3906, 1.3281, 1.5391, 1.4922, 0.9336,\n",
       "                      1.6641, 1.5625, 1.5469, 1.6875, 1.4766, 1.5234, 1.6797, 1.5156, 1.5547,\n",
       "                      1.4844, 1.5391, 1.4297, 1.6641, 1.6250, 1.2891, 1.5156, 1.4297, 1.7656,\n",
       "                      1.5312, 1.6172, 1.6875, 1.8125, 1.8125, 1.3984, 1.5156, 1.4922, 1.6641,\n",
       "                      1.5703, 1.4141, 1.5469, 0.8594, 1.6172, 1.7891, 1.7500, 1.6328, 1.6875,\n",
       "                      1.5547, 2.0625, 1.4375, 1.5000, 1.4375, 1.5625, 1.5938, 1.6875, 1.4219,\n",
       "                      1.6250, 1.1719, 1.6719, 1.6719, 1.6484, 1.6172, 1.6641, 1.0312, 1.6328,\n",
       "                      1.4688, 1.6797, 1.5000, 1.4531, 1.8594, 1.5938, 1.7188, 1.6797, 1.5391,\n",
       "                      0.8945, 1.4531, 1.7578, 1.4297, 1.2891, 1.4766, 1.1797, 1.2031, 1.5547,\n",
       "                      1.1953, 1.5547, 1.6797, 1.4922, 1.5391, 1.4844, 1.6562, 1.3672, 1.3594,\n",
       "                      1.5156, 1.7344, 1.7266, 1.4062, 1.9375, 1.1641, 1.5469, 1.6328, 1.5078,\n",
       "                      1.3438, 4.4688, 1.7344, 1.4219, 1.6094, 1.4688, 1.3281, 1.3594, 1.5547,\n",
       "                      1.6406, 1.7344, 1.4609, 1.6484, 1.6094, 1.4609, 0.5078, 1.3203, 0.4082,\n",
       "                      1.6406, 1.6328, 1.5469, 1.6562, 1.4844, 1.3203, 1.6094, 1.2266, 1.4062,\n",
       "                      1.3516, 1.6641, 1.5938, 1.7344, 1.5625, 1.5625, 0.6914, 1.6172, 1.4766,\n",
       "                      1.5703, 1.4609, 1.4219, 1.4375, 1.5625, 1.6484, 1.4922, 1.3672, 1.4219,\n",
       "                      1.5000, 1.4766, 1.4453, 1.5625, 1.5391, 1.4375, 1.7656, 1.5781, 1.6094,\n",
       "                      1.4844, 1.4375, 1.3828, 1.1875, 1.3359, 1.4219, 1.6016, 1.3984, 1.5469,\n",
       "                      1.4453, 1.3438, 1.5234, 1.6016, 1.7188, 1.5625, 1.3906, 1.7500, 1.7578,\n",
       "                      1.6172, 1.4375, 1.8359, 1.5000, 1.3047, 1.1016, 1.5625, 1.2734, 1.7734,\n",
       "                      1.5703, 1.4766, 1.5234, 1.4688, 1.5625, 1.7500, 1.4609, 2.5469, 1.5781,\n",
       "                      1.6172, 1.7969, 1.7266, 1.5547, 1.7422, 1.2812, 0.8164, 1.4766, 1.4141,\n",
       "                      1.5938, 1.6797, 1.6797, 1.5625, 1.5391, 1.7266, 1.4141, 1.4688, 1.5703,\n",
       "                      1.0938, 1.3047, 1.3672, 0.8867, 1.3516, 1.3750, 1.5938, 1.1016, 1.4375,\n",
       "                      1.3438, 1.5000, 1.2578, 1.7031, 1.5938, 1.5000, 1.2109, 1.5078, 1.7344,\n",
       "                      1.3281, 1.8984, 1.6797, 1.5312, 1.6719, 1.8125, 1.6250, 1.8516, 1.3672,\n",
       "                      1.7266, 1.7266, 1.5000, 1.4375, 1.7734, 1.1875, 1.4375, 1.4531, 1.6953,\n",
       "                      1.5234, 1.5156, 1.4141, 1.4922, 1.5078, 1.5625, 1.8203, 1.4062, 1.4609,\n",
       "                      1.4297, 1.5156, 1.3125, 1.6641, 1.7109, 1.6406, 1.4141, 1.5234, 1.7969,\n",
       "                      1.4922, 1.7578, 1.1562, 1.5547, 1.4219, 1.6406, 1.6797, 1.1094, 1.4531,\n",
       "                      1.5312, 1.6797, 1.5469, 1.8047, 1.5625, 1.5859, 1.6016, 1.6406, 1.3672,\n",
       "                      1.7734, 1.6797, 1.3906, 1.5391, 1.2109, 1.4453, 1.2969, 2.3750, 1.4844,\n",
       "                      1.5859, 1.7422, 1.4375, 1.3672, 1.5547, 1.3828, 1.4219, 1.6328, 1.1172,\n",
       "                      1.5156, 1.4219, 1.1406, 1.6953, 1.6016, 1.4766, 0.9062, 1.5156, 1.4688,\n",
       "                      1.3828, 1.7031, 1.3516, 0.5820, 1.5391, 1.2578, 1.6250, 1.3203, 1.5391,\n",
       "                      1.3281, 1.4922, 1.6094, 1.5156, 1.2344, 1.6328, 1.4219, 0.9062, 1.6172,\n",
       "                      0.8750, 1.8984, 1.6562, 1.5781, 3.6094, 1.3750, 1.4531, 1.3516, 1.6641,\n",
       "                      1.5391, 1.5234, 1.3594, 1.5938, 1.5312, 1.0469, 1.6562, 1.3906, 1.5312,\n",
       "                      1.7109, 1.5156, 1.6562, 1.5859, 1.5312, 1.5859, 1.7812, 1.6016, 1.5859,\n",
       "                      1.8281, 1.2969, 1.8125, 1.5547, 0.9023, 1.6250, 1.4844, 1.5781, 1.7422,\n",
       "                      0.5742, 1.5625, 1.5469, 1.5156, 1.5859, 1.5312, 1.5391, 1.5000, 1.5938,\n",
       "                      0.9570, 1.6406, 1.2500, 1.5703, 1.5000, 1.7969, 1.1875, 1.2422, 1.5078,\n",
       "                      1.3203, 1.3516, 1.1172, 1.7188, 1.4609, 1.7344, 1.4297, 1.3594, 1.6328,\n",
       "                      1.5000, 1.7109, 1.5469, 1.7656, 1.4922, 1.6094, 1.5078, 1.5156, 1.5625,\n",
       "                      1.6875, 1.7422, 1.6094, 1.5469, 1.4609, 1.4766, 1.4922, 1.3828, 1.4297,\n",
       "                      1.5469, 1.7734, 1.5391, 1.6016, 1.2891, 1.5625, 1.5703, 1.6875, 1.7422,\n",
       "                      1.5703, 0.8672, 1.3828, 1.3125, 1.7109, 1.5703, 1.5625, 1.3828, 1.7734,\n",
       "                      1.6406, 1.6172, 1.7969, 1.5469, 1.4766, 1.6328, 2.3125, 1.8203, 1.4922,\n",
       "                      1.5859, 1.4297, 1.3047, 1.5938, 1.5625, 1.6172, 1.4453, 1.5234, 2.2969,\n",
       "                      1.3516, 1.4922, 1.5078, 1.3438, 1.8359, 1.4531, 1.5391, 1.6406, 2.6406,\n",
       "                      1.6797, 1.3984, 1.8438, 1.3594, 1.4219, 1.1719, 1.6719, 1.6875, 1.4844,\n",
       "                      1.5391, 1.5391, 1.3984, 2.0781, 1.3828, 1.1484, 1.5391, 1.4688, 1.6328,\n",
       "                      1.7109, 1.3594, 1.3984, 1.6484, 1.3125, 1.6484, 1.5078, 1.2734, 1.5703,\n",
       "                      1.4219, 1.7344, 1.3984, 1.2734, 1.6328, 1.4453, 1.3594, 1.4453, 1.7422,\n",
       "                      0.5391, 1.3359, 1.4219, 1.7734, 1.6094, 1.5156, 1.4844, 1.3594, 1.6484,\n",
       "                      1.4609, 1.0234, 1.4766, 1.4922, 1.3203, 1.3750, 1.5703, 1.5000, 0.4512,\n",
       "                      1.6250, 1.3828, 1.5859, 1.3828, 0.9375, 1.6172, 1.6406, 1.4766, 1.5625,\n",
       "                      1.6484, 1.3125, 1.6016, 1.2188, 1.4453, 1.8125, 1.6094, 1.3984, 1.4609,\n",
       "                      1.5938, 1.5625, 1.7031, 1.5859, 1.6719, 1.6016, 1.5156, 1.5547, 1.5938,\n",
       "                      1.2656, 1.5859, 1.4922, 1.6875, 1.3047, 1.6484, 1.4453, 1.5547, 1.4062,\n",
       "                      1.3359, 1.5234, 1.5938, 1.4609, 1.2344, 1.3281, 1.5625, 1.4297, 1.6328,\n",
       "                      1.5625, 1.4844, 1.4531, 1.4453, 1.6641, 1.3984, 1.5859, 1.5000, 1.6562,\n",
       "                      1.6094, 1.5000, 1.4141, 1.5859, 1.6406, 1.3984, 1.4219, 1.3281, 1.1250,\n",
       "                      1.6172, 1.7734, 1.5469, 1.2500, 1.8359, 0.6797, 1.4922, 1.4062, 1.4922,\n",
       "                      1.6641, 1.3438, 1.4062, 1.5156, 1.5469, 1.4453, 1.2188, 1.3906, 1.6406,\n",
       "                      1.6562, 1.5000, 1.4688, 1.6016, 1.4141, 1.5078, 1.3828, 1.2969, 1.3984,\n",
       "                      1.2812, 1.2266, 1.3203, 0.7773, 0.6523, 1.4297, 1.3594, 1.5312, 1.4219,\n",
       "                      1.4688, 1.7422, 1.4922, 1.6797, 1.4922, 1.8125, 1.5234, 1.5703, 1.5781,\n",
       "                      1.4609, 1.6953, 1.6094, 1.5703, 1.7031, 1.5469, 1.4219, 1.4844, 1.6406,\n",
       "                      1.3672, 1.7422, 1.6406, 1.5703, 1.6484, 1.4844, 1.7031, 1.3984, 0.8203,\n",
       "                      1.4844, 1.6406, 1.5625, 1.2812, 1.7344, 1.6562, 1.5078, 1.4766, 1.6953,\n",
       "                      1.4844, 1.5469, 1.6250, 1.7109, 1.6406, 1.5391, 1.2812, 1.8828, 2.6875,\n",
       "                      1.6406, 1.5469, 1.7734, 0.7773, 1.4219, 1.3984, 1.5312, 1.6250, 1.0469,\n",
       "                      1.6484, 1.3906, 1.4922, 1.6641, 1.3281, 1.3359, 1.5000, 1.5625, 1.7031,\n",
       "                      1.5391, 1.5312, 1.4609, 1.5391, 1.4453, 1.5547, 1.5547, 1.5703, 1.3984,\n",
       "                      1.7031, 1.5234, 1.4219, 1.3594, 1.3047, 1.4297, 1.5703, 1.4219, 1.2656,\n",
       "                      1.6562, 1.4688, 1.4609, 1.5234, 1.7578, 1.4688, 1.5469, 1.5547, 1.6562,\n",
       "                      1.5547, 1.5547, 1.3516, 1.5000, 1.3594, 1.5469, 1.3203, 1.6562, 1.4922,\n",
       "                      1.3281, 1.6875, 1.2969])),\n",
       "             ('0.auto_model.encoder.block.11.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0040, -0.0077,  0.0342,  ..., -0.0308, -0.0177,  0.0014],\n",
       "                      [-0.0216, -0.0718,  0.0007,  ..., -0.0449, -0.0422, -0.0002],\n",
       "                      [ 0.0153,  0.0119, -0.0201,  ..., -0.0052,  0.0208, -0.0061],\n",
       "                      ...,\n",
       "                      [-0.0102, -0.0123,  0.0075,  ..., -0.0356, -0.0405,  0.0070],\n",
       "                      [-0.0226,  0.0137, -0.0175,  ...,  0.0286,  0.0219,  0.0226],\n",
       "                      [ 0.0601,  0.0162, -0.0053,  ..., -0.0004,  0.0201,  0.0259]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.1523, -0.0283, -0.1514,  ...,  0.2441,  0.2354, -0.2393],\n",
       "                      [-0.0481, -0.1260,  0.0430,  ..., -0.0161,  0.1504, -0.0767],\n",
       "                      [ 0.1099, -0.0596,  0.4219,  ...,  0.0579, -0.0732, -0.1924],\n",
       "                      ...,\n",
       "                      [ 0.3984, -0.0679,  0.0337,  ...,  0.2559, -0.2344,  0.0119],\n",
       "                      [-0.1660, -0.4102, -0.1484,  ..., -0.1807, -0.0300,  0.2949],\n",
       "                      [ 0.0815,  0.1177,  0.3027,  ...,  0.0182,  0.1396,  0.0126]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.4316,  0.3359,  0.6836,  ...,  1.3516,  0.2080, -0.7383],\n",
       "                      [ 0.8633, -1.3359,  0.5469,  ...,  0.8164, -1.4219, -0.7383],\n",
       "                      [ 1.2734,  0.2227, -2.2812,  ...,  0.2324, -0.9688,  0.2793],\n",
       "                      ...,\n",
       "                      [-0.2891, -0.8359, -0.7227,  ...,  1.5234, -0.8750, -0.1846],\n",
       "                      [-0.0649,  0.6328,  1.5156,  ..., -0.3223, -1.5859,  0.4082],\n",
       "                      [ 0.2676, -0.6250,  1.0547,  ...,  0.2617, -0.1729,  1.8906]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-1.6562,  1.2344, -1.6719,  ...,  1.2734, -0.7578, -1.0156],\n",
       "                      [ 2.1250,  4.7188,  1.1719,  ...,  1.5625, -1.0234, -0.1602],\n",
       "                      [-2.7500, -2.5156,  0.9609,  ...,  1.1719, -2.3906, -0.7500],\n",
       "                      ...,\n",
       "                      [-2.0469,  0.0991,  0.5078,  ...,  0.5273,  0.7969,  0.5117],\n",
       "                      [ 0.8086,  2.2344,  0.5898,  ...,  1.1094,  0.7148,  0.7031],\n",
       "                      [ 1.3203,  0.8945,  0.4609,  ..., -1.0781,  0.2432, -3.3594]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1309,  0.1367,  0.1147,  0.1289,  0.1904,  0.1216,  0.0483,  0.1387,\n",
       "                       0.0554,  0.1670,  0.1191,  0.1187,  0.1250,  0.1060,  0.1670,  0.1553,\n",
       "                       0.1138,  0.1357,  0.0703,  0.1621,  0.1445,  0.1348,  0.0757,  0.1553,\n",
       "                       0.1611,  0.1348,  0.1777,  0.1270,  0.1338,  0.1553,  0.1348,  0.1455,\n",
       "                       0.1455,  0.1699,  0.1162,  0.1533,  0.0825,  0.1055,  0.1641,  0.0508,\n",
       "                       0.1226,  0.1074,  0.0942,  0.1523, -0.0537,  0.1494,  0.1069,  0.1602,\n",
       "                       0.1445,  0.0298,  0.1270,  0.1436,  0.0840,  0.1758, -0.0525,  0.1494,\n",
       "                       0.0972,  0.1445,  0.1484,  0.1748,  0.1523,  0.1729,  0.1445,  0.1572,\n",
       "                       0.1787,  0.0320,  0.1650,  0.1504,  0.1270,  0.1611,  0.1533,  0.0728,\n",
       "                       0.1670,  0.1514,  0.1553,  0.1514,  0.0967,  0.1611,  0.1523,  0.1650,\n",
       "                       0.1699,  0.1445,  0.1592,  0.1182,  0.1484,  0.1582,  0.1147,  0.1426,\n",
       "                       0.1514,  0.1719,  0.1445,  0.1641,  0.1729,  0.1348,  0.1650,  0.1709,\n",
       "                       0.1406,  0.1699,  0.1641,  0.1602,  0.1123,  0.1562,  0.0664,  0.1689,\n",
       "                       0.1641,  0.0938,  0.1543,  0.1514,  0.1514,  0.0674,  0.1338,  0.1543,\n",
       "                       0.1426,  0.1357,  0.1533,  0.1826,  0.1426,  0.1592,  0.0840,  0.1680,\n",
       "                       0.1699,  0.1553,  0.1084,  0.1631,  0.0947,  0.1426,  0.1562,  0.1484,\n",
       "                       0.1216,  0.1128,  0.0566,  0.1250,  0.1387,  0.1562,  0.1406,  0.1074,\n",
       "                       0.1279,  0.1455,  0.1318,  0.1201,  0.1514,  0.1128,  0.0859,  0.1328,\n",
       "                       0.1396,  0.1099,  0.1475,  0.1230,  0.1514,  0.1514,  0.1562,  0.1357,\n",
       "                       0.1206,  0.1309,  0.1436,  0.1641,  0.1299,  0.1699,  0.1079,  0.1582,\n",
       "                       0.1543,  0.1396,  0.1299, -0.0801,  0.1846,  0.1455,  0.1572,  0.1318,\n",
       "                       0.1289,  0.0518,  0.1177,  0.1611,  0.1670,  0.0854,  0.1523,  0.1523,\n",
       "                       0.1279,  0.0479,  0.1094, -0.0210,  0.1641,  0.1768,  0.1318,  0.1768,\n",
       "                       0.1494,  0.1357,  0.1729,  0.1177,  0.1436,  0.1309,  0.1602,  0.1816,\n",
       "                       0.1592,  0.1514,  0.1621,  0.0243,  0.1631,  0.1187,  0.1494,  0.1719,\n",
       "                       0.1602,  0.1279,  0.1611,  0.1621,  0.1611,  0.1221,  0.1504,  0.1426,\n",
       "                       0.1562,  0.1543,  0.1299,  0.1377,  0.1279,  0.1318,  0.1445,  0.1279,\n",
       "                       0.1416,  0.1592,  0.1270,  0.0410,  0.0796,  0.1367,  0.1738,  0.1396,\n",
       "                       0.1211,  0.1602,  0.1226,  0.0928,  0.1475,  0.1494,  0.1436,  0.1348,\n",
       "                       0.1562,  0.1816,  0.1582,  0.1670,  0.1602,  0.1250,  0.1157,  0.0625,\n",
       "                       0.1133,  0.1226,  0.1699,  0.1660,  0.1572,  0.1475,  0.1455,  0.1484,\n",
       "                       0.1621,  0.1201,  0.0742,  0.1484,  0.1572,  0.1631,  0.1514,  0.1426,\n",
       "                       0.1699,  0.1299,  0.0576,  0.1396,  0.1162,  0.0537,  0.1465,  0.1592,\n",
       "                       0.1602,  0.1689,  0.1416,  0.1338,  0.1357,  0.1504,  0.0659,  0.1187,\n",
       "                       0.0928,  0.0154,  0.1084,  0.1533,  0.1650,  0.0903,  0.1533,  0.1348,\n",
       "                       0.1226,  0.1230,  0.1660,  0.1318,  0.1338,  0.1011,  0.1689,  0.1572,\n",
       "                       0.1309,  0.0840,  0.1553,  0.1514,  0.1436,  0.1494,  0.1680,  0.1689,\n",
       "                       0.1309,  0.1768,  0.1377,  0.1245,  0.1396,  0.1582,  0.1230,  0.1357,\n",
       "                       0.1758,  0.1670,  0.1504,  0.1455,  0.1602,  0.1328,  0.1533,  0.1147,\n",
       "                       0.1660,  0.1167,  0.1299,  0.1348,  0.1201,  0.0986,  0.1338,  0.1641,\n",
       "                       0.1650,  0.0923,  0.1445,  0.1777,  0.1235,  0.1611,  0.0938,  0.1177,\n",
       "                       0.1523,  0.1553,  0.1670,  0.0649,  0.1245,  0.1445,  0.1592,  0.1572,\n",
       "                       0.1641,  0.1670,  0.1367,  0.1152,  0.1416,  0.1484,  0.1572,  0.1602,\n",
       "                       0.1348,  0.1348,  0.1099,  0.1318,  0.0835,  0.0679,  0.1226,  0.1572,\n",
       "                       0.1562,  0.1885,  0.1191,  0.1445,  0.1357,  0.1426,  0.1514,  0.0718,\n",
       "                       0.1514,  0.1060,  0.0820,  0.1621,  0.1523,  0.1445,  0.0493,  0.1514,\n",
       "                       0.1514,  0.1572,  0.1245,  0.1250,  0.0669,  0.1494,  0.0972,  0.1514,\n",
       "                       0.1006,  0.1631,  0.0801,  0.1055,  0.1777,  0.1406,  0.0962,  0.1396,\n",
       "                       0.1709,  0.0400,  0.1465,  0.0679,  0.1904,  0.1245,  0.1699,  0.0698,\n",
       "                       0.1572,  0.1040,  0.1289,  0.1592,  0.1387,  0.1680,  0.1338,  0.1738,\n",
       "                       0.1660,  0.0305,  0.1797,  0.1680,  0.1416,  0.1475,  0.1338,  0.1543,\n",
       "                       0.1768,  0.1641,  0.1426,  0.1787,  0.1348,  0.1328,  0.1543,  0.1211,\n",
       "                       0.1475,  0.1523,  0.0581,  0.1050,  0.0879,  0.1396,  0.1660,  0.0469,\n",
       "                       0.1660,  0.1592,  0.1299,  0.1855,  0.1367,  0.1377,  0.1338,  0.1602,\n",
       "                       0.0334,  0.1699,  0.0801,  0.1582,  0.1533,  0.1562,  0.1045,  0.1328,\n",
       "                       0.1553,  0.0981,  0.1367,  0.0767,  0.1533,  0.1475,  0.1533,  0.1289,\n",
       "                       0.1221,  0.1426,  0.1465,  0.1699,  0.1602,  0.1631,  0.1475,  0.1221,\n",
       "                       0.1406,  0.1416,  0.1245,  0.1650,  0.1719,  0.1592,  0.1719,  0.1592,\n",
       "                       0.1367,  0.1445,  0.1416,  0.1455,  0.1641,  0.1699,  0.1641,  0.1807,\n",
       "                       0.1387,  0.1699,  0.1416,  0.1641,  0.1543,  0.1465,  0.0540,  0.1387,\n",
       "                       0.1226,  0.0593,  0.1357,  0.1660,  0.1226,  0.1562,  0.1426,  0.0806,\n",
       "                       0.1680,  0.1455,  0.1611,  0.1504,  0.0586,  0.1689,  0.1631,  0.1387,\n",
       "                       0.1465,  0.1562,  0.1602,  0.1787,  0.1533,  0.1216,  0.1006,  0.0287,\n",
       "                       0.1572,  0.1367,  0.1523,  0.1226,  0.1875,  0.1475,  0.1338,  0.1641,\n",
       "                       0.0762,  0.1748,  0.1357,  0.1641,  0.1387,  0.1318,  0.0698,  0.1299,\n",
       "                       0.1748,  0.1475,  0.1709,  0.1494,  0.1514,  0.0461,  0.1260,  0.0869,\n",
       "                       0.1631,  0.1328,  0.1016,  0.1484,  0.1475,  0.1387,  0.1318,  0.1240,\n",
       "                       0.1406,  0.1484,  0.1104,  0.1416,  0.1270,  0.1621,  0.1187,  0.1016,\n",
       "                       0.1660,  0.1602,  0.1172,  0.1396,  0.1592,  0.0569,  0.0889,  0.1240,\n",
       "                       0.1572,  0.1182,  0.1416,  0.1504,  0.1426,  0.1602,  0.1289,  0.0718,\n",
       "                       0.1396,  0.1602,  0.1157,  0.1113,  0.1553,  0.1436,  0.0219,  0.1523,\n",
       "                       0.1367,  0.1621,  0.1201,  0.0938,  0.1641,  0.1611,  0.1377,  0.1289,\n",
       "                       0.1758,  0.1367,  0.1582,  0.0669,  0.1484,  0.1680,  0.1611,  0.1387,\n",
       "                       0.1670,  0.1846,  0.1621,  0.1562,  0.1592,  0.1631,  0.1455,  0.1187,\n",
       "                       0.1719,  0.1406,  0.1475,  0.1465,  0.1328,  0.1445,  0.0781,  0.1582,\n",
       "                       0.1211,  0.1572,  0.1387,  0.0796,  0.1484,  0.1602,  0.1582,  0.1152,\n",
       "                       0.1064,  0.1436,  0.1504,  0.1455,  0.1289,  0.1582,  0.1455,  0.1206,\n",
       "                       0.1406,  0.1357,  0.1572,  0.1494,  0.1328,  0.1328,  0.1523,  0.0913,\n",
       "                       0.1455,  0.1455,  0.1338,  0.1177,  0.1196,  0.0659,  0.1455,  0.1562,\n",
       "                       0.1279,  0.1206,  0.1719,  0.0564,  0.1602,  0.1270,  0.0596,  0.1709,\n",
       "                       0.1133,  0.1377,  0.1426,  0.1328,  0.1416,  0.1406,  0.1045,  0.1621,\n",
       "                       0.1523,  0.1670,  0.1426,  0.1738,  0.1357,  0.1689,  0.0928,  0.1040,\n",
       "                       0.1553,  0.1133,  0.1089,  0.1318,  0.0708,  0.0311,  0.1465,  0.1289,\n",
       "                       0.1514,  0.1152,  0.1426,  0.1338,  0.1455,  0.1699,  0.1465,  0.1201,\n",
       "                       0.1455,  0.1514,  0.1553,  0.1641,  0.1377,  0.1504,  0.1494,  0.1465,\n",
       "                       0.1484,  0.1631,  0.1504,  0.1689,  0.1514,  0.0557,  0.1562,  0.1465,\n",
       "                       0.1562,  0.1572,  0.1553,  0.1543,  0.0771,  0.1504,  0.1562,  0.1465,\n",
       "                       0.1196,  0.1553,  0.1445,  0.1357,  0.1396,  0.1650,  0.1533,  0.1338,\n",
       "                       0.1245,  0.1621,  0.1689,  0.1123,  0.1030,  0.1650,  0.0547,  0.1543,\n",
       "                       0.1582,  0.1719,  0.0369,  0.1416,  0.1250,  0.1436,  0.1387,  0.0815,\n",
       "                       0.1611,  0.0967,  0.1133,  0.1436,  0.1187,  0.1206,  0.1279,  0.1387,\n",
       "                       0.1729,  0.1406,  0.1416,  0.1406,  0.1475,  0.1436,  0.1465,  0.1572,\n",
       "                       0.1543,  0.1465,  0.1484,  0.1387,  0.1504,  0.1167,  0.1157,  0.1377,\n",
       "                       0.1650,  0.1270,  0.1289,  0.1562,  0.1289,  0.0674,  0.1436,  0.1562,\n",
       "                       0.1118,  0.1416,  0.1123,  0.1582,  0.1270,  0.1680,  0.1426,  0.1191,\n",
       "                       0.1279,  0.1416,  0.1279,  0.1543,  0.1216,  0.1270,  0.1553,  0.1216])),\n",
       "             ('0.auto_model.encoder.block.11.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.2461,  0.6523,  0.0317,  ..., -0.0811, -0.0613,  0.5078],\n",
       "                      [-1.5234, -1.4297,  0.1729,  ...,  0.1030,  0.0786, -0.1177],\n",
       "                      [ 1.1172,  0.5234,  0.5195,  ...,  0.8281, -1.3984,  0.1904],\n",
       "                      ...,\n",
       "                      [ 0.5195,  0.3184, -0.7773,  ...,  0.3926, -0.0129,  0.1162],\n",
       "                      [ 0.4844, -1.9141, -0.0820,  ..., -0.5898, -0.3711, -0.6211],\n",
       "                      [ 0.2520, -0.8164,  0.0703,  ...,  1.5469, -0.9570, -0.4297]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-1.8066e-02, -4.1602e-01, -8.7109e-01,  ...,  1.6724e-02,\n",
       "                        1.7090e-01,  4.9414e-01],\n",
       "                      [ 5.4297e-01,  2.7344e-01,  1.0889e-01,  ..., -6.3965e-02,\n",
       "                       -2.5195e-01, -6.6406e-01],\n",
       "                      [ 3.0469e-01, -1.5723e-01,  6.4453e-01,  ...,  7.4219e-02,\n",
       "                       -5.5859e-01,  8.7500e-01],\n",
       "                      ...,\n",
       "                      [-7.7637e-02, -1.2207e-01, -1.7383e-01,  ...,  3.6133e-02,\n",
       "                        1.1670e-01,  5.7422e-01],\n",
       "                      [-4.3678e-04, -2.0898e-01, -7.7734e-01,  ..., -4.5166e-03,\n",
       "                       -1.2061e-01,  4.6484e-01],\n",
       "                      [ 5.8838e-02,  5.3125e-01,  6.9531e-01,  ..., -3.9864e-04,\n",
       "                       -6.4062e-01, -6.2891e-01]])),\n",
       "             ('0.auto_model.encoder.block.11.layer.1.layer_norm.weight',\n",
       "              tensor([0.8516, 0.8203, 0.8359, 0.7891, 1.2031, 1.0625, 0.1680, 0.8555, 0.7852,\n",
       "                      0.8828, 0.7305, 0.8047, 1.0156, 0.8242, 1.1328, 0.9727, 0.7773, 0.8359,\n",
       "                      0.5742, 1.0625, 1.0156, 0.8594, 2.8906, 1.0391, 1.1719, 0.8789, 1.1250,\n",
       "                      0.7891, 0.8711, 1.0703, 0.8945, 0.9727, 0.9570, 1.0625, 0.7930, 1.0156,\n",
       "                      0.7852, 0.7773, 1.0469, 1.0703, 0.8320, 0.8906, 0.5898, 0.9141, 1.0000,\n",
       "                      1.0547, 0.8164, 1.0000, 0.8555, 0.4766, 0.7227, 0.9609, 0.8242, 1.0469,\n",
       "                      0.9102, 0.8906, 2.1875, 0.9023, 0.9609, 0.9648, 0.9648, 1.0938, 1.0547,\n",
       "                      1.0156, 1.0703, 0.3301, 1.1016, 0.9102, 0.9414, 1.0312, 1.0781, 0.5508,\n",
       "                      1.0859, 0.9531, 0.9844, 1.0703, 0.7812, 0.9922, 1.0547, 0.9414, 1.0469,\n",
       "                      0.8477, 1.0469, 0.7930, 1.0469, 0.9805, 0.9609, 0.9883, 0.9375, 1.0156,\n",
       "                      0.9180, 1.0391, 1.0703, 0.9492, 1.1641, 1.0703, 1.0703, 0.9883, 1.0703,\n",
       "                      0.9023, 0.9336, 1.1641, 0.6680, 1.0078, 0.9727, 1.0078, 1.0391, 1.0312,\n",
       "                      1.1719, 1.3125, 0.9180, 1.0469, 0.8438, 0.8867, 0.8438, 1.0312, 0.8867,\n",
       "                      1.0391, 0.6445, 1.0078, 1.1328, 0.9922, 0.9180, 1.0703, 0.6289, 1.0000,\n",
       "                      0.9219, 1.0469, 0.9297, 0.9258, 1.0781, 0.8750, 1.0469, 1.0703, 1.0078,\n",
       "                      0.6016, 0.8906, 0.9805, 0.8594, 0.8086, 0.9219, 0.7695, 0.8008, 0.9688,\n",
       "                      0.9492, 0.9219, 1.0625, 0.8789, 1.0391, 1.0703, 0.9336, 0.8555, 0.9219,\n",
       "                      0.9883, 1.0391, 1.1250, 0.9297, 1.1797, 0.7539, 0.9375, 1.0234, 0.9414,\n",
       "                      0.9062, 2.2656, 1.0312, 0.9375, 1.0781, 0.7188, 0.7617, 0.5352, 0.9648,\n",
       "                      0.9375, 1.0156, 0.7188, 1.0156, 0.9609, 0.8516, 0.5859, 0.8164, 0.1592,\n",
       "                      1.0312, 1.0156, 0.8750, 1.0938, 0.8789, 0.8242, 1.0312, 0.7891, 0.9062,\n",
       "                      0.7930, 1.1094, 1.0547, 1.0859, 0.9453, 1.0625, 0.4609, 0.9961, 0.7656,\n",
       "                      0.9141, 0.9766, 1.0781, 0.8047, 0.9922, 1.0469, 0.9766, 0.8555, 1.0781,\n",
       "                      0.9883, 0.9180, 1.0781, 0.9219, 0.8984, 0.8594, 1.1016, 0.9414, 0.9375,\n",
       "                      0.9219, 0.9297, 0.8828, 0.7188, 0.9062, 0.8984, 1.0156, 0.9453, 0.7539,\n",
       "                      1.0078, 0.8203, 1.4062, 0.9688, 0.8750, 0.9492, 0.8906, 1.5938, 1.0938,\n",
       "                      1.1172, 1.1094, 2.0156, 0.8242, 0.9102, 0.6055, 0.8750, 0.7930, 1.0625,\n",
       "                      1.0547, 1.1562, 0.9414, 0.8789, 0.9727, 1.1172, 0.8945, 1.0859, 1.0391,\n",
       "                      1.0469, 1.0547, 1.0938, 1.0469, 0.9531, 0.8594, 0.7695, 0.8945, 0.7852,\n",
       "                      0.5977, 0.9961, 1.0859, 1.0078, 1.0703, 1.0234, 0.9727, 0.9883, 1.0312,\n",
       "                      1.2969, 0.7344, 0.7773, 0.1729, 0.8047, 0.9883, 1.0078, 0.5742, 0.9844,\n",
       "                      0.9727, 0.9727, 0.8438, 1.0547, 1.0391, 0.9922, 0.7734, 1.1797, 1.0156,\n",
       "                      0.7852, 1.0156, 1.0781, 0.7969, 1.0703, 1.0859, 0.9805, 1.1562, 0.8750,\n",
       "                      1.0625, 1.0312, 0.9336, 0.9531, 0.9609, 0.8203, 0.8633, 1.0469, 1.1172,\n",
       "                      0.9258, 1.0234, 0.9453, 0.8789, 1.0234, 0.9727, 1.0703, 0.8867, 0.9688,\n",
       "                      0.9492, 1.1484, 0.7539, 0.9688, 0.9883, 1.0469, 0.7852, 0.9844, 1.1016,\n",
       "                      0.8203, 1.0391, 0.7188, 0.8633, 0.8867, 0.8945, 1.1172, 0.7148, 0.9141,\n",
       "                      0.9336, 0.9336, 0.9727, 1.1641, 1.0625, 0.9688, 1.1562, 1.0859, 0.9258,\n",
       "                      1.1016, 0.9219, 0.9883, 0.8125, 0.7305, 0.8438, 0.7812, 1.0469, 0.9453,\n",
       "                      1.1094, 0.9648, 1.0547, 0.8203, 1.0938, 0.8828, 0.8945, 0.9883, 0.6602,\n",
       "                      1.0703, 0.7148, 0.6680, 1.1016, 1.0859, 0.9688, 0.5195, 0.9180, 1.0391,\n",
       "                      0.9414, 1.0625, 0.8320, 0.5039, 1.0234, 0.8398, 1.0312, 0.9375, 0.9648,\n",
       "                      1.0547, 0.7812, 1.0000, 0.8711, 0.8359, 1.0312, 1.0625, 0.5547, 1.0156,\n",
       "                      0.5117, 1.0938, 0.9375, 0.9531, 3.2656, 0.8828, 0.8125, 0.9023, 1.0859,\n",
       "                      0.9297, 1.1484, 0.8398, 1.0312, 1.0234, 0.7383, 1.1406, 1.0000, 0.9727,\n",
       "                      1.0312, 0.8906, 1.0391, 1.0625, 1.0000, 0.9102, 1.0859, 0.9844, 0.9297,\n",
       "                      1.0469, 0.9258, 1.0938, 1.0391, 0.8203, 0.7891, 0.8516, 1.0078, 1.1094,\n",
       "                      0.4277, 0.9766, 1.0547, 0.8945, 1.0234, 0.8594, 0.9531, 0.7695, 1.0391,\n",
       "                      0.5820, 0.9961, 0.7656, 1.0000, 0.8945, 1.0469, 0.7266, 0.8867, 0.9141,\n",
       "                      0.7812, 0.9883, 0.5859, 1.1484, 0.8750, 0.9336, 0.9219, 0.8164, 0.9648,\n",
       "                      0.9844, 1.1016, 0.9570, 1.0312, 0.9648, 1.0312, 0.8594, 0.8008, 0.9180,\n",
       "                      0.9883, 1.1875, 1.0156, 1.0547, 0.9141, 0.8320, 1.0547, 0.9727, 0.9922,\n",
       "                      1.0469, 1.1406, 1.0391, 1.0547, 0.8281, 1.1328, 0.9258, 1.0078, 1.2422,\n",
       "                      0.8633, 1.0625, 0.9414, 0.8789, 1.0234, 0.9531, 1.0312, 0.8711, 1.1172,\n",
       "                      1.0781, 0.6836, 1.1484, 0.9180, 0.9922, 0.9375, 1.2969, 1.1875, 0.9336,\n",
       "                      0.9297, 0.8867, 0.8477, 1.0625, 1.1094, 0.9805, 0.9336, 0.7852, 1.3125,\n",
       "                      0.9062, 0.9102, 0.8477, 0.8750, 1.1562, 0.9453, 0.9531, 1.0938, 1.2891,\n",
       "                      1.1406, 0.8867, 1.1406, 0.7891, 0.9180, 0.7266, 0.9297, 1.1016, 0.9727,\n",
       "                      1.1328, 0.9844, 0.9297, 0.8516, 0.8203, 0.6484, 0.9727, 0.8594, 0.8750,\n",
       "                      1.1016, 1.0156, 0.9336, 0.9180, 0.8008, 1.0156, 0.9414, 0.7422, 0.8320,\n",
       "                      0.7539, 0.9141, 0.8477, 0.7617, 1.0234, 1.1016, 0.7891, 0.8984, 1.0781,\n",
       "                      0.3613, 0.8438, 0.8633, 1.2031, 1.0234, 0.9531, 1.0469, 0.8867, 0.9961,\n",
       "                      0.9570, 0.5977, 0.8828, 0.9023, 0.7383, 0.7227, 0.9180, 0.8125, 0.1709,\n",
       "                      0.8711, 0.8359, 1.0703, 0.8789, 0.6172, 1.2109, 0.8516, 0.9766, 1.0234,\n",
       "                      1.0156, 0.8789, 1.0234, 0.6367, 0.9180, 1.1641, 1.1016, 1.1094, 1.0703,\n",
       "                      1.0703, 1.0781, 1.1328, 1.1641, 1.0859, 0.9570, 0.9570, 0.9727, 0.8867,\n",
       "                      0.9180, 1.0000, 0.8594, 1.0391, 0.7500, 1.0703, 0.8867, 0.9883, 0.8672,\n",
       "                      0.6484, 0.9336, 1.0391, 0.9609, 0.8359, 0.7734, 0.9336, 0.8828, 1.0547,\n",
       "                      0.8711, 0.9023, 0.8477, 0.8164, 0.9688, 0.8711, 1.0547, 0.8711, 0.9180,\n",
       "                      1.0469, 1.0234, 0.8945, 0.8984, 0.9141, 0.8984, 0.8438, 0.8945, 0.6758,\n",
       "                      0.9531, 1.1094, 0.8555, 0.8398, 0.9922, 0.5703, 1.0312, 0.8711, 0.5781,\n",
       "                      1.0000, 0.8477, 0.8867, 0.9648, 0.9961, 0.8320, 0.7656, 0.7930, 1.1250,\n",
       "                      1.0859, 1.0859, 0.9688, 1.0391, 0.9414, 0.9688, 0.8008, 0.6602, 1.0469,\n",
       "                      0.7656, 0.7422, 0.8711, 0.5000, 0.3184, 0.8906, 0.7852, 0.9453, 0.9219,\n",
       "                      1.0078, 1.0703, 1.0391, 1.0703, 0.9570, 1.0078, 0.9492, 1.0078, 0.9883,\n",
       "                      1.0469, 1.0078, 0.9883, 0.9023, 1.0312, 0.9414, 1.0234, 1.0312, 1.0469,\n",
       "                      0.8164, 0.8750, 0.9531, 1.0234, 1.0469, 0.8711, 1.0625, 0.9023, 0.4902,\n",
       "                      0.9180, 1.0078, 1.0156, 0.7891, 1.0312, 1.1406, 0.8633, 0.9805, 1.1250,\n",
       "                      0.9922, 0.8320, 0.9297, 1.1172, 1.0781, 0.8438, 0.7500, 1.1250, 0.9336,\n",
       "                      0.8984, 0.9961, 1.0391, 0.4609, 0.9883, 0.8438, 0.9023, 0.9531, 0.5352,\n",
       "                      1.0391, 0.8984, 0.9258, 0.9922, 0.8242, 0.8438, 0.9141, 0.9531, 1.0000,\n",
       "                      0.8047, 0.8672, 1.0078, 1.0703, 0.9336, 0.8594, 1.0156, 0.9570, 0.8125,\n",
       "                      1.0625, 0.9141, 0.9883, 0.7148, 0.7461, 0.8672, 1.0781, 0.7617, 0.8633,\n",
       "                      1.0234, 0.9688, 0.9727, 0.9336, 1.1250, 1.0078, 0.9531, 0.9258, 1.0000,\n",
       "                      0.9375, 0.9688, 0.9336, 0.8711, 0.8633, 1.1875, 0.8828, 1.1016, 0.9531,\n",
       "                      0.8477, 1.0156, 0.8359])),\n",
       "             ('0.auto_model.encoder.final_layer_norm.weight',\n",
       "              tensor([ 0.4785,  0.2773,  0.2930,  0.3105,  0.7500,  0.6133,  0.0212,  0.5039,\n",
       "                       0.0913,  0.6406,  0.3145,  0.2754,  0.4590,  0.3203,  0.7578,  0.6367,\n",
       "                       0.4258,  0.5898,  0.1807,  0.6680,  0.6211,  0.6211,  0.0339,  0.6289,\n",
       "                       0.7695,  0.5234,  0.6875,  0.4531,  0.5586,  0.7461,  0.6172,  0.5703,\n",
       "                       0.5156,  0.6797,  0.3125,  0.6172,  0.1992,  0.2520,  0.7617,  0.0430,\n",
       "                       0.5039,  0.2080,  0.1846,  0.5781,  0.1729,  0.5938,  0.1729,  0.6055,\n",
       "                       0.5312,  0.1006,  0.2656,  0.5430,  0.2910,  0.7539,  0.0231,  0.5703,\n",
       "                       0.0669,  0.6133,  0.6289,  0.6523,  0.5742,  0.7344,  0.5977,  0.6914,\n",
       "                       0.7109,  0.0113,  0.6523,  0.6211,  0.3809,  0.6211,  0.6016,  0.1533,\n",
       "                       0.6914,  0.6055,  0.6797,  0.6875,  0.3789,  0.5664,  0.7344,  0.6250,\n",
       "                       0.7109,  0.4551,  0.7305,  0.3594,  0.5898,  0.5664,  0.4219,  0.4727,\n",
       "                       0.5664,  0.7266,  0.5586,  0.6758,  0.6719,  0.5469,  0.6836,  0.6914,\n",
       "                       0.6523,  0.6875,  0.7383,  0.6094,  0.3438,  0.5977,  0.1582,  0.6836,\n",
       "                       0.5898,  0.1689,  0.6484,  0.6602,  0.7227,  0.1523,  0.4688,  0.6250,\n",
       "                       0.6133,  0.6523,  0.6133,  0.7148,  0.4883,  0.6797,  0.1865,  0.6797,\n",
       "                       0.8242,  0.6094,  0.2715,  0.6719,  0.1562,  0.6367,  0.6484,  0.4902,\n",
       "                       0.5703,  0.3750,  0.1260,  0.3828,  0.6719,  0.7109,  0.5352,  0.2256,\n",
       "                       0.4902,  0.6367,  0.3984,  0.4551,  0.5547,  0.2070,  0.1748,  0.2598,\n",
       "                       0.5820,  0.3145,  0.7344,  0.3398,  0.6328,  0.5039,  0.6484,  0.4961,\n",
       "                       0.5391,  0.2598,  0.7656,  0.6875,  0.3574,  0.7148,  0.3066,  0.5898,\n",
       "                       0.7422,  0.5977,  0.5039,  0.0520,  0.6445,  0.5469,  0.6016,  0.3770,\n",
       "                       0.4902,  0.1865,  0.2490,  0.6289,  0.6016,  0.1572,  0.5625,  0.3203,\n",
       "                       0.5703,  0.0620,  0.3086,  0.0092,  0.6602,  0.6992,  0.6055,  0.6953,\n",
       "                       0.5039,  0.4297,  0.7422,  0.2617,  0.5742,  0.5781,  0.6016,  0.7539,\n",
       "                       0.6172,  0.5586,  0.7539,  0.0206,  0.6016,  0.3184,  0.5859,  0.7070,\n",
       "                       0.7305,  0.4844,  0.6484,  0.6719,  0.6719,  0.3555,  0.6211,  0.6445,\n",
       "                       0.6211,  0.6250,  0.4688,  0.6836,  0.4590,  0.3652,  0.5938,  0.5078,\n",
       "                       0.6172,  0.6445,  0.5859,  0.1406,  0.1748,  0.5195,  0.7148,  0.5938,\n",
       "                       0.4844,  0.6367,  0.3906,  0.1348,  0.6016,  0.4160,  0.4551,  0.5234,\n",
       "                       0.1289,  0.7305,  0.7266,  0.6406,  0.1758,  0.5469,  0.2988,  0.1602,\n",
       "                       0.3848,  0.5391,  0.6367,  0.6328,  0.6992,  0.6172,  0.6094,  0.5156,\n",
       "                       0.7773,  0.5391,  0.1289,  0.6484,  0.6562,  0.7305,  0.7188,  0.5977,\n",
       "                       0.6641,  0.5508,  0.0151,  0.6328,  0.2598,  0.2344,  0.7188,  0.6602,\n",
       "                       0.6758,  0.6641,  0.6406,  0.6016,  0.6523,  0.6992,  0.2012,  0.2188,\n",
       "                       0.1992,  0.0114,  0.2314,  0.5391,  0.6758,  0.1631,  0.6758,  0.5234,\n",
       "                       0.4277,  0.2656,  0.6719,  0.5430,  0.5938,  0.2002,  0.7578,  0.6641,\n",
       "                       0.2285,  0.1982,  0.7109,  0.5273,  0.7266,  0.6953,  0.6758,  0.7422,\n",
       "                       0.4355,  0.6914,  0.7031,  0.5273,  0.4277,  0.7148,  0.2812,  0.5312,\n",
       "                       0.5781,  0.6875,  0.5391,  0.6445,  0.5469,  0.5117,  0.6016,  0.3789,\n",
       "                       0.6797,  0.4355,  0.3418,  0.5156,  0.3594,  0.3457,  0.4551,  0.6797,\n",
       "                       0.7734,  0.2129,  0.4902,  0.7227,  0.2969,  0.6562,  0.1826,  0.3027,\n",
       "                       0.5664,  0.6523,  0.7422,  0.1562,  0.2910,  0.5977,  0.6367,  0.6562,\n",
       "                       0.6797,  0.6367,  0.5508,  0.3223,  0.5586,  0.4648,  0.4102,  0.6641,\n",
       "                       0.5586,  0.5898,  0.2266,  0.5742,  0.2100,  0.1455,  0.3105,  0.7031,\n",
       "                       0.5781,  0.6484,  0.3672,  0.5156,  0.5156,  0.4219,  0.7148,  0.1748,\n",
       "                       0.6641,  0.2070,  0.1553,  0.8242,  0.6914,  0.6758,  0.1064,  0.6289,\n",
       "                       0.7070,  0.6055,  0.5000,  0.4473,  0.0337,  0.6289,  0.3262,  0.7188,\n",
       "                       0.1797,  0.6406,  0.1426,  0.3652,  0.6055,  0.4941,  0.1729,  0.6094,\n",
       "                       0.6484,  0.1299,  0.6445,  0.1211,  0.6797,  0.4570,  0.6758,  0.0122,\n",
       "                       0.5195,  0.3438,  0.3965,  0.6133,  0.4160,  0.6914,  0.5391,  0.7070,\n",
       "                       0.6445,  0.0096,  0.7734,  0.7227,  0.6641,  0.5898,  0.4551,  0.6680,\n",
       "                       0.7070,  0.6680,  0.6484,  0.6797,  0.5625,  0.5078,  0.7617,  0.6211,\n",
       "                       0.7227,  0.7969,  0.0747,  0.3828,  0.1416,  0.6016,  0.7852,  0.0454,\n",
       "                       0.5781,  0.6836,  0.5469,  0.6836,  0.5469,  0.3691,  0.3848,  0.7148,\n",
       "                       0.1318,  0.7617,  0.1865,  0.6992,  0.6172,  0.6680,  0.1826,  0.4512,\n",
       "                       0.5352,  0.3086,  0.4160,  0.1885,  0.6250,  0.5547,  0.7031,  0.5469,\n",
       "                       0.2930,  0.5898,  0.6172,  0.7148,  0.6562,  0.7070,  0.5117,  0.4648,\n",
       "                       0.3730,  0.5352,  0.3594,  0.7383,  0.6992,  0.6289,  0.6953,  0.5156,\n",
       "                       0.5352,  0.5820,  0.6523,  0.6055,  0.7031,  0.6875,  0.7031,  0.6719,\n",
       "                       0.5117,  0.7500,  0.6172,  0.6484,  0.7109,  0.7070,  0.0162,  0.5195,\n",
       "                       0.4707,  0.1387,  0.6328,  0.6719,  0.3242,  0.6562,  0.6484,  0.2344,\n",
       "                       0.7188,  0.5703,  0.6445,  0.4375,  0.0679,  0.6797,  0.6211,  0.3770,\n",
       "                       0.5469,  0.6328,  0.6406,  0.6758,  0.6445,  0.3555,  0.3750,  0.0140,\n",
       "                       0.5977,  0.4551,  0.5859,  0.4434,  0.8086,  0.5859,  0.4355,  0.7734,\n",
       "                       0.1211,  0.6875,  0.4883,  0.6914,  0.5352,  0.5547,  0.2002,  0.3477,\n",
       "                       0.7617,  0.5469,  0.7773,  0.5938,  0.5820,  0.1484,  0.4492,  0.1738,\n",
       "                       0.5977,  0.5039,  0.1904,  0.6055,  0.5508,  0.5625,  0.5391,  0.4023,\n",
       "                       0.6055,  0.6562,  0.2695,  0.6133,  0.4883,  0.6758,  0.4707,  0.1904,\n",
       "                       0.6953,  0.6875,  0.4883,  0.6172,  0.6680,  0.0796,  0.2188,  0.3574,\n",
       "                       0.7148,  0.2656,  0.7031,  0.6641,  0.5586,  0.7305,  0.4551,  0.1982,\n",
       "                       0.4668,  0.6523,  0.3281,  0.3340,  0.5312,  0.5117,  0.0110,  0.5625,\n",
       "                       0.4453,  0.6484,  0.4277,  0.1445,  0.7422,  0.6602,  0.5898,  0.4766,\n",
       "                       0.6797,  0.3223,  0.6211,  0.1650,  0.5820,  0.7539,  0.7031,  0.5938,\n",
       "                       0.7383,  0.7188,  0.7383,  0.6445,  0.7188,  0.6992,  0.6133,  0.3555,\n",
       "                       0.7305,  0.6016,  0.5781,  0.6367,  0.4570,  0.6016,  0.1875,  0.6094,\n",
       "                       0.3633,  0.6328,  0.4512,  0.1553,  0.5977,  0.7578,  0.7188,  0.4316,\n",
       "                       0.2217,  0.5977,  0.5625,  0.6953,  0.4922,  0.6914,  0.5586,  0.2988,\n",
       "                       0.6250,  0.2441,  0.6328,  0.5938,  0.4746,  0.6641,  0.6328,  0.2217,\n",
       "                       0.5547,  0.6328,  0.4844,  0.2695,  0.6211,  0.1846,  0.4609,  0.6484,\n",
       "                       0.5117,  0.2559,  0.7148,  0.1196,  0.5469,  0.5312,  0.1631,  0.6680,\n",
       "                       0.4531,  0.5117,  0.7148,  0.6055,  0.4199,  0.3555,  0.2363,  0.6875,\n",
       "                       0.6211,  0.6562,  0.6289,  0.6836,  0.6094,  0.6406,  0.2090,  0.1807,\n",
       "                       0.6797,  0.3379,  0.2168,  0.4902,  0.1318, -0.0108,  0.6055,  0.2637,\n",
       "                       0.5820,  0.4629,  0.6250,  0.7266,  0.5703,  0.6602,  0.5430,  0.3398,\n",
       "                       0.6250,  0.6211,  0.4863,  0.7031,  0.6406,  0.6289,  0.6250,  0.6172,\n",
       "                       0.5039,  0.6562,  0.4082,  0.7812,  0.6211,  0.1689,  0.6094,  0.4727,\n",
       "                       0.6641,  0.5430,  0.7656,  0.5430,  0.1807,  0.6836,  0.7070,  0.6289,\n",
       "                       0.3691,  0.7109,  0.5430,  0.5469,  0.5156,  0.7422,  0.5273,  0.5469,\n",
       "                       0.5312,  0.7539,  0.6250,  0.3672,  0.2676,  0.7148,  0.1035,  0.5742,\n",
       "                       0.5859,  0.6992,  0.0879,  0.5273,  0.4082,  0.3438,  0.5547,  0.1377,\n",
       "                       0.6914,  0.3164,  0.3555,  0.6992,  0.5508,  0.4629,  0.5195,  0.6562,\n",
       "                       0.7188,  0.4629,  0.5195,  0.6797,  0.6875,  0.5625,  0.6328,  0.6250,\n",
       "                       0.6328,  0.5742,  0.6211,  0.4902,  0.5742,  0.2520,  0.3984,  0.4004,\n",
       "                       0.6484,  0.4570,  0.4434,  0.7500,  0.4434,  0.1475,  0.4688,  0.5625,\n",
       "                       0.2637,  0.5625,  0.4551,  0.6484,  0.5625,  0.6562,  0.4004,  0.4805,\n",
       "                       0.4980,  0.5625,  0.5273,  0.7266,  0.3887,  0.4355,  0.5781,  0.4746])),\n",
       "             ('2.linear.weight',\n",
       "              tensor([[ 0.0371, -0.0349,  0.0664,  ...,  0.0708, -0.0635, -0.0019],\n",
       "                      [-0.0566, -0.0581, -0.0413,  ...,  0.0339, -0.0096,  0.0742],\n",
       "                      [-0.0110, -0.0027, -0.0334,  ..., -0.0190, -0.0272,  0.0092],\n",
       "                      ...,\n",
       "                      [-0.0713, -0.0527,  0.0674,  ..., -0.0101, -0.0209,  0.0635],\n",
       "                      [ 0.0332, -0.0267,  0.0420,  ...,  0.0082, -0.0205, -0.0175],\n",
       "                      [-0.0757,  0.0791, -0.0049,  ..., -0.0601,  0.0544,  0.0220]]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = model.__getitem__(3)\n",
    "dense = model.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.activation_function.__dir__()\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/46a149433fe9af0851f7fa6f9bf37b5ffa2c891c/sentence_transformers/models/Dense.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f\n",
      "Reusing dataset parquet (/home/pgrimal/.cache/huggingface/datasets/PaulLerner___parquet/PaulLerner--triviaqa_for_viquae-ab2ffed0dcbf3b5f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 4/4 [00:00<00:00, 172.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PaulLerner/triviaqa_for_viquae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    with_viquae_test: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "    with_viquae_train: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1181\n",
       "    })\n",
       "    with_viquae_validation: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 1234\n",
       "    })\n",
       "    without_viquae: Dataset({\n",
       "        features: ['id', 'input', 'meta', 'output'],\n",
       "        num_rows: 47000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_kilt = load_from_disk(\"/home/pgrimal/Documents/data_viquae/kilt_trivia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kilt_id', 'wikipedia_id', 'wikipedia_title', 'text', 'anchors', 'categories', 'wikidata_info', 'history'],\n",
       "    num_rows: 5903530\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['summarization', 'translation_en_to_de', 'translation_en_to_fr', 'translation_en_to_ro'])\n"
     ]
    }
   ],
   "source": [
    "print(model.config.task_specific_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British spy.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"James Bond is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship built in 1912.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"Titanic is a <extra_id_0> <extra_id_1>\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP preprocessing\n",
    "\n",
    "voir dans VL-T5/src/caption_clip_data.py\n",
    "\n",
    "But : comprendre comment sont générés les features actuellement avec clip\n",
    "Si bonne facon d'utiliser clip on laisse comme ça sinon on essaie de fournir juste un embedding d'image et pas plusieurs embedding en fonction des boxes\n",
    "\n",
    "Fichier utilisé cvlep.CLIPT5.clip_prepro_feats\n",
    "\n",
    "charger le modèle :\n",
    "\n",
    "Dans VLT5 il y a cette fonction\n",
    "```py\n",
    "def vis_forward(self, batch, device):\n",
    "    if hasattr(self, \"vis_encoder\"):\n",
    "        # self.vis_encoder.eval() # freeze the batchnorm statistics\n",
    "        images = batch[\"images\"].to(device)\n",
    "\n",
    "        if self.config.vis_pooling_output:\n",
    "            _, vis_feats = self.vis_encoder(images)\n",
    "        else:\n",
    "            vis_feats, _ = self.vis_encoder(images)\n",
    "        # vis_feats: (B, dim, L ** 0.5, L ** 0.5)\n",
    "        B, L, D = vis_feats.shape\n",
    "        vis_pos = torch.zeros(B, L, 4, dtype=vis_feats.dtype)\n",
    "\n",
    "        batch[\"vis_feats\"] = vis_feats\n",
    "        batch[\"boxes\"] = vis_pos\n",
    "```\n",
    "\n",
    "Voir ce que renvoie le modèle avec la fonction\n",
    "\n",
    "```py\n",
    "self.vis_encoder = get_vis_encoder(\n",
    "        backbone=vis_encoder_type, \n",
    "        image_size=eval(self.args.image_size)[0],\n",
    "        adapter_type=None,\n",
    "    )\n",
    "self.model.vis_encoder = self.vis_encoder\n",
    "```\n",
    "\n",
    "get_vis encoder dans vis_encoder.py\n",
    "\n",
    "Use CLIP-ResNet50 for fair comparaison\n",
    "\n",
    "format en entrée de vis_forward\n",
    "regarder dans vlt5 cococaption T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from cvlep.CLIPT5.vis_encoder import get_vis_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "model = get_vis_encoder(backbone='RN101', adapter_type=None, image_size=eval(\"(224,224)\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(),'data_model/clip/clip_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "model = get_vis_encoder(backbone='data_model/clip/RN101.pt', adapter_type=None, image_size=eval(\"(224,224)\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    ")\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        # PadToSquare(),\n",
    "        Resize(n_px, interpolation=Image.Resampling.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        # MinMaxResize(*n_px),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def augmentation_transform(image_size):\n",
    "    return Compose([\n",
    "        Resize(image_size, interpolation=Image.Resampling.BICUBIC),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomCrop(image_size, padding=int(image_size[0]*0.0625), padding_mode='reflect'),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        RandomErasing(),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform = _transform(eval(\"(224,224)\")[0])\n",
    "transform2 = augmentation_transform(eval(\"(224,224)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/pgrimal/Documents/Projects/entity_image/data/Commons_wikimage/-%20Narcissus%20pseudonarcissus%2003%20-.jpg\"\n",
    "image = Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transform(image)\n",
    "t2 = transform2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = t.view(224,224,3)\n",
    "image2 = t2.view(224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9ee1169810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9edeb54190>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = 36 #self.args.n_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake batch\n",
    "t = torch.unsqueeze(t, dim=0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooling torch.Size([1, 1, 512])\n",
      "output classic torch.Size([1, 49, 2048])\n"
     ]
    }
   ],
   "source": [
    "_, pooling_output = output\n",
    "output_normal, _ = output\n",
    "\n",
    "print(\"pooling\", pooling_output.shape)\n",
    "print(\"output classic\", output_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_normal[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3515/1658407024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2B, L/2, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, L, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 25 but got size 24 for tensor number 1 in the list."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas pooling\n",
    "B, L, D = pooling_output.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=pooling_output.dtype)\n",
    "\n",
    "batch = {}\n",
    "batch[\"vis_feats\"] = pooling_output\n",
    "batch[\"boxes\"] = vis_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 4])\n",
      "torch.Size([1, 49, 2048])\n"
     ]
    }
   ],
   "source": [
    "# cas non pooling\n",
    "B, L, D = output_normal.shape\n",
    "vis_pos = torch.zeros(B, L, 4, dtype=output_normal.dtype)\n",
    "batch = {}\n",
    "batch[\"vis_feats\"] = output_normal\n",
    "batch[\"boxes\"] = vis_pos\n",
    "print(vis_pos.shape)\n",
    "print(output_normal.shape)\n",
    "# on coonsidère dans ce cas qu'on utilise 49 images ?\n",
    "# multiplier les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip\n",
    "COCO_test2015_000000550459.h5\n",
    "train-1272-0-img1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to follow the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_inputs = (batch['vis_feats'], batch['boxes'])\n",
    "sqrt_size = int(36**0.5)\n",
    "output_size = (sqrt_size, sqrt_size)\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 2048])\n",
      "torch.Size([1, 2048, 49])\n",
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 2048, 6, 6])\n",
      "torch.Size([1, 2048, 36])\n",
      "torch.Size([1, 36, 2048])\n",
      "boxes :  torch.Size([1, 36, 4])\n"
     ]
    }
   ],
   "source": [
    "def downsample(inputs):\n",
    "        pool = torch.nn.AdaptiveMaxPool2d(6) # racine de 36\n",
    "        B, L, t = transform(image)\n",
    "dim = inputs.shape\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1) # (2B, dim, L/2)\n",
    "        print(inputs.shape)\n",
    "        sqrt_L = int(L ** 0.5)\n",
    "        inputs = inputs.reshape(B, dim, sqrt_L, sqrt_L)\n",
    "        print(inputs.shape)\n",
    "        inputs = pool(inputs)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.reshape(B, dim, -1)\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        print(inputs.shape)\n",
    "        return inputs\n",
    "\n",
    "inputs_tuple = vis_inputs\n",
    "inputs, boxes = inputs_tuple\n",
    "\n",
    "inputs = downsample(inputs)\n",
    "boxes = boxes[:, :inputs.shape[1]] # Get the first few data because the element are all zeros\n",
    "print('boxes : ',boxes.shape)\n",
    "outputs_tuple = (inputs, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area torch.Size([1, 36, 1])\n",
      "pos torch.Size([1, 36, 5])\n"
     ]
    }
   ],
   "source": [
    "def get_area(pos):\n",
    "    height = pos[:, :, 3] - pos[:, :, 2]\n",
    "    width = pos[:, :, 1] - pos[:, :, 0]\n",
    "    area = height * width\n",
    "    return area\n",
    "\n",
    "\n",
    "\n",
    "area = get_area(boxes).unsqueeze(2) # [B, N, 1]\n",
    "pos = torch.cat([boxes, area], dim=2) # [B, N, 5]\n",
    "print(\"area\", area.shape)\n",
    "print(\"pos\", pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.cat(torch.chunk(boxes[:,:36,:], 2, 1), 0)\n",
    "boxes = boxes[:, :output_normal.shape[1]//2]\n",
    "boxes = torch.cat(torch.chunk(boxes, 2, 0), 1)\n",
    "print(boxes.shape)\n",
    "36**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choix des dimensions en entré en utilisant commentaire (même si on ne veut vraiment se comparer car pas même tâche)\n",
    "\n",
    "Par exemple pooling pas forcémenet intéressant pour nous de le faire\n",
    "on peut peut être mieux comprendre l'image même si on risque de s'y perdre\n",
    "\n",
    "pour une faire comparaison vaut peut être mieux utiliser pooling (même dimension pour clip)\n",
    "\n",
    "```\n",
    "Input images are resized to 224 × 224\n",
    "for the memory efficiency. We extract the 7 × 7 grid fea-\n",
    "tures produced by the last convolutional layer, and then ap-\n",
    "ply adaptive maximum-pooling over the features for down-\n",
    "sampling then to 6 × 6 for a fair comparison to [7].\n",
    "```\n",
    "\n",
    "Dans leur cas n'utilise pas la dernière couche\n",
    "\n",
    "Même si on pourrait nous essayer de l'utiliser car peut être suffisant\n",
    "Peut être essayer les deux avec et sans pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans caption_raw_data.py\n",
    "# get item\n",
    "out_dict[\"image\"] = self.transform(image)\n",
    "\n",
    "out_dict['n_boxes'] = self.args.n_boxes\n",
    "\n",
    "# collate fn\n",
    "n_boxes = entry['n_boxes']\n",
    "images.append(entry['image'])\n",
    "\n",
    "batch_entry['images'] = torch.stack(images)\n",
    "# Mais ne retourne pas n_boxes donc juste batch images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch embedding image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    ")\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        # PadToSquare(),\n",
    "        Resize(n_px, interpolation=Image.Resampling.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        # MinMaxResize(*n_px),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def augmentation_transform(image_size):\n",
    "    return Compose([\n",
    "        Resize(image_size, interpolation=Image.Resampling.BICUBIC),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomCrop(image_size, padding=int(image_size[0]*0.0625), padding_mode='reflect'),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        RandomErasing(),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform = _transform(eval(\"(224,224)\")[0])\n",
    "transform2 = augmentation_transform(eval(\"(224,224)\"))\n",
    "\n",
    "path = \"/home/pgrimal/Documents/Projects/entity_image/data/Commons_wikimage/-%20Narcissus%20pseudonarcissus%2003%20-.jpg\"\n",
    "image = Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transform(image)\n",
    "images = [t,t,t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding fasterrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file data_model/frcnn_model/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3420/426814475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_model/frcnn_model/pytorch_model.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_frcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_img_preprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvisual_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_frcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/processing/embedding_image.py\u001b[0m in \u001b[0;36mload_frcnn\u001b[0;34m(config_path, model_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_frcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfrcnn_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mfrcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneralizedRCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrcnn_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrcnn_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/VLT5/inference/modeling_frcnn.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/visual_language_representation/cvlep/VLT5/inference/modeling_frcnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRes5ROIHeads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROIOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cvlp2/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "path_cfg= \"data_model/frcnn_model/config.yaml\"\n",
    "path_model=\"data_model/frcnn_model/pytorch_model.bin\"\n",
    "from processing.embedding_image import load_frcnn, load_img_preprocessor\n",
    "visual_model, cfg = load_frcnn(path_cfg, path_model)\n",
    "img_process = load_img_preprocessor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLT5 with only image or only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgrimal/miniconda3/envs/cvlp2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight'], unexpected_keys=['shared.weight', 'lm_head.weight'])\n",
      "_IncompatibleKeys(missing_keys=['visual_embedding.feat_embedding.0.weight', 'visual_embedding.feat_embedding.0.bias', 'visual_embedding.feat_embedding.1.weight', 'visual_embedding.absolute_vis_pos_embedding.0.weight', 'visual_embedding.absolute_vis_pos_embedding.0.bias', 'visual_embedding.absolute_vis_pos_embedding.1.weight', 'visual_embedding.obj_order_embedding.weight', 'visual_embedding.img_order_embedding.weight', 'projection.projection.weight'], unexpected_keys=['shared.weight', 'lm_head.weight'])\n",
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['projection.projection.weight'], unexpected_keys=[])\n",
      "Model loaded from  data_model/jointEncoder/state_dict/VLT5epoch30.pth\n",
      "_IncompatibleKeys(missing_keys=['projection.projection.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "import os\n",
    "from cvlep.VLT5.param import Config\n",
    "\n",
    "if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:  # torchrun launch\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "elif int(os.environ.get('SLURM_NPROCS', 1)) > 1:  # slurm launch\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
    "    world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "else:  # single gpu & process launch\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 0\n",
    "\n",
    "config_encoder_question = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_encoder_passage = \"experiments/config_vladapter/local/encoder_simple.json\"\n",
    "config_model = \"experiments/config_vladapter/local/config_model.json\"\n",
    "config_training = \"experiments/config_vladapter/local/training_simple_adapter.json\"\n",
    "\n",
    "# Training config\n",
    "config_training = Config.load_json(config_training)\n",
    "config_training.world_size = world_size\n",
    "config_training.rank = rank\n",
    "config_training.local_rank = local_rank\n",
    "if world_size > 1:\n",
    "    config_training.distributed = True\n",
    "    config_training.multiGPU = True\n",
    "else:\n",
    "    config_training.distributed = False\n",
    "    config_training.multiGPU = False\n",
    "\n",
    "config_encoder_question = Config.load_json(config_encoder_question)\n",
    "config_encoder_passage = Config.load_json(config_encoder_passage)\n",
    "config_model = Config.load_json(config_model)\n",
    "\n",
    "from cvlep.trainer_base_vladapter import Trainer\n",
    "\n",
    "trainer = Trainer(config_encoder_question,config_encoder_passage, config_model, config_training, train=False, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 729kB/s]\n",
      "Downloading: 100%|██████████| 892M/892M [00:13<00:00, 68.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "\n",
    "    \n",
    "#download and save T5\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "model.save_pretrained('data_model/t5forconditionnalgeneration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointEncoder(\n",
       "  (embed_tokens): Embedding(32200, 768)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (visual_embedding): VisualEmbedding(\n",
       "    (feat_embedding): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (1): T5LayerNorm()\n",
       "    )\n",
       "    (absolute_vis_pos_embedding): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (1): T5LayerNorm()\n",
       "    )\n",
       "    (obj_order_embedding): Embedding(32200, 768)\n",
       "    (img_order_embedding): Embedding(2, 768)\n",
       "  )\n",
       "  (projection): ProjectionHead(\n",
       "    (projection): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (activation_function): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.image_passage_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_from_disk, disable_caching\n",
    "disable_caching()\n",
    "path_dataset = \"data_model/dataset/small_dataset\"\n",
    "dataset = load_from_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "import torch\n",
    "from cvlep.utils import device\n",
    "index = 1\n",
    "item = dataset[1]\n",
    "key_vision_features = \"vlt5_features\"\n",
    "key_boxes = \"vlt5_normalized_boxes\"\n",
    "key_text=\"input\"\n",
    "tokenizer = trainer.tokenizer_question\n",
    "\n",
    "vision_features = torch.Tensor(item[key_vision_features]).to(device)\n",
    "boxes = torch.Tensor(item[key_boxes]).to(device)\n",
    "vision_features = torch.squeeze(vision_features, dim=1)\n",
    "vis_inputs = (vision_features,boxes)\n",
    "boxes = torch.squeeze(boxes, dim=2)\n",
    "input_ids = tokenizer(\n",
    "    item[key_text], return_tensors='pt', padding=True, truncation=True)\n",
    "return_pooled_output=True,\n",
    "pool_strategy=\"avg\"\n",
    "print(input_ids.input_ids.size())\n",
    "print(vision_features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch.update(input_ids = input_ids.input_ids.to(device),\n",
    "    attention_mask = input_ids.attention_mask.to(device),\n",
    "    vis_inputs = vis_inputs,\n",
    "    pool_strategy = pool_strategy,\n",
    "    return_pooled_output = return_pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.encoder_question.eval()\n",
    "output = trainer.encoder_question(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "\n",
    "        vis_inputs=(None,None),\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1837, -0.1131, -0.1383,  ...,  0.1381, -0.0979,  0.4198],\n",
       "         [-0.3265, -0.2219, -0.3252,  ..., -0.1107, -0.0176,  0.1044],\n",
       "         [-0.3971, -0.1212, -0.3707,  ..., -0.1000,  0.0895,  0.0319],\n",
       "         ...,\n",
       "         [-0.1491, -0.0147, -0.0670,  ..., -0.2671,  0.2634,  0.3088],\n",
       "         [ 0.3494,  0.0269, -0.0432,  ..., -0.2269, -0.1812, -0.1007],\n",
       "         [ 0.0186,  0.0036,  0.0015,  ..., -0.0018, -0.0079,  0.0117]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 3.1076e-02,  4.0952e-02, -3.5525e-02,  2.8262e-03, -3.6988e-02,\n",
       "         -3.7665e-03, -1.5376e-02,  8.9196e-03, -7.2722e-02, -2.7182e-02,\n",
       "         -5.2511e-02,  9.6131e-03, -5.5457e-03,  2.2509e-02,  1.3012e-03,\n",
       "          2.6011e-02, -2.3566e-02,  9.7976e-03,  3.5896e-02,  5.7960e-03,\n",
       "         -1.0628e-02, -5.3183e-02,  2.1509e-02, -2.2279e-02,  5.9828e-03,\n",
       "          1.5714e-02, -2.8105e-03, -1.2101e-02,  1.7965e-03, -4.7683e-05,\n",
       "         -7.4251e-03,  1.5017e-03, -3.3616e-02,  5.2707e-02,  5.1988e-02,\n",
       "         -3.1571e-02,  2.5431e-02, -2.8651e-02, -2.4637e-02,  1.6383e-02,\n",
       "          2.7427e-02,  8.3541e-02, -3.0389e-02, -2.5368e-02,  3.6865e-02,\n",
       "          3.1402e-02,  7.0555e-02, -3.3257e-02, -2.2094e-02, -5.3222e-02,\n",
       "          3.7467e-02,  4.4310e-02, -1.1672e-02,  4.5239e-02,  2.7329e-02,\n",
       "          2.3315e-03, -4.1664e-02,  1.4883e-02,  2.6715e-02, -1.1130e-02,\n",
       "         -8.6617e-02,  2.3956e-02, -2.5703e-02,  2.1394e-02, -2.4922e-02,\n",
       "         -4.1275e-02, -4.6918e-02, -4.5754e-02,  4.2321e-02, -1.3988e-02,\n",
       "         -7.4593e-02,  5.4012e-03,  1.7271e-02,  3.0113e-02, -3.0584e-02,\n",
       "         -7.1733e-02,  4.8660e-02,  5.9403e-03,  4.0889e-02, -6.0752e-03,\n",
       "         -4.0306e-02, -2.9025e-03,  1.8583e-02,  1.7893e-02,  1.0128e-02,\n",
       "         -1.0717e-02, -2.2649e-02, -8.3794e-03,  2.3814e-02, -6.0437e-02,\n",
       "         -1.8666e-02, -2.5070e-03,  4.1922e-02,  3.9789e-02, -2.1031e-02,\n",
       "         -3.6851e-02,  1.6847e-02,  7.8962e-02, -2.2658e-02, -5.5095e-02,\n",
       "          1.1283e-02, -2.1489e-02, -2.7920e-02, -2.2077e-03,  1.6158e-02,\n",
       "          1.1799e-02, -8.4642e-03,  1.2886e-02, -5.4224e-02, -9.6394e-04,\n",
       "         -4.5834e-02, -4.4359e-02, -3.1510e-02,  1.8336e-03, -3.0537e-02,\n",
       "         -2.0157e-02, -4.5722e-02, -3.7332e-02, -4.1700e-02, -4.0771e-03,\n",
       "         -3.9966e-03,  1.2137e-02, -9.3475e-02, -4.5585e-02,  1.0998e-02,\n",
       "          1.8460e-02, -1.1907e-02, -2.1402e-02, -1.1987e-02, -8.1087e-02,\n",
       "         -5.8804e-02, -6.3390e-02, -4.0586e-03, -3.6891e-02, -3.1658e-02,\n",
       "         -4.2582e-03,  4.7059e-02,  9.2046e-02,  8.8790e-03, -6.5737e-04,\n",
       "          3.7876e-02,  7.8433e-02, -6.6948e-02, -4.0644e-03,  3.2215e-02,\n",
       "          3.3965e-03, -5.3545e-02, -4.4457e-03,  1.6112e-03,  1.6590e-02,\n",
       "          2.9548e-02,  3.8782e-02, -3.1546e-02,  3.8410e-02,  4.2408e-02,\n",
       "          1.5977e-02,  1.3348e-02, -1.1850e-03,  9.8465e-03,  3.8683e-02,\n",
       "          1.9153e-02,  3.8789e-02,  5.1743e-02, -2.4279e-02,  3.5370e-03,\n",
       "         -5.7817e-02, -5.4336e-02, -4.3877e-02,  5.7030e-02,  2.0424e-02,\n",
       "          5.4842e-02,  1.7593e-02,  6.0844e-02, -3.7414e-02,  5.1035e-02,\n",
       "          6.4245e-02,  2.7228e-02, -9.5300e-04, -1.6867e-02, -2.1598e-02,\n",
       "         -8.3982e-02,  2.5811e-02,  4.7513e-02, -1.2071e-02, -6.7354e-02,\n",
       "          3.0766e-02, -3.5740e-02, -2.9413e-02, -2.2502e-02, -2.3300e-02,\n",
       "         -1.5945e-02, -3.8525e-02, -5.7299e-02,  5.3904e-03, -6.9013e-03,\n",
       "         -5.4762e-02, -5.0342e-03, -4.6948e-02,  1.1841e-02, -3.3842e-02,\n",
       "          3.0071e-02, -4.6260e-02, -7.0527e-02, -2.7365e-02,  3.2164e-02,\n",
       "          2.6377e-02,  3.5940e-02, -5.2209e-03,  6.0580e-02, -2.8508e-02,\n",
       "         -2.6659e-02,  5.2053e-02, -4.3045e-02,  3.4004e-02, -6.1436e-02,\n",
       "          4.0715e-02, -1.7313e-02, -4.6292e-03, -1.4719e-02, -1.0231e-02,\n",
       "          5.1822e-03,  6.9090e-02, -4.2609e-02,  1.1242e-02, -4.6797e-02,\n",
       "          2.2487e-02,  3.1967e-02, -1.7605e-02,  1.8832e-03,  2.9714e-02,\n",
       "         -4.8273e-02, -3.2053e-02,  4.4440e-02,  1.6587e-02,  8.9498e-03,\n",
       "         -6.8641e-05,  9.2655e-03,  1.6417e-02,  2.6384e-03, -1.4748e-02,\n",
       "          7.0920e-02,  8.9449e-03, -2.6963e-02, -1.8960e-02,  2.1011e-03,\n",
       "         -2.1669e-02,  1.3601e-02,  5.4157e-02,  6.1657e-02, -3.2274e-02,\n",
       "         -1.8906e-02, -1.4677e-02,  5.3911e-02, -4.1023e-02, -4.5498e-02,\n",
       "          4.8237e-02,  2.9554e-02,  1.0403e-02, -2.6047e-02, -7.6965e-04,\n",
       "          3.3599e-02,  1.2746e-02, -9.4240e-03,  4.6262e-02, -1.0921e-02,\n",
       "          7.5955e-03, -4.5919e-03, -1.5899e-02, -3.1940e-02,  3.6398e-02,\n",
       "         -2.3237e-02,  5.6950e-03, -5.3165e-02,  3.7555e-02,  1.1184e-02,\n",
       "         -3.6243e-02, -2.4911e-02, -2.0246e-03, -3.3931e-02,  6.3350e-02,\n",
       "         -4.5873e-02,  4.8055e-03,  8.1553e-03, -1.4326e-02, -2.5368e-02,\n",
       "          2.9670e-02, -3.5362e-03,  6.4801e-02,  5.0642e-02,  1.5827e-02,\n",
       "          7.5804e-02,  1.9094e-02,  2.7268e-02, -4.6365e-02,  3.2030e-02,\n",
       "          1.7832e-02,  2.4445e-02,  3.8826e-02,  9.7644e-03, -3.4343e-02,\n",
       "          3.5973e-02,  1.5042e-02,  4.4994e-02, -1.3524e-02, -1.5837e-02,\n",
       "         -5.4331e-02, -3.4077e-02,  4.1541e-02, -1.7812e-02,  8.1400e-03,\n",
       "         -4.1743e-02,  7.2351e-02, -5.6643e-02, -1.1169e-02,  3.1626e-02,\n",
       "          4.6955e-02,  4.2511e-02,  2.0369e-02, -1.7426e-02, -1.6481e-02,\n",
       "         -6.3463e-03, -2.0572e-02,  3.5915e-02,  6.3792e-02, -7.2726e-04,\n",
       "         -9.0612e-03,  7.4850e-02, -8.7069e-02, -4.2987e-02,  3.8187e-02,\n",
       "         -1.8416e-02,  9.2268e-03, -2.9571e-02, -1.3321e-02,  4.9924e-02,\n",
       "         -1.4744e-02,  5.8177e-02,  4.1931e-02, -4.3657e-02, -4.8900e-02,\n",
       "          4.6772e-02, -4.9935e-03,  2.6618e-02,  6.8313e-03,  2.1480e-02,\n",
       "         -3.1344e-02, -3.6008e-02, -2.2074e-02,  1.6451e-02, -2.1881e-02,\n",
       "         -3.4180e-02,  4.6145e-03,  2.4089e-03, -1.8532e-02,  4.7456e-02,\n",
       "          7.5310e-03,  5.9439e-02, -3.5949e-02, -1.2058e-02, -1.2402e-02,\n",
       "          4.0902e-03, -1.2708e-02,  2.4374e-02, -1.3269e-02,  1.8148e-03,\n",
       "         -2.7215e-02,  1.9412e-02,  3.8621e-02, -8.4246e-03, -3.3460e-02,\n",
       "         -1.0380e-02, -4.8382e-03, -3.6439e-02, -2.4644e-02,  5.4737e-02,\n",
       "          3.9982e-02,  3.6330e-02,  2.7619e-02, -4.8614e-03, -9.4802e-03,\n",
       "          3.5907e-02,  4.1771e-02,  5.2084e-02,  4.9378e-02, -1.6243e-02,\n",
       "         -4.5024e-02,  3.2323e-02,  2.2822e-03, -4.7496e-02,  1.1609e-01,\n",
       "          3.7902e-02,  1.4954e-02,  3.7784e-02,  4.5572e-03, -3.9679e-02,\n",
       "         -3.0466e-02, -2.3960e-02,  4.4459e-04,  5.1755e-03, -3.6149e-02,\n",
       "         -3.6010e-02,  5.0001e-02, -2.6774e-02,  4.4649e-02,  7.1252e-03,\n",
       "         -3.1789e-03,  4.2865e-02,  6.9888e-02,  2.4574e-02,  2.1734e-02,\n",
       "          3.2443e-02, -6.7218e-02, -6.6369e-03, -8.2367e-02, -6.9490e-02,\n",
       "          4.1763e-02, -5.0843e-02,  3.0804e-02, -1.3746e-02, -2.4166e-02,\n",
       "         -5.1297e-02, -2.6817e-02,  3.1370e-03, -5.9239e-02, -2.5867e-02,\n",
       "          2.2689e-02,  1.1804e-02, -8.4060e-03,  1.8412e-02, -4.7347e-02,\n",
       "         -3.5712e-03,  2.1391e-02, -4.1593e-02, -7.8576e-02,  6.0176e-03,\n",
       "          1.7836e-02, -3.8013e-02, -3.8242e-02,  2.4055e-03,  1.9542e-02,\n",
       "          2.4312e-02, -1.8552e-02,  2.3284e-02, -6.3642e-02, -5.9421e-02,\n",
       "         -8.3262e-02, -5.1917e-02,  1.3602e-02, -8.6873e-02,  2.0789e-02,\n",
       "         -2.1951e-03,  6.8039e-02, -4.9510e-02,  2.2459e-02,  2.3835e-02,\n",
       "         -3.2886e-03,  6.4524e-02, -1.6635e-03, -6.8827e-04,  5.3921e-02,\n",
       "          2.1154e-02,  2.8423e-03, -1.2776e-02, -2.3794e-02, -6.4849e-02,\n",
       "          7.1697e-02, -1.9355e-02,  2.0678e-02, -6.5721e-03, -2.0609e-02,\n",
       "         -2.3952e-02, -6.7754e-02,  2.7758e-03, -2.6362e-03,  1.2337e-02,\n",
       "         -1.6388e-03,  1.8660e-02, -1.0357e-02, -3.6384e-02,  1.0104e-02,\n",
       "          5.2996e-03, -1.3731e-02,  3.5758e-03,  1.2789e-02, -4.3672e-02,\n",
       "         -2.3831e-02, -5.2332e-02,  1.7601e-02,  4.5654e-02,  6.0581e-02,\n",
       "          3.7738e-02,  7.5013e-02,  3.5575e-02, -3.5020e-02, -2.2229e-03,\n",
       "         -7.3353e-03, -4.2906e-02, -6.6230e-03, -3.3250e-02, -6.6068e-03,\n",
       "         -9.8040e-03, -3.5305e-03, -1.0190e-02, -2.0701e-02,  2.4180e-03,\n",
       "         -1.8743e-03, -4.6850e-02,  6.3266e-04, -1.3501e-02, -3.6113e-02,\n",
       "          1.6401e-02,  3.3113e-02, -3.0767e-02,  2.4937e-02,  4.6129e-02,\n",
       "          2.9578e-02,  5.0118e-02,  8.3672e-02,  1.7297e-02,  7.0042e-02,\n",
       "         -1.3667e-02, -4.5010e-02, -1.6665e-02,  2.8052e-02, -7.1951e-03,\n",
       "          1.8203e-03, -1.6512e-02,  3.2137e-02,  1.9477e-02, -1.8886e-02,\n",
       "         -7.8755e-02, -1.3231e-02,  6.1968e-03, -3.5785e-02,  6.6870e-03,\n",
       "          3.3198e-02, -5.1723e-03, -5.2393e-04, -9.8310e-03, -8.7881e-03,\n",
       "         -4.8240e-02, -4.8436e-02, -5.3570e-03,  6.9803e-03,  5.2250e-02,\n",
       "          1.8371e-02, -3.0948e-02,  3.3500e-02,  3.1250e-02,  6.8026e-02,\n",
       "          1.0053e-01,  2.5263e-02, -1.4293e-02,  1.1879e-02, -1.1002e-02,\n",
       "          2.4716e-02,  2.0221e-02, -1.8656e-02,  2.9996e-04, -5.2470e-02,\n",
       "         -3.3394e-02,  2.7430e-02, -3.7066e-02,  1.0923e-02, -8.1992e-02,\n",
       "          5.7077e-03, -7.2665e-02,  1.7019e-02,  2.9501e-02,  7.0267e-03,\n",
       "          5.4089e-02,  1.9116e-02,  4.5675e-02, -4.5773e-04, -4.3515e-02,\n",
       "          2.3488e-02,  5.3497e-02, -3.9397e-02,  2.2194e-02, -5.3000e-02,\n",
       "         -8.8903e-03,  2.4079e-02, -1.4019e-02,  6.6247e-02,  4.2537e-03,\n",
       "         -2.3802e-02, -1.7372e-02, -2.7146e-03,  4.6904e-02,  4.2506e-02,\n",
       "          6.8520e-03, -1.1349e-02,  4.6769e-02,  4.9382e-02,  4.5297e-02,\n",
       "         -5.0731e-02,  1.0408e-01,  3.6664e-02, -1.8554e-02,  7.5874e-03,\n",
       "         -3.3396e-02,  5.5735e-02, -5.2918e-02,  5.5982e-02, -2.4583e-02,\n",
       "          7.5801e-02,  5.3600e-02, -7.1831e-03,  1.9582e-02, -4.4576e-02,\n",
       "         -1.0280e-02, -6.0165e-04, -3.4103e-02, -6.0360e-02,  4.8171e-02,\n",
       "          3.3792e-02,  2.7355e-02, -1.6527e-02, -2.9219e-02,  4.5121e-03,\n",
       "          3.0088e-02, -1.1927e-02, -3.0286e-03, -2.6488e-03, -3.2543e-02,\n",
       "         -3.0537e-03,  1.3217e-02, -1.9615e-02, -2.0013e-02, -2.5314e-02,\n",
       "         -2.6106e-02,  1.5721e-02, -3.7469e-03,  5.0616e-02,  5.0653e-02,\n",
       "          2.4627e-02,  8.4346e-02,  7.0482e-03,  1.0446e-03, -3.1975e-02,\n",
       "         -5.5662e-02,  3.5648e-03, -4.3138e-03,  4.0741e-02,  2.9199e-02,\n",
       "         -4.8046e-02, -4.0720e-02, -5.4493e-03,  5.1702e-03, -3.6056e-03,\n",
       "          4.0438e-02,  5.7988e-02, -4.5940e-02,  3.4645e-02, -6.9635e-02,\n",
       "          6.0825e-02,  2.5606e-02,  8.0196e-04,  1.7650e-02, -2.1830e-02,\n",
       "         -4.4601e-02, -6.7156e-02, -1.1347e-02, -3.3444e-02,  9.0831e-03,\n",
       "          4.0737e-02, -6.5244e-02, -1.7177e-02, -2.5905e-02, -6.1166e-03,\n",
       "         -3.3916e-02,  8.3732e-04, -6.7717e-03, -7.3469e-02, -7.7076e-02,\n",
       "          1.0634e-02,  3.1727e-02, -6.4175e-03,  1.4050e-02,  4.3373e-02,\n",
       "          1.0533e-02,  3.7544e-02, -6.2736e-03, -2.5703e-02, -4.9138e-02,\n",
       "         -4.2963e-03,  5.1083e-02,  2.4160e-02, -4.7603e-03,  3.6301e-03,\n",
       "          5.2913e-02,  2.7058e-03,  5.3587e-03,  2.2324e-02, -1.6435e-02,\n",
       "          3.9436e-02,  5.0491e-02,  8.8427e-03, -3.6444e-02,  6.2750e-02,\n",
       "         -1.2429e-03,  1.0602e-02, -1.5016e-02,  5.9311e-03, -3.4697e-02,\n",
       "         -7.0416e-02,  2.4519e-02, -2.1112e-02, -2.7250e-02, -2.5324e-02,\n",
       "          2.0280e-02,  1.6790e-02,  2.7234e-02, -7.7265e-02,  1.1304e-02,\n",
       "          2.9310e-02, -5.3218e-02,  7.1870e-03,  2.2235e-02,  3.0863e-02,\n",
       "         -1.7449e-02,  6.5035e-03, -6.9410e-02, -4.6167e-02,  2.5737e-02,\n",
       "          1.9029e-03,  2.7175e-02, -2.1034e-02,  1.1156e-02,  1.9104e-02,\n",
       "         -1.4924e-03,  1.9229e-02,  2.5952e-02, -6.9570e-03, -3.3875e-02,\n",
       "          2.5969e-02, -3.0974e-02,  3.5701e-02,  6.2056e-02, -2.4639e-02,\n",
       "         -6.0354e-02,  1.5684e-03, -4.3925e-02, -5.4687e-02,  3.7836e-02,\n",
       "         -5.2466e-02,  1.1239e-02, -3.9780e-02, -1.0517e-02, -7.3567e-02,\n",
       "          1.5888e-02, -8.8965e-03,  6.3358e-02, -7.9624e-03,  1.7485e-02,\n",
       "         -1.3744e-02, -2.6550e-02,  6.3432e-02,  8.4408e-03,  4.1818e-02,\n",
       "         -8.4470e-03, -2.4950e-02,  6.0329e-02, -2.0293e-03,  2.0118e-02,\n",
       "         -5.4960e-02,  2.3652e-02, -3.7304e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1837, -0.1131, -0.1383,  ...,  0.1381, -0.0979,  0.4198],\n",
       "         [-0.3265, -0.2219, -0.3252,  ..., -0.1107, -0.0176,  0.1044],\n",
       "         [-0.3971, -0.1212, -0.3707,  ..., -0.1000,  0.0895,  0.0319],\n",
       "         ...,\n",
       "         [-0.1491, -0.0147, -0.0670,  ..., -0.2671,  0.2634,  0.3088],\n",
       "         [ 0.3494,  0.0269, -0.0432,  ..., -0.2269, -0.1812, -0.1007],\n",
       "         [ 0.0186,  0.0036,  0.0015,  ..., -0.0018, -0.0079,  0.0117]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 3.1076e-02,  4.0952e-02, -3.5525e-02,  2.8262e-03, -3.6988e-02,\n",
       "         -3.7665e-03, -1.5376e-02,  8.9196e-03, -7.2722e-02, -2.7182e-02,\n",
       "         -5.2511e-02,  9.6131e-03, -5.5457e-03,  2.2509e-02,  1.3012e-03,\n",
       "          2.6011e-02, -2.3566e-02,  9.7976e-03,  3.5896e-02,  5.7960e-03,\n",
       "         -1.0628e-02, -5.3183e-02,  2.1509e-02, -2.2279e-02,  5.9828e-03,\n",
       "          1.5714e-02, -2.8105e-03, -1.2101e-02,  1.7965e-03, -4.7683e-05,\n",
       "         -7.4251e-03,  1.5017e-03, -3.3616e-02,  5.2707e-02,  5.1988e-02,\n",
       "         -3.1571e-02,  2.5431e-02, -2.8651e-02, -2.4637e-02,  1.6383e-02,\n",
       "          2.7427e-02,  8.3541e-02, -3.0389e-02, -2.5368e-02,  3.6865e-02,\n",
       "          3.1402e-02,  7.0555e-02, -3.3257e-02, -2.2094e-02, -5.3222e-02,\n",
       "          3.7467e-02,  4.4310e-02, -1.1672e-02,  4.5239e-02,  2.7329e-02,\n",
       "          2.3315e-03, -4.1664e-02,  1.4883e-02,  2.6715e-02, -1.1130e-02,\n",
       "         -8.6617e-02,  2.3956e-02, -2.5703e-02,  2.1394e-02, -2.4922e-02,\n",
       "         -4.1275e-02, -4.6918e-02, -4.5754e-02,  4.2321e-02, -1.3988e-02,\n",
       "         -7.4593e-02,  5.4012e-03,  1.7271e-02,  3.0113e-02, -3.0584e-02,\n",
       "         -7.1733e-02,  4.8660e-02,  5.9403e-03,  4.0889e-02, -6.0752e-03,\n",
       "         -4.0306e-02, -2.9025e-03,  1.8583e-02,  1.7893e-02,  1.0128e-02,\n",
       "         -1.0717e-02, -2.2649e-02, -8.3794e-03,  2.3814e-02, -6.0437e-02,\n",
       "         -1.8666e-02, -2.5070e-03,  4.1922e-02,  3.9789e-02, -2.1031e-02,\n",
       "         -3.6851e-02,  1.6847e-02,  7.8962e-02, -2.2658e-02, -5.5095e-02,\n",
       "          1.1283e-02, -2.1489e-02, -2.7920e-02, -2.2077e-03,  1.6158e-02,\n",
       "          1.1799e-02, -8.4642e-03,  1.2886e-02, -5.4224e-02, -9.6394e-04,\n",
       "         -4.5834e-02, -4.4359e-02, -3.1510e-02,  1.8336e-03, -3.0537e-02,\n",
       "         -2.0157e-02, -4.5722e-02, -3.7332e-02, -4.1700e-02, -4.0771e-03,\n",
       "         -3.9966e-03,  1.2137e-02, -9.3475e-02, -4.5585e-02,  1.0998e-02,\n",
       "          1.8460e-02, -1.1907e-02, -2.1402e-02, -1.1987e-02, -8.1087e-02,\n",
       "         -5.8804e-02, -6.3390e-02, -4.0586e-03, -3.6891e-02, -3.1658e-02,\n",
       "         -4.2582e-03,  4.7059e-02,  9.2046e-02,  8.8790e-03, -6.5737e-04,\n",
       "          3.7876e-02,  7.8433e-02, -6.6948e-02, -4.0644e-03,  3.2215e-02,\n",
       "          3.3965e-03, -5.3545e-02, -4.4457e-03,  1.6112e-03,  1.6590e-02,\n",
       "          2.9548e-02,  3.8782e-02, -3.1546e-02,  3.8410e-02,  4.2408e-02,\n",
       "          1.5977e-02,  1.3348e-02, -1.1850e-03,  9.8465e-03,  3.8683e-02,\n",
       "          1.9153e-02,  3.8789e-02,  5.1743e-02, -2.4279e-02,  3.5370e-03,\n",
       "         -5.7817e-02, -5.4336e-02, -4.3877e-02,  5.7030e-02,  2.0424e-02,\n",
       "          5.4842e-02,  1.7593e-02,  6.0844e-02, -3.7414e-02,  5.1035e-02,\n",
       "          6.4245e-02,  2.7228e-02, -9.5300e-04, -1.6867e-02, -2.1598e-02,\n",
       "         -8.3982e-02,  2.5811e-02,  4.7513e-02, -1.2071e-02, -6.7354e-02,\n",
       "          3.0766e-02, -3.5740e-02, -2.9413e-02, -2.2502e-02, -2.3300e-02,\n",
       "         -1.5945e-02, -3.8525e-02, -5.7299e-02,  5.3904e-03, -6.9013e-03,\n",
       "         -5.4762e-02, -5.0342e-03, -4.6948e-02,  1.1841e-02, -3.3842e-02,\n",
       "          3.0071e-02, -4.6260e-02, -7.0527e-02, -2.7365e-02,  3.2164e-02,\n",
       "          2.6377e-02,  3.5940e-02, -5.2209e-03,  6.0580e-02, -2.8508e-02,\n",
       "         -2.6659e-02,  5.2053e-02, -4.3045e-02,  3.4004e-02, -6.1436e-02,\n",
       "          4.0715e-02, -1.7313e-02, -4.6292e-03, -1.4719e-02, -1.0231e-02,\n",
       "          5.1822e-03,  6.9090e-02, -4.2609e-02,  1.1242e-02, -4.6797e-02,\n",
       "          2.2487e-02,  3.1967e-02, -1.7605e-02,  1.8832e-03,  2.9714e-02,\n",
       "         -4.8273e-02, -3.2053e-02,  4.4440e-02,  1.6587e-02,  8.9498e-03,\n",
       "         -6.8641e-05,  9.2655e-03,  1.6417e-02,  2.6384e-03, -1.4748e-02,\n",
       "          7.0920e-02,  8.9449e-03, -2.6963e-02, -1.8960e-02,  2.1011e-03,\n",
       "         -2.1669e-02,  1.3601e-02,  5.4157e-02,  6.1657e-02, -3.2274e-02,\n",
       "         -1.8906e-02, -1.4677e-02,  5.3911e-02, -4.1023e-02, -4.5498e-02,\n",
       "          4.8237e-02,  2.9554e-02,  1.0403e-02, -2.6047e-02, -7.6965e-04,\n",
       "          3.3599e-02,  1.2746e-02, -9.4240e-03,  4.6262e-02, -1.0921e-02,\n",
       "          7.5955e-03, -4.5919e-03, -1.5899e-02, -3.1940e-02,  3.6398e-02,\n",
       "         -2.3237e-02,  5.6950e-03, -5.3165e-02,  3.7555e-02,  1.1184e-02,\n",
       "         -3.6243e-02, -2.4911e-02, -2.0246e-03, -3.3931e-02,  6.3350e-02,\n",
       "         -4.5873e-02,  4.8055e-03,  8.1553e-03, -1.4326e-02, -2.5368e-02,\n",
       "          2.9670e-02, -3.5362e-03,  6.4801e-02,  5.0642e-02,  1.5827e-02,\n",
       "          7.5804e-02,  1.9094e-02,  2.7268e-02, -4.6365e-02,  3.2030e-02,\n",
       "          1.7832e-02,  2.4445e-02,  3.8826e-02,  9.7644e-03, -3.4343e-02,\n",
       "          3.5973e-02,  1.5042e-02,  4.4994e-02, -1.3524e-02, -1.5837e-02,\n",
       "         -5.4331e-02, -3.4077e-02,  4.1541e-02, -1.7812e-02,  8.1400e-03,\n",
       "         -4.1743e-02,  7.2351e-02, -5.6643e-02, -1.1169e-02,  3.1626e-02,\n",
       "          4.6955e-02,  4.2511e-02,  2.0369e-02, -1.7426e-02, -1.6481e-02,\n",
       "         -6.3463e-03, -2.0572e-02,  3.5915e-02,  6.3792e-02, -7.2726e-04,\n",
       "         -9.0612e-03,  7.4850e-02, -8.7069e-02, -4.2987e-02,  3.8187e-02,\n",
       "         -1.8416e-02,  9.2268e-03, -2.9571e-02, -1.3321e-02,  4.9924e-02,\n",
       "         -1.4744e-02,  5.8177e-02,  4.1931e-02, -4.3657e-02, -4.8900e-02,\n",
       "          4.6772e-02, -4.9935e-03,  2.6618e-02,  6.8313e-03,  2.1480e-02,\n",
       "         -3.1344e-02, -3.6008e-02, -2.2074e-02,  1.6451e-02, -2.1881e-02,\n",
       "         -3.4180e-02,  4.6145e-03,  2.4089e-03, -1.8532e-02,  4.7456e-02,\n",
       "          7.5310e-03,  5.9439e-02, -3.5949e-02, -1.2058e-02, -1.2402e-02,\n",
       "          4.0902e-03, -1.2708e-02,  2.4374e-02, -1.3269e-02,  1.8148e-03,\n",
       "         -2.7215e-02,  1.9412e-02,  3.8621e-02, -8.4246e-03, -3.3460e-02,\n",
       "         -1.0380e-02, -4.8382e-03, -3.6439e-02, -2.4644e-02,  5.4737e-02,\n",
       "          3.9982e-02,  3.6330e-02,  2.7619e-02, -4.8614e-03, -9.4802e-03,\n",
       "          3.5907e-02,  4.1771e-02,  5.2084e-02,  4.9378e-02, -1.6243e-02,\n",
       "         -4.5024e-02,  3.2323e-02,  2.2822e-03, -4.7496e-02,  1.1609e-01,\n",
       "          3.7902e-02,  1.4954e-02,  3.7784e-02,  4.5572e-03, -3.9679e-02,\n",
       "         -3.0466e-02, -2.3960e-02,  4.4459e-04,  5.1755e-03, -3.6149e-02,\n",
       "         -3.6010e-02,  5.0001e-02, -2.6774e-02,  4.4649e-02,  7.1252e-03,\n",
       "         -3.1789e-03,  4.2865e-02,  6.9888e-02,  2.4574e-02,  2.1734e-02,\n",
       "          3.2443e-02, -6.7218e-02, -6.6369e-03, -8.2367e-02, -6.9490e-02,\n",
       "          4.1763e-02, -5.0843e-02,  3.0804e-02, -1.3746e-02, -2.4166e-02,\n",
       "         -5.1297e-02, -2.6817e-02,  3.1370e-03, -5.9239e-02, -2.5867e-02,\n",
       "          2.2689e-02,  1.1804e-02, -8.4060e-03,  1.8412e-02, -4.7347e-02,\n",
       "         -3.5712e-03,  2.1391e-02, -4.1593e-02, -7.8576e-02,  6.0176e-03,\n",
       "          1.7836e-02, -3.8013e-02, -3.8242e-02,  2.4055e-03,  1.9542e-02,\n",
       "          2.4312e-02, -1.8552e-02,  2.3284e-02, -6.3642e-02, -5.9421e-02,\n",
       "         -8.3262e-02, -5.1917e-02,  1.3602e-02, -8.6873e-02,  2.0789e-02,\n",
       "         -2.1951e-03,  6.8039e-02, -4.9510e-02,  2.2459e-02,  2.3835e-02,\n",
       "         -3.2886e-03,  6.4524e-02, -1.6635e-03, -6.8827e-04,  5.3921e-02,\n",
       "          2.1154e-02,  2.8423e-03, -1.2776e-02, -2.3794e-02, -6.4849e-02,\n",
       "          7.1697e-02, -1.9355e-02,  2.0678e-02, -6.5721e-03, -2.0609e-02,\n",
       "         -2.3952e-02, -6.7754e-02,  2.7758e-03, -2.6362e-03,  1.2337e-02,\n",
       "         -1.6388e-03,  1.8660e-02, -1.0357e-02, -3.6384e-02,  1.0104e-02,\n",
       "          5.2996e-03, -1.3731e-02,  3.5758e-03,  1.2789e-02, -4.3672e-02,\n",
       "         -2.3831e-02, -5.2332e-02,  1.7601e-02,  4.5654e-02,  6.0581e-02,\n",
       "          3.7738e-02,  7.5013e-02,  3.5575e-02, -3.5020e-02, -2.2229e-03,\n",
       "         -7.3353e-03, -4.2906e-02, -6.6230e-03, -3.3250e-02, -6.6068e-03,\n",
       "         -9.8040e-03, -3.5305e-03, -1.0190e-02, -2.0701e-02,  2.4180e-03,\n",
       "         -1.8743e-03, -4.6850e-02,  6.3266e-04, -1.3501e-02, -3.6113e-02,\n",
       "          1.6401e-02,  3.3113e-02, -3.0767e-02,  2.4937e-02,  4.6129e-02,\n",
       "          2.9578e-02,  5.0118e-02,  8.3672e-02,  1.7297e-02,  7.0042e-02,\n",
       "         -1.3667e-02, -4.5010e-02, -1.6665e-02,  2.8052e-02, -7.1951e-03,\n",
       "          1.8203e-03, -1.6512e-02,  3.2137e-02,  1.9477e-02, -1.8886e-02,\n",
       "         -7.8755e-02, -1.3231e-02,  6.1968e-03, -3.5785e-02,  6.6870e-03,\n",
       "          3.3198e-02, -5.1723e-03, -5.2393e-04, -9.8310e-03, -8.7881e-03,\n",
       "         -4.8240e-02, -4.8436e-02, -5.3570e-03,  6.9803e-03,  5.2250e-02,\n",
       "          1.8371e-02, -3.0948e-02,  3.3500e-02,  3.1250e-02,  6.8026e-02,\n",
       "          1.0053e-01,  2.5263e-02, -1.4293e-02,  1.1879e-02, -1.1002e-02,\n",
       "          2.4716e-02,  2.0221e-02, -1.8656e-02,  2.9996e-04, -5.2470e-02,\n",
       "         -3.3394e-02,  2.7430e-02, -3.7066e-02,  1.0923e-02, -8.1992e-02,\n",
       "          5.7077e-03, -7.2665e-02,  1.7019e-02,  2.9501e-02,  7.0267e-03,\n",
       "          5.4089e-02,  1.9116e-02,  4.5675e-02, -4.5773e-04, -4.3515e-02,\n",
       "          2.3488e-02,  5.3497e-02, -3.9397e-02,  2.2194e-02, -5.3000e-02,\n",
       "         -8.8903e-03,  2.4079e-02, -1.4019e-02,  6.6247e-02,  4.2537e-03,\n",
       "         -2.3802e-02, -1.7372e-02, -2.7146e-03,  4.6904e-02,  4.2506e-02,\n",
       "          6.8520e-03, -1.1349e-02,  4.6769e-02,  4.9382e-02,  4.5297e-02,\n",
       "         -5.0731e-02,  1.0408e-01,  3.6664e-02, -1.8554e-02,  7.5874e-03,\n",
       "         -3.3396e-02,  5.5735e-02, -5.2918e-02,  5.5982e-02, -2.4583e-02,\n",
       "          7.5801e-02,  5.3600e-02, -7.1831e-03,  1.9582e-02, -4.4576e-02,\n",
       "         -1.0280e-02, -6.0165e-04, -3.4103e-02, -6.0360e-02,  4.8171e-02,\n",
       "          3.3792e-02,  2.7355e-02, -1.6527e-02, -2.9219e-02,  4.5121e-03,\n",
       "          3.0088e-02, -1.1927e-02, -3.0286e-03, -2.6488e-03, -3.2543e-02,\n",
       "         -3.0537e-03,  1.3217e-02, -1.9615e-02, -2.0013e-02, -2.5314e-02,\n",
       "         -2.6106e-02,  1.5721e-02, -3.7469e-03,  5.0616e-02,  5.0653e-02,\n",
       "          2.4627e-02,  8.4346e-02,  7.0482e-03,  1.0446e-03, -3.1975e-02,\n",
       "         -5.5662e-02,  3.5648e-03, -4.3138e-03,  4.0741e-02,  2.9199e-02,\n",
       "         -4.8046e-02, -4.0720e-02, -5.4493e-03,  5.1702e-03, -3.6056e-03,\n",
       "          4.0438e-02,  5.7988e-02, -4.5940e-02,  3.4645e-02, -6.9635e-02,\n",
       "          6.0825e-02,  2.5606e-02,  8.0196e-04,  1.7650e-02, -2.1830e-02,\n",
       "         -4.4601e-02, -6.7156e-02, -1.1347e-02, -3.3444e-02,  9.0831e-03,\n",
       "          4.0737e-02, -6.5244e-02, -1.7177e-02, -2.5905e-02, -6.1166e-03,\n",
       "         -3.3916e-02,  8.3732e-04, -6.7717e-03, -7.3469e-02, -7.7076e-02,\n",
       "          1.0634e-02,  3.1727e-02, -6.4175e-03,  1.4050e-02,  4.3373e-02,\n",
       "          1.0533e-02,  3.7544e-02, -6.2736e-03, -2.5703e-02, -4.9138e-02,\n",
       "         -4.2963e-03,  5.1083e-02,  2.4160e-02, -4.7603e-03,  3.6301e-03,\n",
       "          5.2913e-02,  2.7058e-03,  5.3587e-03,  2.2324e-02, -1.6435e-02,\n",
       "          3.9436e-02,  5.0491e-02,  8.8427e-03, -3.6444e-02,  6.2750e-02,\n",
       "         -1.2429e-03,  1.0602e-02, -1.5016e-02,  5.9311e-03, -3.4697e-02,\n",
       "         -7.0416e-02,  2.4519e-02, -2.1112e-02, -2.7250e-02, -2.5324e-02,\n",
       "          2.0280e-02,  1.6790e-02,  2.7234e-02, -7.7265e-02,  1.1304e-02,\n",
       "          2.9310e-02, -5.3218e-02,  7.1870e-03,  2.2235e-02,  3.0863e-02,\n",
       "         -1.7449e-02,  6.5035e-03, -6.9410e-02, -4.6167e-02,  2.5737e-02,\n",
       "          1.9029e-03,  2.7175e-02, -2.1034e-02,  1.1156e-02,  1.9104e-02,\n",
       "         -1.4924e-03,  1.9229e-02,  2.5952e-02, -6.9570e-03, -3.3875e-02,\n",
       "          2.5969e-02, -3.0974e-02,  3.5701e-02,  6.2056e-02, -2.4639e-02,\n",
       "         -6.0354e-02,  1.5684e-03, -4.3925e-02, -5.4687e-02,  3.7836e-02,\n",
       "         -5.2466e-02,  1.1239e-02, -3.9780e-02, -1.0517e-02, -7.3567e-02,\n",
       "          1.5888e-02, -8.8965e-03,  6.3358e-02, -7.9624e-03,  1.7485e-02,\n",
       "         -1.3744e-02, -2.6550e-02,  6.3432e-02,  8.4408e-03,  4.1818e-02,\n",
       "         -8.4470e-03, -2.4950e-02,  6.0329e-02, -2.0293e-03,  2.0118e-02,\n",
       "         -5.4960e-02,  2.3652e-02, -3.7304e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.encoder_question(\n",
    "        vis_inputs=batch['vis_inputs'],\n",
    "        vis_attention_mask=None,\n",
    "        task ='IR',\n",
    "        return_pooled_output=True,\n",
    "        pool_strategy=\"avg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0528, -0.3212, -0.1362,  ..., -0.1120, -0.3062,  0.3349],\n",
       "         [ 0.3147, -0.2520, -0.1157,  ..., -0.2526,  0.0031,  0.2288],\n",
       "         [ 0.3366, -0.0863, -0.0805,  ..., -0.1821,  0.0955,  0.1816],\n",
       "         ...,\n",
       "         [ 0.1156, -0.1263, -0.1870,  ..., -0.1023, -0.1788,  0.2908],\n",
       "         [ 0.2343,  0.0855, -0.1409,  ..., -0.1332,  0.0280,  0.3021],\n",
       "         [ 0.0130,  0.0181,  0.0114,  ...,  0.0195,  0.0004, -0.0061]]],\n",
       "       grad_fn=<MulBackward0>), pooler_output=tensor([[ 1.1141e-02,  2.9527e-02,  3.4888e-02,  3.0994e-02,  3.1747e-02,\n",
       "         -2.1791e-03,  1.1540e-02,  7.4310e-03, -2.1859e-02,  9.5443e-03,\n",
       "          3.8616e-02, -2.9687e-03,  4.1030e-02,  1.0764e-03, -3.2001e-02,\n",
       "          5.9767e-02,  4.7179e-02,  2.3814e-02, -2.4132e-02,  2.0956e-02,\n",
       "          5.9360e-03, -4.4456e-02, -2.9656e-02,  7.3808e-02,  1.2182e-02,\n",
       "          5.1184e-02,  1.3780e-02,  2.1093e-02,  4.1076e-02, -6.8030e-03,\n",
       "         -2.6715e-02,  1.3569e-02,  3.3610e-02, -2.3134e-02, -4.6583e-03,\n",
       "          1.4096e-02, -2.5515e-02,  3.8498e-02,  8.7387e-02, -5.1957e-02,\n",
       "          2.1768e-02, -2.6462e-02,  1.3822e-02,  1.2345e-02, -5.4481e-02,\n",
       "         -9.3630e-03, -3.2514e-02, -2.3731e-02,  5.9107e-02,  1.9389e-02,\n",
       "          5.4833e-02,  1.5771e-02, -6.0517e-02,  4.1877e-02, -2.3741e-03,\n",
       "          4.7007e-02, -1.5106e-02,  2.0781e-02, -2.4591e-02,  2.3310e-03,\n",
       "          6.2534e-04,  9.0231e-03, -4.1891e-02, -6.5638e-04,  4.4366e-02,\n",
       "          4.2994e-02, -1.7151e-02, -3.0492e-02, -3.6934e-02,  6.2417e-03,\n",
       "         -4.0289e-02, -4.2161e-02,  1.8361e-02,  1.3624e-02, -1.6550e-02,\n",
       "         -6.2351e-02,  3.2014e-02, -1.2341e-03,  1.3419e-02,  1.1625e-02,\n",
       "          3.6493e-02,  2.5327e-03,  4.0991e-02, -2.6817e-02,  5.5545e-02,\n",
       "          1.1512e-02,  1.2727e-02, -2.0768e-02,  1.3631e-02,  8.0584e-03,\n",
       "          4.9382e-02, -3.1706e-02,  3.7904e-02,  4.2935e-02, -4.2201e-02,\n",
       "         -2.9552e-02,  4.6967e-02,  2.1785e-02,  4.1112e-02, -5.2800e-02,\n",
       "          3.0878e-02, -4.3824e-02,  6.7855e-02, -7.8629e-02,  1.9454e-02,\n",
       "         -4.8725e-03, -1.6286e-02, -4.0775e-02,  3.9517e-02, -2.6957e-02,\n",
       "         -3.0690e-02,  6.2432e-02, -4.2037e-02, -2.2310e-04, -1.9161e-02,\n",
       "          2.8058e-02, -5.4267e-02,  8.1024e-03,  9.0377e-03, -1.8065e-02,\n",
       "          4.2626e-02, -1.8343e-02, -2.9788e-02, -2.1594e-02,  5.7485e-03,\n",
       "         -3.3730e-02,  6.4088e-03, -7.5415e-03, -3.2286e-02, -5.0855e-02,\n",
       "         -3.9680e-02, -1.4126e-02,  5.4366e-02, -4.8055e-02,  5.6655e-03,\n",
       "         -5.9209e-02,  1.1397e-02,  2.6210e-02,  9.7489e-03,  3.5872e-02,\n",
       "          1.4800e-02, -7.7524e-02, -2.4429e-02,  3.1946e-02,  5.2478e-02,\n",
       "         -4.7017e-02, -2.7924e-02,  6.0170e-03,  3.3089e-03, -4.3049e-02,\n",
       "          2.1647e-02,  2.0531e-02,  1.4777e-03,  3.1465e-02, -4.2716e-03,\n",
       "         -6.5555e-02, -4.7801e-03, -6.1265e-02,  2.2364e-02, -3.7903e-02,\n",
       "          4.5837e-02, -3.0903e-03,  6.3819e-02,  2.4021e-02, -4.3878e-02,\n",
       "         -3.4706e-02, -1.8871e-02,  6.7586e-02, -2.0197e-02, -2.3661e-02,\n",
       "         -4.6894e-02,  4.2615e-02,  5.7124e-03, -2.8081e-02, -4.8081e-03,\n",
       "         -3.0367e-02,  4.5741e-02, -2.0870e-02,  4.9252e-02, -1.7377e-02,\n",
       "          2.2599e-02,  1.8284e-02,  3.9730e-02, -2.1273e-02, -3.9356e-02,\n",
       "          9.6869e-03, -6.6052e-03,  3.4606e-02, -7.5343e-02, -1.7138e-02,\n",
       "         -5.2751e-02,  2.4319e-02,  2.1507e-02, -3.2050e-02, -3.5266e-02,\n",
       "          1.5661e-02, -2.2188e-02,  8.3924e-02, -8.3353e-02,  9.1543e-03,\n",
       "         -3.3678e-02,  6.7458e-03, -7.8778e-03,  1.8269e-02,  1.4085e-02,\n",
       "          2.6056e-02,  8.8088e-03, -1.8437e-02,  3.3142e-02, -3.7398e-02,\n",
       "         -1.0444e-02, -2.4495e-02,  8.6844e-03, -5.6558e-02,  1.7872e-02,\n",
       "          4.9636e-02,  5.7166e-02, -8.7549e-03,  1.7965e-02, -4.1085e-02,\n",
       "         -2.5980e-02, -1.4851e-02, -9.1135e-02,  2.1197e-02,  2.3942e-02,\n",
       "          4.3214e-02,  2.3764e-02,  1.0621e-01,  2.3131e-02, -2.2315e-02,\n",
       "          2.2716e-03, -3.5407e-02,  3.2248e-02, -3.1816e-02, -4.5163e-02,\n",
       "          1.0618e-01, -1.1081e-02, -4.4101e-02, -1.6936e-02, -2.8486e-02,\n",
       "          1.8967e-02, -2.3701e-02,  7.6156e-02, -4.9087e-02, -4.1987e-02,\n",
       "          1.7366e-03, -3.1243e-02,  5.0558e-02, -1.6749e-02, -3.9906e-02,\n",
       "         -1.5157e-02, -6.7109e-02,  6.2368e-02,  8.9723e-02,  9.0913e-03,\n",
       "          6.2614e-03, -1.4337e-02, -6.6539e-03, -4.2671e-02,  2.6270e-02,\n",
       "          3.7645e-02, -3.4093e-03, -4.0774e-02,  9.6512e-02, -8.1995e-03,\n",
       "          4.3255e-02,  3.8961e-02, -5.8711e-02, -4.2292e-03, -1.0520e-02,\n",
       "         -7.3724e-03,  7.9119e-03, -4.6770e-02,  5.2098e-02, -8.0188e-03,\n",
       "         -2.2600e-02, -1.4348e-02,  5.8979e-03, -1.9483e-03, -9.0587e-03,\n",
       "         -3.2327e-02, -1.0540e-04, -4.8298e-02,  4.1065e-02,  7.0103e-03,\n",
       "         -5.3164e-03,  8.6206e-03, -4.1595e-02, -4.8083e-02,  2.6460e-02,\n",
       "         -1.5663e-02, -4.0803e-02,  1.5489e-02,  7.2297e-03, -5.1435e-03,\n",
       "          1.0855e-02, -5.3207e-02, -1.5510e-02, -2.4659e-02, -2.7477e-02,\n",
       "          3.7330e-02, -5.9467e-03, -7.5487e-04, -1.2307e-02, -2.2944e-02,\n",
       "         -6.3433e-03, -2.7232e-02,  6.5812e-02, -7.2203e-02, -1.0173e-01,\n",
       "         -2.2256e-02,  1.7931e-02,  3.3941e-03, -5.8435e-02, -1.3629e-02,\n",
       "          1.9533e-02,  4.0627e-03, -2.5499e-03,  7.4093e-02,  4.1792e-02,\n",
       "         -2.5793e-02,  4.6117e-03, -3.8229e-02,  2.1686e-02,  3.0866e-02,\n",
       "         -2.2809e-02,  1.5763e-02,  4.1190e-02,  7.2587e-03, -2.7767e-02,\n",
       "          2.1271e-02, -2.7824e-02, -1.7524e-02,  3.5131e-02, -2.4696e-02,\n",
       "          2.3124e-02,  4.9992e-02, -5.9244e-03, -3.9897e-02,  3.5711e-02,\n",
       "         -9.1571e-02, -6.7426e-03, -1.8163e-02,  5.2871e-03,  6.7106e-02,\n",
       "          2.5574e-02,  3.0231e-03,  3.6073e-04,  1.1217e-02, -1.2284e-02,\n",
       "          1.4528e-03,  7.8401e-03,  6.3190e-03, -4.4428e-02,  2.4028e-02,\n",
       "         -1.3939e-02, -2.7508e-02,  1.9761e-04, -1.9970e-02,  3.1809e-02,\n",
       "          4.5720e-02, -2.8694e-03,  7.3405e-02,  3.3951e-02,  2.1691e-02,\n",
       "         -3.4590e-02, -3.3888e-02,  4.1077e-02,  1.0111e-01, -1.0725e-02,\n",
       "          4.8417e-02,  4.5991e-02,  9.8651e-03, -6.5020e-03, -2.8984e-02,\n",
       "          3.9481e-03, -1.1363e-02, -3.9864e-02,  7.4180e-02, -7.7408e-03,\n",
       "         -2.3997e-02,  3.0426e-02,  1.2504e-02,  1.5651e-02, -2.2032e-02,\n",
       "          3.6843e-02, -1.8935e-02, -2.2779e-02, -6.7014e-03, -3.5355e-02,\n",
       "         -3.5818e-02,  1.0890e-02, -3.9282e-02, -4.9832e-03,  2.3974e-02,\n",
       "         -3.5927e-02, -3.6501e-02,  8.9276e-03, -2.5620e-02,  7.3993e-02,\n",
       "          1.3390e-02, -9.4302e-03, -3.3568e-02, -1.1808e-02,  3.1833e-02,\n",
       "          1.0439e-02, -5.7559e-02, -3.8582e-02, -1.4956e-03, -2.8232e-02,\n",
       "         -6.5044e-03, -9.4906e-03,  2.8997e-02, -2.6460e-03, -2.3025e-02,\n",
       "         -1.1541e-02,  6.8925e-04,  1.5146e-02,  5.8412e-03, -6.9582e-02,\n",
       "          3.0549e-02, -6.3506e-02, -5.9183e-02,  7.2651e-03, -8.2540e-03,\n",
       "          3.7743e-03,  3.1495e-02,  2.7645e-03, -3.4277e-02,  3.5459e-02,\n",
       "         -9.7102e-03,  3.9571e-02,  1.4085e-04,  2.8177e-02, -1.2916e-02,\n",
       "         -6.6258e-03, -1.4432e-02, -2.6593e-02, -2.5644e-02,  9.0381e-02,\n",
       "          5.5448e-02,  4.8781e-02,  2.9802e-02, -5.1513e-02,  6.8002e-02,\n",
       "          4.3234e-02, -1.2143e-02, -5.1789e-02,  9.0447e-03, -2.9475e-03,\n",
       "          1.4561e-02, -1.2432e-04, -3.7571e-02,  3.7762e-03,  1.7552e-02,\n",
       "         -1.2773e-02,  1.0148e-02,  4.0684e-02, -2.8917e-02, -3.2506e-02,\n",
       "          3.1508e-02,  1.3791e-02,  2.3235e-03, -2.8603e-02,  1.4237e-03,\n",
       "         -3.4337e-02, -5.7933e-04,  6.5651e-02, -2.4047e-02, -4.1288e-02,\n",
       "         -1.6919e-03,  5.6977e-02, -2.8923e-02, -2.3704e-03,  7.9486e-03,\n",
       "         -5.4917e-02, -2.4883e-02,  4.0214e-02,  6.6958e-02,  2.9338e-02,\n",
       "         -2.6761e-02, -2.3486e-02, -2.1964e-03, -9.5349e-03, -2.7072e-02,\n",
       "         -3.0199e-02,  5.2439e-02, -5.2839e-02,  2.6173e-02, -5.1244e-03,\n",
       "          3.2116e-02,  1.6128e-02,  2.3547e-04,  4.3031e-02,  4.0474e-02,\n",
       "         -8.7495e-04,  3.0229e-02,  2.9435e-02, -3.1776e-02, -1.3279e-03,\n",
       "         -2.6104e-02, -6.4365e-03,  4.2901e-02,  8.0440e-03, -4.4574e-02,\n",
       "          5.3806e-02, -3.7725e-03,  4.1281e-02, -4.3497e-03, -3.7347e-02,\n",
       "         -3.8808e-02,  2.9567e-02, -9.9790e-04, -1.9896e-02, -3.0907e-02,\n",
       "         -3.6581e-02, -3.4279e-02,  3.2609e-03,  1.3185e-02,  8.5909e-02,\n",
       "          3.1380e-02,  5.7731e-03,  6.6297e-02, -8.4414e-03, -1.0790e-02,\n",
       "          4.3386e-02, -2.6205e-02,  6.5233e-02,  1.7526e-02,  9.6573e-03,\n",
       "         -2.0112e-02, -2.7064e-02, -1.1075e-03,  2.8592e-02, -2.0692e-02,\n",
       "          4.4237e-02,  4.0831e-02,  1.1741e-02,  9.8706e-03,  1.0009e-02,\n",
       "         -2.0976e-02, -4.0328e-02, -2.8182e-02,  2.8454e-02, -3.0628e-02,\n",
       "         -9.3778e-03, -4.3492e-02, -2.0529e-02,  4.7529e-03, -1.4916e-02,\n",
       "          6.7367e-02,  1.2221e-02,  3.8352e-02,  4.7468e-02,  3.8534e-03,\n",
       "          2.1061e-02, -7.6329e-03,  1.5663e-02, -7.6206e-02,  4.9982e-02,\n",
       "         -2.8393e-03, -8.2800e-03,  4.7447e-02,  7.3598e-02,  2.8010e-02,\n",
       "          1.8744e-02, -2.6232e-02,  1.6060e-02, -2.1994e-02, -5.7386e-02,\n",
       "         -5.5921e-02,  5.5825e-02, -8.2338e-03, -9.9523e-02, -9.8314e-03,\n",
       "          1.1715e-02, -3.4156e-02, -1.0836e-02,  3.3719e-03,  1.4359e-02,\n",
       "         -3.6248e-02,  5.4016e-02,  2.7571e-02,  2.6818e-02, -8.1820e-03,\n",
       "          1.3855e-02, -4.4405e-03, -4.7425e-02, -7.5364e-02,  3.0974e-02,\n",
       "          3.9795e-02,  2.4099e-02,  1.1214e-02, -5.8985e-02, -8.3971e-03,\n",
       "          8.4893e-03, -7.7287e-03, -2.5387e-02, -2.9465e-02,  1.5568e-02,\n",
       "          1.0974e-02,  2.4415e-02, -6.5067e-02, -2.2622e-02, -2.1673e-02,\n",
       "         -2.5087e-02,  7.1343e-02, -7.0867e-02,  2.2380e-02, -6.2843e-02,\n",
       "         -1.5249e-02,  1.0529e-02,  5.4188e-02,  4.9126e-02,  1.7339e-02,\n",
       "          1.7903e-02, -1.2623e-02,  6.9095e-02,  1.0737e-02, -4.3703e-03,\n",
       "         -8.0643e-03, -5.7861e-02,  4.2986e-02, -3.6689e-03,  3.7054e-02,\n",
       "          2.8227e-02,  4.0519e-02, -7.2134e-02, -4.8416e-02,  5.2104e-02,\n",
       "         -3.5291e-02,  1.0215e-03, -3.8601e-02, -7.8099e-02,  5.9201e-03,\n",
       "          3.2580e-02,  8.1910e-03, -2.7952e-02,  2.8138e-02,  1.9642e-03,\n",
       "         -1.1732e-02,  1.5781e-02,  3.8204e-02,  4.6833e-02, -2.7546e-02,\n",
       "         -3.0566e-04, -6.2686e-02,  4.6648e-02, -3.6344e-02,  1.0134e-02,\n",
       "          4.8781e-02,  7.5638e-02, -2.3476e-02,  2.9625e-02, -5.7191e-02,\n",
       "          7.3899e-02, -1.2151e-02, -8.4688e-03,  2.1134e-02,  1.8684e-02,\n",
       "          4.3696e-02,  5.1188e-02,  1.3596e-02,  1.6584e-02, -2.9596e-02,\n",
       "          2.1694e-03, -3.1062e-02,  7.7539e-03, -1.3437e-02, -3.6784e-02,\n",
       "         -2.1235e-02,  5.8665e-03,  7.5668e-03, -5.6352e-02, -1.1105e-02,\n",
       "          2.1975e-02, -3.4344e-02,  1.1825e-02,  4.4872e-02, -9.3333e-02,\n",
       "          7.8513e-02,  3.7116e-02,  2.2134e-02,  2.9848e-03,  1.5360e-02,\n",
       "          3.5323e-03, -2.0160e-03, -6.1588e-02, -7.8256e-03,  7.3935e-02,\n",
       "          1.1463e-02, -2.9989e-02,  2.4094e-02,  1.0079e-01, -3.4269e-02,\n",
       "         -4.9645e-02,  2.9506e-02, -1.6708e-02,  2.7895e-02,  4.0243e-03,\n",
       "         -4.1583e-02, -2.1057e-02, -6.6574e-02, -7.1321e-02,  3.9332e-02,\n",
       "         -5.6926e-02,  3.7912e-03,  5.3879e-02,  6.9315e-02, -2.9899e-02,\n",
       "          2.8902e-02,  4.6288e-02,  2.0793e-02,  1.4264e-02,  2.3692e-02,\n",
       "         -1.0654e-02,  1.1289e-01, -5.6434e-03, -2.3216e-02, -2.8937e-02,\n",
       "          3.3503e-02,  4.1517e-02, -8.6506e-03,  1.8666e-02,  2.5439e-04,\n",
       "          4.9504e-02, -1.9065e-02, -3.2843e-02, -3.7289e-03,  2.9507e-02,\n",
       "          4.0867e-02,  1.2275e-02, -7.2988e-02,  2.3712e-02, -4.6631e-02,\n",
       "         -4.5153e-02, -8.6268e-03,  4.2787e-02, -1.8446e-02,  2.3507e-02,\n",
       "          6.2174e-02,  9.5326e-03,  8.9571e-03,  8.9986e-02,  1.1591e-02,\n",
       "          2.2118e-02,  2.3911e-02, -1.4916e-02, -6.3667e-02, -3.7898e-03,\n",
       "         -3.9939e-02,  1.0074e-02,  1.4598e-02,  8.0638e-03, -3.0133e-02,\n",
       "          2.5571e-02, -1.2954e-02,  3.6278e-02,  4.0046e-02,  1.1276e-02,\n",
       "         -6.6703e-02,  7.6960e-02, -1.8426e-02, -6.3298e-03,  3.4498e-02,\n",
       "         -2.0391e-02, -4.7185e-02,  2.0280e-02]], grad_fn=<DivBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cvlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "025084c40b588eb05019761c48ebfbdc57758708fc0433510479ba20b4a83a56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
